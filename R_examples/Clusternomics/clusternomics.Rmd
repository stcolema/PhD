---
title: "Clusternomics"
author: "Stephen Coleman"
date: "17/10/2019"
output: pdf_document
bibliography: clusternomics.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Principal aim: to model both **global** (across dataset) and **local**
(dataset-specific) clustering structure. Specifically, Clusternomics 
[@gabasova2017clusternomics] can allow for clusters merging and separating in
different datasets (also referred to as **contexts**). A motivating example is 
shown below for two 1D datasets.

```{r motivating_example}
library(ggplot2) # ubiquitous
library(ggExtra) # for ggMarginal

# Personal preference
theme_set(theme_bw())

# Data generated at http://drawdata.xyz/
my_data <- read.csv("./example_data.csv")

# Plot the data
p1 <- ggplot(data = my_data, mapping = aes(x = x, y = y, colour = z)) +
  geom_point() +
  labs(
    title = "Example of different cluster behaviour across contexts",
    x = "Context 1",
    y = "Context 2",
    colour = "Cluster"
  )

# Add marginal density plots by grouping
ggMarginal(p1, groupColour = T, groupFill = T)
```

Here we have two separate subpopulations. In Context 1 the sub-populations are 
discernible as two local clusters. However this not the case in Contex 2. 
Clusternomics allows for such disagreement in local models while conveying
the complexity of clustering structure (in this case that there really is two 
sub-populations present) to the global model.

## The model

Clusternomics, which is embedded in the Bayesian clustering framework, uses a 
hierarchical Dirichlet mixture model to identify structure on both the local and
the global level. The model is fit to the data via Gibbs sampling. The model 
does not assume that cluster behaviour will be consistent across heterogeneous 
datasets. This is not to assume that the clustering structure uncovered in one 
dataset should not inform the clustering in another dataset. This can be 
summarised as so:

1. Clustering structure in one dataset should inform the clustering in another 
dataset. If two points are clustered together in one context they should be 
more inclined to cluster together in other contexts.

2. Different degrees of dependence should be allowed between clusters across
contexts. The model should work when datasets have the same underlying structure
and also when each dataset is effecitvely independent of all others. Fundamental 
to this is allowing datasets to have different numbers of clusters.

To enable these modelling aims, Clusternomics explicitly represents the local 
clusters (i.e. dataset specific) and the global structure that emerges when 
considering the combination of the datasets. The global clusters are defined by 
combinations of local clusters. Consider the case where 3 clusters emerge in 
Context 1 (denoted by labels $\{1, 2, 3\}$) and 2 clusters emerge in Context 2 
(denoted by labels $\{A, B\}$). In this case our global structure has the 
possibile form:

$$\{(1, A), (2, A), (3, A), (1, B), (2, B), (3, B)\}$$

Thus if a point is assigned a label of 1 in Context 1 and a label of $A$ in 
Context 2 it increases the probability of cluster $(1, A)$ becoming populated 
at the global level. However, it is possible that some of the possible 
global clusters described above are not realised as some local clusters overlap 
across datasets. Consider the case that labellings 1 and 2 from the first 
context are captured entirely by label $A$ in the second context with a albel of 
3 corresponding perfectly to label $B$. In this case our global structure would 
take the form:

$$\{(1, A), (2, A), (3, B)\}$$

In this way the local structure informs the global structure.

The original paper introduces two models that are "asymptotically equivalent". 
The first is easier to develop an intuition of, but it is the second that is 
implmented as it is more computationally efficient.

### Notation

Let us denote the number of datasets by $L$ and all observed data by $X$. Let

$$X = (X_1,  \ldots, X_L),$$
$$X_l = (X_{l1},\ldots,X_{ln})$$
where $X_l$ is the data of the $lth$ context.

It is assumed that it is the same $n$ individuals in each dataset in the same 
order. Therefore we have $L$ membership vectors denoting cluster membership:

$$C = (C_1, \ldots, C_L),$$
$$C_l = (c_{l1},\ldots,c_{ln}).$$

### Basic model

The basis of the integrative model is a finite approximation of a Dirichlet 
process known as a Dirichlet-Multinomial Allocation mixture model 
[@green2001modelling]. There is a nice explanation of this model in 
@savage2013identifying.

In this case we model the latent structure in the $lth$ dataset using a mixture 
of $K_l$ components. This means that the full model density is the weighted 
sum of the probability density functions associated with each component where
the weights, $\pi_{lk}$, are the proportion of the total population assigned to the 
$kth$ components:

$$p(X_{li}| c_{li} = k) = \pi_{lk} f(X_{li} | \theta_{lk}),$$
$$p(X_{li}) = \sum_{k=1}^{K_l} \pi_{lk} f_l(X_{li} | \theta_{lk}).$$
Here $K_l$ is the total number of clusters present and $\theta_{lk}$ are the 
parameters defining the $kth$ distribution in the $lth$ dataset.

The weights, $\pi_l = (\pi_{l1},\ldots,\pi_{lK_l})$, follow a Dirichlet 
distribution with concentration parameter $\alpha_0$.

The distributions in the mixture model for each dataset are:

\[
\begin{aligned}
\pi_l &\sim \textrm{Dirichlet}\left(\frac{\alpha_0}{K_l},\ldots,\frac{\alpha_0}{K_l}\right) \\
c_{li} &\sim \textrm{Categorical}(\pi_l) \\
\theta_{lk} &\sim h_l \\
X_{li} | c_{ln} = k &\sim f_l(X_{li} | \theta_{lk})
\end{aligned}
\]


where $H_l$ is some prior distribution for parameteters for each mixture
component; $F_l$ is a probability distribution for samples given the parameters
$\theta_{lk}$. Note that each context may have different distributions and
hyperparameters.

### First formulation: the intuitive model

For ease of understanding, let $L=2$. Then each context has its own mixture 
weights with symmetric Dirichlet priors:

\[
\begin{aligned}
\pi_1 &\sim \textrm{Dirichlet}\left(\frac{\alpha_1}{K_1},\ldots,\frac{\alpha_1}{K_1}\right) \\
\pi_2 &\sim \textrm{Dirichlet}\left(\frac{\alpha_2}{K_2},\ldots,\frac{\alpha_2}{K_2}\right)
\end{aligned}
\]


These weights form the basis of the local clustering within each dataset. A 
third mixure distribution is used to link the two local clusters. This has a 
Dirichlet prior over the global mixture weights, $\rho$, which is defined over 
the outer product of the local weights:


\[
\rho \sim \textrm{Dirichlet}\left(\gamma \textrm{ vec}(\pi_1 \otimes \pi_2)\right).
\]

The outer product is defined:

\[
\begin{aligned}
\pi_1 \otimes \pi_2 &= \pi_1 \pi_2^T \\
  &= \begin{pmatrix}
  \pi_{11}\pi_{21} & \pi_{11}\pi_{22} & \cdots & \pi_{11}\pi_{2K_2} \\
  \pi_{12}\pi_{21} & \pi_{12}\pi_{22} & \cdots & \pi_{12}\pi_{2K_2} \\
  \vdots & \vdots & \ddots & \vdots \\
  \pi_{1K_1}\pi_{21} & \pi_{1K_1}\pi_{22} & \cdots & \pi_{1K_1}\pi_{2K_2} \\
  \end{pmatrix}
\end{aligned}
\]

Then the vector function takes the column vectors of the matrix and places them 
in one vector:

\[
\mathrm{vec}(\pi_1 \otimes \pi_2) = \begin{pmatrix}
  \pi_{11}\pi_{21} \\
  \vdots \\
  \pi_{1K_1}\pi_{21}\\
  \pi_{11}\pi_{22} \\
  \vdots \\
  \pi_{1K_1}\pi_{22} \\
  \pi_{11}\pi_{23} \\
  \vdots \\
  \pi_{1K_1}\pi_{2K_2}
\end{pmatrix}
\]

This is the basis for the non-symmetric concentration parametera of the 
Dirichlet distribution over the global mixture weights, $\rho$. 

# References

