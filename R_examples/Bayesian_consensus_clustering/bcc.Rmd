---
title: "Bayesian consensus clustering"
author: "Stephen Coleman"
date: "22/10/2019"
output: pdf_document
bibliography: bcc.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notation

\[
\begin{aligned}
n & & \textrm{ The number of samples in the data.} \\
p & & \textrm{ The number of measurements for each sample.} \\
L & & \textrm{ The number of datasets present.} \\
K_l & & \textrm{ The number of components present in the $lth$ dataset.} \\
x &= (x_1, \ldots,x_l) & \textrm{ The datasets. } \\
x_l &= (x_{l1}, \ldots, x_{ln}) & \textrm{ The observed data for the $lth$ dataset.} \\
c &= (c_1, \ldots, c_L) & \textrm{ The membership vectors for each dataset (our latent variable).} \\
c_l &= (c_{l1}, \ldots, c_{ln}) & \textrm{ The context-specific component membership.} \\
C &= (C_1, \ldots, C_n) & \textrm{ The global allocation vector.} \\
\pi_l &= (\pi_{l1,}\ldots, \pi_{lK_l}) & \textrm{ The mixture weights in the $lth$ context.}
\end{aligned}
\]

## Intro

### Dirichlet-Multinomial Allocation mixture model

This model [@lock2013bayesian] is based upon a finite approximation of the 
Dirichlet process known as the Dirichlet-Multinomial Allocation (DMA) mixture model 
[@green2001modelling], as is Clusternomics [@gabasova2017clusternomics] and 
Multiple Dataset Integration [@kirk2012bayesian].

Consider a dataset of $N$ samples, $x = (x_1,\ldots,x_N)$ where each sample is 
itself a $p$-vector of measurements for some $p \in \mathbb{N}$. We are 
interested in uncovering structure in the data. To do this we associate each 
sample, $x_i$, with a *cluster*.This moves us from a $p$-dimensional 
space to a 1-dimensional space based upon similarity of samples which enables 
interpretation. We want to understand if there are sub-populations in our sample
that are responsible for heterogeneity. By partitioning the data into clusters 
we hope to have some insight into this underlying structure and improve our 
understanding of the data. There are a myriad of ways to uncover such 
partitions. Some methods have more obvious disadvantages than others, a common 
problem being that the number of clusters allowed in the result is arbitrary. 
Another common problem is that some of the methods are not model based which
gives an ad-hoc nature to the paritioning. A method that does not suffer from 
these problems is the Dirichlet process. A tractable approximation of this
is the DMA mixture model which is a common choice 
in the Bayesian-model based clustering. In this case we model each 
sub-population by an individual distribution. We also allow the number of
clusters present to be inferred from the data, thus avoiding a heuristic or 
arbitrary means of selecting the number of clusters allowed.

In this model we use a mixture of $K$ components to model the sub-populations.
The components are all modelled by a distribution with a density function 
$f(\cdot)$. The $kth$ component has associated parameters, $\theta_k$, based on 
the samples allocated to this component and is thus defined by density 
$f(\theta_k)$. The proportion of samples within the components define the 
associated mixture weights, $\pi=(\pi_1,\ldots,\pi_K)$. Each sample is allocated 
to a specific component; this component memberhsip is represented by the latent
variable $c=(c_1,\ldots,c_n)$, where 
$c_i \in \{1,\ldots, K\} \forall i \in \{1,\ldots,N\}$. Therefore we have 
allocation probability:

\[
p(x_i| c_i = k) = \pi_k f(x_i \theta_k),
\]
and the full model density, which is a weighted sum of the mixture densities:
\[
p(x_i) = \sum_{k=1}^K\pi_k f(x_1 | \theta_k)
\]
Note that while there are component-specific parameters, the density function 
used, $f(\cdot)$, is common across all components.

In the DMA mixture model, the mixture weights are 
given a Dirichlet prior, normally with a symmetric concentration parameter. The 
allocation variable is then sampled from a Cateogrical distrbution defined by 
the component weights. The component parameters are then updated based upon the 
allocation of samples and the full model is then the weighted sum described 
above. The full hierarhcical model is described below:
\[
\begin{aligned}
\pi &\sim \textrm{Dirichlet}\left(\frac{\alpha_0}{K},\ldots,\frac{\alpha_0}{K}\right), \\
c_i &\sim \textrm{Categorical}(\pi), \\
\theta_k &\sim h(\cdot), \\
x_i | c_i = k &\sim f(x_i | \theta_k).
\end{aligned}
\]
Here $h(\cdot)$ is some distribution, often with additional hyperparameters. If 
one let's $K \to \infty$ then this model becomes a Dirichlet process. To 
approximate this $K$ is set to an arbitrarily large number. In this scenario one
must have chosen a $K$ sufficiently large that there are empty clusters for the 
model to be a true approximation of the Dirichlet process. For example, say we 
set $K=30$ in our initialisation, and that in the output 30 clusters are 
occupied (i.e. in our $c$ vector we have 30 unique labels). One must use a 
higher $K$ then this; if we try again with $K=50$ and $c$ contains 40 unique 
values than the number of clusters is learnt from the data and not an arbitrary 
choice of the user.

# Integrative model

Bayesian Consensus Clustering [@lock2013bayesian] is extension of the DMA 
mixture model. Consider the case of $L$ datasets. In this case there are 
context-specific models similar to that described above with a common number of 
components, $K$, in each model. There is then an additional *global clustering*,
$C=(C_1, \ldots, C_N)$, where $C_i \in \{1, \ldots, K\} \forall i \in [1, N]$.
In this model it is the local allocations that are dependent upon the global 
membership. For the $lth$ context, given the global clustering $C$, the context-
specific concentration parameter, $\alpha_l$:

\[
P(c_{li} = k | C_i) = \nu(k, C_i, \alpha_l)
\]


## References