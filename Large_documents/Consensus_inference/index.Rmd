--- 
title: "Consensus inference"
author: "Stephen Coleman"
date: "`r Sys.Date()`"
output:
  bookdown::gitbook:
    lib_dir: "book_assets"
  bookdown::pdf_book:
    keep_tex: yes
documentclass: book
bibliography: [sim_plan.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "A description of Consensus inference."
header-includes:
- \usepackage{caption}
- \usepackage{multirow}
- \usepackage{amsmaths}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{wrapfig}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), "bookdown", "knitr", "rmarkdown", "mcclust", "mclust", "coda"
), "packages.bib")

library(mdiHelpR)
library(tidyr)
library(plot3D)
library(plotly)
library(magrittr)
library(ggplot2)
library(tibble)
library(dplyr)
library(patchwork)

# devtools::install_github("sarawade/mcclust.ext")
library(mcclust.ext)

# Function for converting surfaces for 3D plot to long data for ggplot2
surface2Df <- function(x, n_seeds, n_iter) {
  data.frame(x) %>%
    set_colnames(1:n_seeds) %>%
    add_column(N_iter = as.factor(1:n_iter)) %>%
    pivot_longer(1:n_seeds,
      names_to = "N_chains",
      values_to = "ARI",
      names_ptypes = list(N_chains = factor()),
      values_ptypes = list(ARI = numeric())
    )
}

setMyTheme()
col_pal <- dataColPal()
sim_col_pal <- simColPal()

fracTrue <- log(1:1e4) / log(1e4)
nReq <- length(fracTrue)

ggplot(data.frame(fracTrue = fracTrue, Iter = 1:1e4), aes(x = Iter, y = fracTrue)) +
  geom_line() +
  labs(
    title = "Fraction of items allocated correctly",
    caption = "Within generated clusterings, the fraction of items allocated correctly as a function of the ``chain length''. ",
    x = "Number of iterations",
    y = "Fraction of items correctly allocated"
  )

# Generate an example clustering
ciSim <- function(true_labels, n_iter, K,
                  truth_at = 1e4) {

  # The number of samples present
  n <- length(true_labels)

  # The number of samples clustered truly
  n_true <- floor(1 + n * fracTrue[min(floor(1 + nReq * n_iter / truth_at), nReq)])

  # The index of these samples
  true_ind <- sample(1:n, n_true, replace = F)

  # Random partition
  out <- sample(1:K, size = n, replace = T)

  # Set the truly clustered correctly
  out[true_ind] <- true_labels[true_ind]

  out
}
```

# Abstract {-#abstract}

I propose Consensus inference as an alternative to Bayesian or Frequentist inference of clustering methods. It requires that methods are sampling-based (such as using Markov-Chain Monte Carlo methods). Consensus inference is proposed as MCMC methods can be slow and often fail to converge in finite time in large clustering problems; particularly when performing integrative clustering across many datasets. There exist clever implementations of MCMC methods that attempt to overcome this problem of convergence in clustering (for instance split-merge moves), but this methods remain very slow to run. I propose Consensus inference as a general solution to these problems. In clustering problems, partially due to the discrete nature of clustering labels, Gibbs sampling normally is quick to find a local mode (often within 10's of iterations). However, the sampler then remains trapped there for a very large number of iterations. This means that there is no variation in samples after burn-in. As a result of this, taking only a single sample from the chain after it becomes trapped is equivalent to taking all of the samples in terms of the inference performed. Consensus inference uses this behaviour to both acquire impressive reducitons in runtime, but also to attempt better exploration of possible clusterings. I propose running many short chains using many different random initialisations to try and explore the model space. Taking the first sample after burn-in from each of many chains and translating these into a _consensus matrix_ offers a more robust exploration of realistic clusterings possible in the data than using a single long chain. Furthermore, as the chains can be very short (often on the scale of 10's or 100's of iterations) and are independent of one another, one can take advantage of a parallel environment to drastically reduce runtime. I do not offer any guarantees that consensus inference is sampling the posterior space, rather than attempting to approximate Bayesian inference, this should be thought of as ensemble method combining many weak models into one more powerful, more robust model. I offer empirical evidence from simulations and real data from the Cancer Genome Atlas to show that Consensus inference does perform comparably to Bayesian and Frequentist inference in many scenarios, even outperforming these approaches in some.