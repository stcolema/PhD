# Theory {#introduction}

* Clustering
 * Mixture models
 * Frequentist 
 * EM
 * Bayesian
 * Gibbs sampling
 * Consensus clustering
 * Integrative clustering
 * Integrative Bayesian clustering 
* Problems

In this section I explain some of the reasons one may wish to do a cluster 
analysis. I then explain what mixture models are and how Bayesian and 
Frequentist inference of these models works. Following from this I introduce the
concept of integrative clustering as an extension of cluster analysis and give 
some examples of relevant methods and the results they have delivered. This 
background is necessary to understand some of the problems that currently exist, 
and why I am suggesting Consensus inference as a solution to these.

## Cluster analysis {#clusterAnalysis}

Cluster analysis – also known as unsupervised learning – constists of defining
partitions of items into homogeneous, separable groups known as clusters. It is 
used in multivariate statistics to uncover _useful_ latent groups suspected in 
the data or to discover _interesting_ groups of homogeneous observations. In 
large datasets, such as modern 'omics datasets, estimation of clusters is often
necessary for improved understanding and interpretation [@meinshausen2010stability]. 

Cluster analysis has a myriad of applications. These include data mining, 
which started from the search for groupings of customers and products in massive
retail datasets [@fraley2002model]; document clustering and the analysis of 
internet use data; spatial proteomics [@crook2018bayesian] where subcellular
localisation of proteins is predicted by clustering-based methods; and image
analysis, where clustering is used for image segmentation and quantization. The
area of interest for me is in clustering 'omics data to 
improve understanding of disease aetiology and (ideally) help clinical practice
through identifying driving biological mechanisms or in defining subtypes within
given diseases. Defining subtypes based upon behaviour at the 
subcellular level rather than on phenotypic patterns can enable more effective 
treatment. Examples where cluster analysis has been used to propose new subtypes
include Breast cancer [@cancer2012comprehensive; @berger2018comprehensive], 
Coronary artery disease [@guo2017cluster], tinnitus [@van2017cluster] and asthma
[@ortega2014cluster]. Another interesting application is the defining of subpopulations of 
patients; this application is integral to _Precision medicine_, which relies 
upon "the classification of people into subpopulations using their common 
genetic patterns, lifestyles, drug responses, and environmental and cultural
factors" [@gameiro2018precision].

Within cluster analysis there exists a large number of possible methods. 
Traditional approaches [as taught in many textbooks, see for e.g. @fidell2001using; @manly2016multivariate; @stevens2012applied] include hierarchical and non-hierarchical (normally $k$-means) 
clustering. These methods are heuristic in nature; they are not based upon a 
formal model and what guidance there is for solving important practical 
questions that arise in every cluster analysis (such as the number of clusters
present, which clustering method to, etc.) is subjective and informal. Despite 
this, most of the clustering that is done in practive is based upon these 
methods; they are intuitively reasonable and, perhaps more importantly, most 
clustering methods available in commercial software are of this type 
[@fraley2002model]. However, I am interested in _model-based cluster analysis_. 
This embeds the cluster analysis within a statistical framework and offers a
principled approach to many issues associated with cluster analysis. 
Speicifcally, my interest is in _mixture models_ where each subpopulation is 
described within the model by a probability distribution. In this setting, the
problems of determining the number of clusters and of choosing an appropriate 
clustering method can be recast as statistical model choice problems, and 
models that differ in numbers of components and/or in component distributions 
can be compared. Outliers are handled by adding one or more components representing a different distribution for outlying data [@fraley2002model; see an example in @crook2018bayesian].

Mathematical description of mixture models.


### Frequentist inference

The most common form of inference, traditionally, is Frequentist; specifically Maximum Likelihood (*ML*) based. In mixture models, this inference is normally implemented using Expectation-Maximisation (*EM*), a two-step iterative process. This inference is very quick but suffers from several problems:

1. Difficult to include prior beliefs or data;
2. Sensitive to initialisation, prone to finding local maxima;
3. Difficulty in converging - as the covariacne matrix has to be computed and inverted, this method often fails.

#### Expectation maximisation

Perform inference

### Bayesian inference

An alternative to the Frequentist paradigm is Bayesian inference. This relies upon a number of statements:

* Conditional probability:

\begin{eqnarray}
p(A, B | C) &=& p(B | A, C) p (A | C)
(\#eq:condProb) 
\end{eqnarray}

* The law of total probability:

\begin{eqnarray}
p(A | C) &=& \int_B p(A | B', C) p(B' | C) dB' 
(\#eq:totalProb)
\end{eqnarray}

* Bayes' Theorem (combining \@ref(eq:condProb) and \@ref(eq:totalProb)):

\begin{eqnarray}
p(A|B, C) &=& \frac{p(A, B | C)}{P(B|C)} \notag \\
&=& \frac{p(B | A, C) p (A | C)}{P(B|C)}
(\#eq:bayesTheorem) 
\end{eqnarray}

Bayesian inference is focuses upon the concept of updating one's beliefs via equation \@ref(eq:bayesTheorem). If one has a prior belief about the behaviour of some 
parameter $\theta$, captured in the distribution $p(\theta)$, and then
observe some data $X$, then one describes the change in one's belief about $\theta$ given the observed variable $X$ by:

\begin{eqnarray}
p(\theta | X) = \frac{p(X | \theta) p(\theta)}{p(X)}.
(\#eq:posteriorDistribution)
\end{eqnarray}

In this case the following terminology is used:

* $p(\theta)$ is the _prior distribution_ of $\theta$ (so-called as it encompasses one beliefs prior to observing $X$);
* $p(X)$ is the _marginal likelihood_ of $X$ as it is the likelihood with $\theta$ marginalised out; and
* $p(\theta | X)$ is the _posterior distribution_ of $\theta$ (as it denotes the belief about $\theta$ one has _a posteriori_, that is after observing empirical fact).

Analytical vs sampling based.

Pior

Likelihood

Posterior

Use language of variables and distributions.

#### MCMC

Markov-Chain Monte Carlo methods are one way of performing Bayesian inference. These methods draw samples from Markov Chains constructed such that the stationary distribution is the posterior distribution. Samples are drawn to use Monte Carlo integration to describe the posterior distribution.

* Markov chains
* Monte carlo integration
* MCMCM
* Metropolis hastings & Gibbs

#### Gibbs sampling

Consider some vector of variables $\theta = (\theta_1, \ldots, \theta_q)$ that we wish to perform inference upon. One method of implementing this is to perform _Gibbs sampling_ [@geman1984stochastic]. This works by iterating over each variable, updating it based upon the current values of all the other variables and then repeating this a large number of times (ideally an infinite number of times). 

Let $\theta^{(j)} = (\theta^{(j)}_1, \ldots, \theta^{(j)}_q)$ be the sampled values of $\theta$ in the $j^{th}$ iteration of Gibbs sampling. Our update for $\theta^{(j)}_i$ is conditioned on all the current values for the other variables - this means that the first $(i - 1)$ variables
have already been updated $j$ times, but the remaining $q - i$ variables are still based upon the
$(j - 1)^{th}$ iteration, i.e. our update probability is of the form:

\begin{eqnarray}
p(\theta^{(j)}_i | \theta^{(j)}_1, \ldots, \theta^{(j)}_{i-1}, \theta^{(j)}_{i+1}, \ldots, \theta^{(j)}_q).
(\#eq:updateProbs)
\end{eqnarray}

Consider Gibbs sampling performed upon a mixture model of $K$ components clustering observed variables $X=(x_1, \ldots, x_N)$. In this case the unobserved variables present are the $N$ allocation variables, $z=(z_1, \ldots, z_N)$, $K$ component parameters $\theta=(\theta_1, \ldots, \theta_K)$ (possibly each $\theta_k$ is a vector of parameters) and $K$ component weights $\pi=(\pi_1, \ldots, \pi_K)$. I use the notation $x_{-i}=(x_1,\ldots, x_{i-1}, x_{i+1}, \ldots, x_N)$. The hierarchical model is described in figure \@ref(fig:hierarchical-model).

```{r, hierarchical-model, echo=FALSE, engine="tikz", out.width="90%", fig.cap="The mixture model we are interested in performing inference upon.", fig.align="center", cache=T}
 \usetikzlibrary{arrows}
\usetikzlibrary{fit,positioning}
\centering
\begin{tikzpicture}[scale=.7, auto,>=latex']
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 22mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
 \node[main] (pi) {$\pi_k$ };
 \node[main] (z_i) [below right=of pi] {$z_i$};
 \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
 \node[main] (theta) [above left=of x_i] {$\theta_k$};
 \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
 \node[rectangle, inner sep=-0.5mm, fit= (z_i) (x_i),label=above right:$N$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm,draw=black!100, fill=blue! 25, fill opacity=0.2, fit= (z_i) (x_i)] {};
 \node[rectangle, inner sep=-0.8mm, fit= (theta) (pi),label=above right:$K$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm, draw=black! 100, fit= (theta)(pi)] {};
 \node[main] (pi) {$\pi_k$ };
 \node[main, fill = white!1] (z_i) [below right=of pi] {$z_i$};
 \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
 \node[main] (theta) [above left=of x_i] {$\theta_k$};
 \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
\end{tikzpicture}

```

In the case of a mixture model, often the *only* variables of interest are the unseen allocation variables, $z$. This means that we are not interested in the samples produced for the other variables.

Consider the sampling of $z_i$. As this can only hold a relatively small number of values we
can consider the probability for each possible $k$. Based upon the relationships depicted in figure \@ref(fig:hierarchical-model) and equations \@ref(eq:totalProb) and \@ref(eq:bayesTheorem):

\begin{eqnarray}
		p(z_i = k | x, z_{-i}, \pi) &=& \frac{p(z_i = k | \pi, x_{-i}, z_{-i}, \pi) p(x_i | z, x_{-i}, \pi)}{p(x_i | x_{-i}, z_{-i}, \pi)} \\
		&\propto& p(z_i = k | \pi_k) \int_{\theta} p(x_i | \theta, z, x_{-i}, \pi) p(\theta | z, x_{-i}, \pi) d \theta \\
		&=& \pi_k \int_{\theta} p(x_i | \theta) p(\theta | z, x_{-i}) d \theta
	(\#eq:ziCondProb) 
\end{eqnarray}

Note that the term in the denominator, $p(x_i | x_{-i}, z_{-i}, \pi)$ is independent of $z_i$ and thus the same for all values of $k$. The integral in \@ref(eq:ziCondProb) is the posterior predictive probability for $x_i$ given the items in the component; thus one may consider \@ref(eq:ziCondProb) as how well does each component predict $x_i$, or as a model comparison problem.

 An alternative way of describing this involves the ratio of marginal likelihoods. As we are component specific (given $z_i = k$), I drop the $z$ and $\pi$ from my conditional and assume we are referring only to the $x_j$ for which $z_j = k$.
 
\begin{eqnarray}
p(x_i | z, x_{-i}, \pi) &=& \frac{p(x | z)}{p(x_{-i} | z)} \\
&=& \frac{\int_\theta p(x | \theta) p(\theta) d\theta}{\int_\theta p(x_{-i} | \theta) p(\theta) d\theta}
\end{eqnarray}

Therefore we can write the posterior predictive distribution as this ratio of marginal likelihoods:
 
\begin{eqnarray}
p(z_i = k | x, z_{-i}, \pi) \propto \pi_k \frac{p(x)}{p(x_{-i})}
\end{eqnarray}

Thus we can create a $K$-vector of probabilities for the allocation of $x_i$ to each component by finding the ratio of marginal likelihoods for each component including and excluding $x_i$, and multiplying these by the associated component weight, $\pi_k$. One can normalise these by dividing by the sum of the members of this vector due to the independence of the normalising constant from $z_i$.

#### Example: Gaussian mixture models

In this section we derive the marginal likelihood for a component of the Gaussian mixture model assuming that the mean $\mu$ and the precision $\lambda$ are unknown. Before we can continue we state the associated probability density functions of the Normal and Gamma distributions:
	
\begin{eqnarray}
\mathcal{N}(x | \mu, \lambda^{-1}) &=& \sqrt{\frac{\lambda}{2\pi}} \exp \left(- \frac{\lambda}{2}(x - \mu) ^ 2\right) \\
Ga(x | \alpha, \mathrm{rate }= \beta) &=& \frac{\beta^\alpha}{\Gamma(\alpha)} x ^{\alpha - 1} \exp(-\beta x)
\end{eqnarray}

The model likelihood for $n$ observations is:
	
\begin{eqnarray}
p(x | \mu, \lambda) &=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \sum_{i=1}^n(x_i - \mu)^2 \right)
(\#eq:ngammaLikelihood1)
\end{eqnarray}

Considering specifically the sum within the exponent here in equation \@ref(eq:ngammaLikelihood1), and letting $\bar{x}$ be the sample mean:

\begin{eqnarray}
	\sum_{i=1}^n(x_i - \mu)^2 &=& \sum_{i=1}^n(x_i - \bar{x} + \bar{x} - \mu)^2 \\
	&=& \sum_{i=1}^n\left[(x_i - \bar{x})^2 + (\mu - \bar{x})^2 + 2(x_i \bar{x} - \bar{x}^2 - x_i \mu + \bar{x} \mu)\right] \\
	&=& n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2
\end{eqnarray}

Substituting this back into equation \@ref(eq:ngammaLikelihood1), we have:

\begin{eqnarray}
p(x | \mu, \lambda) &=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \left[n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2\right] \right) 
(\#eq:likelihood)
\end{eqnarray}
		
The conjugate prior for this model is the \emph{Normal-Gamma} distribution. This has the probability density function:
	
\begin{eqnarray}
	NG(\mu, \lambda | \mu_0, \kappa_0, \alpha_0, \beta_0) &\coloneqq& \mathcal{N}(\mu | \mu_0, (\kappa_0 \lambda)^{-1})Ga(\lambda | \alpha_0, \beta_0) \\
	&=& \sqrt{\frac{\kappa_0 \lambda}{2\pi}} \exp \left(- \frac{\kappa_0 \lambda}{2}(\mu - \mu_0) ^ 2\right) \\
	& & \hspace{3mm} \times \hspace{3mm} \frac{\beta_0 ^{\alpha_0}}{\Gamma(\alpha_0)} \lambda ^{\alpha_0 - 1} \exp(-\beta_0 \lambda) \\
	&=& \sqrt{\frac{\kappa_0}{2\pi}} \frac{\beta_0 ^ {\alpha_0}}{\Gamma(\alpha_0)} \\
	& & \hspace{3mm} \times \hspace{3mm} \lambda^{\alpha_0 - \frac{1}{2}} \exp\left(-\frac{\lambda}{2}\left[\kappa_0 (\mu - \mu_0)^2 + 2 \beta_0\right]\right) 
(\#eq:priorPDF)
\end{eqnarray}

Here the normalising constant is:
	
\begin{eqnarray}
Z_0^{-1} = \sqrt{\frac{\kappa_0}{2\pi}} \frac{\beta_0 ^ {\alpha_0}}{\Gamma(\alpha_0)}
\end{eqnarray}

One can see that this function in equation \@ref(eq:priorPDF) will naturally complement the likelihood described in equation \@ref(eq:likelihood). 

To derive the poisterior probability, we apply Bayes' theorem (equation \@ref(eq:bayesTheorem)):
	
\begin{eqnarray}
	p(\mu, \lambda | x) &\propto& p(x | \mu, \lambda) p(\mu, \lambda) \\
	&=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \left[n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2\right] \right) \\
	&& \hspace{3mm} \times \hspace{3mm} \lambda^{\alpha_0 - \frac{1}{2}} \exp\left(-\frac{\lambda}{2}\left[\kappa_0 (\mu - \mu_0)^2 + 2 \beta_0\right]\right) \\
	&\propto& \lambda^{\alpha_0 + \frac{n}{2} - \frac{1}{2}} \exp \left\{-\frac{\lambda}{2}\left[n(\mu - \bar{x})^2 + \kappa_0(\mu - \mu_0)^2 + \sum_{i=1}^n(x_i - \bar{x})^2 + 2\beta_0\right]\right\} 
	(\#eq:posteriorStep1)
\end{eqnarray}
	
We try and anticipate the parameters of our posterior. 
In Bayesian inference there is no difference between the prior and the likelihood (bar that one corresponds to observed variables and the other to unobersved variables). Thus we would expect that if we wish to combine these quantities and we have conjugacy, that the intuition for what each parameter means within the observed part of our equation should help us to have an intution for how to think about combining the likelihood with the prior distribution. Thus if we think as the prior as being based upon some previously observed data, then the $\kappa_0$ parameter would be the number of observations in this data and $\mu_0$ as the mean observed (based upon a correspondence between the likelihood and the prior). In this case we would now expect that the updated mean, $\mu_n$, should correspond to a weighted average of the sample mean of the data currently being observed ($\bar{x}$), and that in our prior ($\mu_0$). Similarly, the ``total'' number of observations between the prior data and the current data, $\kappa_n$, should be the sum of the number of samples in the dataset under observation, $n$, and the number of samples in our prior dataset, $\kappa_0$. This logic implies that including a very small $\kappa_0$ is one way of including an uninformative prior. Describing this mathematically:
	
\begin{eqnarray}
	\kappa_n &=& \kappa_0 + n \\
	\mu_n &=& \frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}
\end{eqnarray}
	
If this is the case we would expect a term in the exponent:
	
\begin{eqnarray}
	\kappa_n (\mu - \mu_n)^2 &=& (\kappa_0 + n)\left[\mu - \left(\frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}\right)\right]^2 \\
	&=& (\kappa_0 + n) \left(\mu^2 - 2\mu(\kappa_0 \mu_0 + n \bar{x}) + \left(\frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}\right)^2\right) %\\
	%&=& \kappa_0 \mu^2 + n\mu^2 - 2\mu (\kappa_0 + n) (\kappa_0 \mu_0 + n \bar{x}) - \frac{(\kappa_0\mu_0)^2 + 2n\kappa_0\mu_0 \bar{x} + (n\bar{x})^2}{\kappa_0 + n}
\end{eqnarray}

Returning to equation \@ref(eq:posteriorStep1) and considering the part of the exponent containing $\mu$:
	
\begin{eqnarray}
	\kappa_0 (\mu - \mu_0) ^ 2 + n (\mu - \bar{x}) ^2 &=& \mu^2 (\kappa_0 + n) + n\bar{x}^2 + \kappa_0 \mu_0^2 -2n\mu\bar{x} -2 \kappa_0 \mu \mu_0 \\
	&=& (\kappa_0 + n) \left[\mu^2 + \frac{n\bar{x}^2}{\kappa_0 + n} + \frac{\kappa_0 \mu_0^2}{\kappa_0 + n} - 2 \mu \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right] 
(\#eq:posteriorExponentExpansion)
\end{eqnarray}
	
Notice that several of the desired terms are present! If one focuses upon the components of this equation that do not fit the expected form:
	
\begin{eqnarray}
	\frac{n\bar{x}^2}{\kappa_0 + n} + \frac{\kappa_0 \mu_0^2}{\kappa_0 + n} &=& \frac{n(\kappa_0 + n)\bar{x}^2 + \kappa_0(\kappa_0 + n)\mu_0^2}{(\kappa_0 + n)^2} \\
	&=& \frac{n^2 \bar{x}^2 + 2\kappa_0 n \mu_0 \bar{x} + \kappa_0^ 2 \mu_0 ^ 2}{(\kappa_0 + n)^2} 2 \frac{\kappa_0 n \bar{x}^2 + \kappa_0 n\mu_0^2 - 2\kappa_0 n \mu_0 \bar{x}}{(\kappa_0 + n)^2} \\
	\\
	&=& \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right)^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{(\kappa_0 + n)^2}
\end{eqnarray}

Substituting this result into equation \@ref(eq:posteriorExponentExpansion) yields:
	
\begin{eqnarray}
\kappa_0 (\mu - \mu_0) ^ 2 + n (\mu - \bar{x}) ^2 &=& (\kappa_0 + n) \left[\mu + \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right]^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{\kappa_0 + n}
\end{eqnarray}

Returning to equation \@ref(eq:posteriorStep1):

\begin{eqnarray}
	p(\mu, \lambda | x) &\propto& \lambda^{\alpha_0 + \frac{n}{2} - \frac{1}{2}} \exp \left\{-\frac{\lambda}{2}\left[
	(\kappa_0 + n) \left(\mu + \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right)^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{\kappa_0 + n} + \sum_{i=1}^n(x_i - \bar{x})^2 + 2\beta_0\right]\right\} 
	(\#eq:posterior)
\end{eqnarray}

This is the probability density function of a Normal-Gamma distribution.

\begin{eqnarray}
	p(\mu, \lambda| x) &=& NG(\mu, \lambda | \mu_n, \kappa_n, \alpha_n, \beta_n) \\
	\mu_n &=& \frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n} \\
	\kappa_n &=& \kappa_0 + n \\
	\alpha_n &=& \alpha_0 + \frac{n}{2} \\
	\beta_n &=& \beta_0 + \frac{1}{2}\sum_{i=1}^n(x_i - \bar{x})^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{2(\kappa_0 + n)}
	\end{eqnarray}
	
As this posterior distribution is $NG(\mu_n,\kappa_n, \alpha_n, \beta_n)$, we know that the associated normalising constant is:

\begin{eqnarray}
Z_n &=& \frac{\Gamma(\alpha_n)}{\beta_n^{\alpha_n}}\sqrt{\frac{2\pi}{\kappa_n}}.
\end{eqnarray}

To derive the _Marginal Likelihood_, consider the posterior but track the normalising constants (such as the $(2\pi)^{-n/2}$ in the likelihood). Denote the prior, likelihood and posterior less their normalising constants by $p'(\mu, \lambda)$, $p(x | \mu, \lambda)$ and $p'(\mu, \lambda | x)$ respectively:

\begin{eqnarray}
\frac{1}{Z_n} p'(\mu, \lambda | x) &=& \frac{1}{p(x)}\frac{1}{Z_0} p'(\mu, \lambda)\left(\frac{1}{2\pi}\right)^{\frac{n}{2}} p'(x | \mu, \lambda)
\end{eqnarray}

We know that the product of the unnormalised prior and the unnormalised likelihood give the right hand side of equation \@ref(eq:posterior), and that this is our unnormalised posterior, $p'(\mu, \lambda | x)$. Thus:
	
\begin{eqnarray}
\frac{1}{Z_n} p'(\mu, \lambda | x) &=& \frac{1}{p(x)}\frac{1}{Z_0} \left(\frac{1}{2\pi}\right)^{\frac{n}{2}} p'(\mu, \lambda | x) \\
\implies p(x) &=& \frac{Z_n}{Z_0}(2\pi)^{-\frac{n}{2}} 
\end{eqnarray}

Thus our marginal likelihood is the ratio of the posterior normalising constants to the product of those of the likelihood and prior. Expanding this we finally have:
	
\begin{eqnarray}
	p(x) &=& \frac{\Gamma(\alpha_n)}{\beta_n^{\alpha_n}} \sqrt{\frac{2\pi}{\kappa_n}} \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \sqrt{\frac{\kappa_0}{2\pi}} (2\pi)^{-\frac{n}{2}} \\
	&=& \frac{\Gamma(\alpha_n)}{\Gamma(\alpha_0)}\frac{\beta_0^{\alpha_0}}{\beta_n^{\alpha_n}} \sqrt{\frac{\kappa_0}{\kappa_n}}(2\pi)^{-\frac{n}{2}}
\end{eqnarray}

#### Dirichlet process

Dirichlet process [@ferguson1973bayesian]

Multinomical dirichlet allocation model

Current Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference in Dirichlet process (DP) mixture models (see e.g. @neal2000markov; @jain2004split) are computationally costly, and often infeasible for large datasets [@crook2019fast].
	
#### Variational inference

@blei2006variational and @blei2017variational

## Consensus clustering 

@monti2003consensus and @wilkerson2010consensusclusterplus

## Integrative clustering

sequential vs post-hoc vs simultaneous, local vs global

### iCluster
@shen2009integrative and @mo2018fully

### Bayesian integrative clustering


#### Multiple Dataset Integration

@kirk2012bayesian and @mason2016mdi

#### Clusternomics

@gabasova2017clusternomics

#### Bayesian Consensus Clustering

@lock2013bayesian

#### MOFA

@argelaguet2019mofaplus and @argelaguet2018multi

### COCA

@cancer2012comprehensive

#### KLIC

@cabassi2019multiple

## Problems