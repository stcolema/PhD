# Theory {#introduction}

* Clustering
  * Ill-posed problem
  * Mixture models
    * Frequentist 
      * EM
    * Bayesian
      * Gibbs sampling
  * Consensus clustering
  * Integrative clustering
    * Integrative Bayesian clustering 
* Problems

In this section I explain some of the reasons one may wish to do a cluster 
analysis. I then explain what mixture models are and how Bayesian and 
Frequentist inference of these models works. Following from this I introduce the
concept of integrative clustering as an extension of cluster analysis and give 
some examples of relevant methods and the results they have delivered. This 
background is necessary to understand some of the problems that currently exist, 
and why I am suggesting Consensus inference as a solution to these.

## Cluster analysis
Cluster analysis – also known as unsupervised learning – is used in multivariate
statistics to uncover useful latent groups suspected in the data or to discover 
interesting groups of homogeneous observations through partitioning of the data.
These groups are referred to as clusters. In large datasets, such as modern 
'omics datasets, estimation of clusters is often necessary for improved 
understanding and interpretation [@meinshausen2010stability]. 

### Ill-posed problem
Clustering is an ill-posed problem. How is one to formally define what is meant by 
"useful" or "interesting"? How does one choose which clustering solution is 
"best" or most "natural"? The answer to these questions is inherently domain or
dataset specific. Certain qualities might be generally desireable, for example 
it is often ideal that the clusters are as dissimilar as possible and that the 
observations within the same cluster are as similar as possible, however the 
broader statement of "best solution" is not generic; consider that the
definition of useful (and hence the best solution) when clustering genes in
patients with Type 1 Diabetes is unlikely to be relevant when attempting to 
define animal taxonomies. 

There have been efforts made to address this question of "best solution", and
remove the subjectivity present in cluster analysis. However, these often rely
upon ranking of solutions by some "objective" score. This merely shifts the 
subjectivity to choice of score itself!

> "For every score preferring one clustering over the other one can invent 
another score which does the opposite. A unique, global, objective score for all
clustering problems does not exist."
`r tufte::quote_footer('@von2012clustering')`

This is not to dismiss the different measures of cluster analysis; these scores
can contribute useful information, but one should avoid overstating their value.

From this is may be realised that there does not exist a single unique
clustering solution; different aims imply different solutions [@hennig2015true].
Thus, thinking clearly about the aims and making an explicit statement of these
is integral to good practice of cluster analysis. Having stated one's aims, one
may avoid the problem of defining a generic "best" or a "natural" solution, one
may instead focus on one's specific problem. 

> "The nature of the classification that we make . . . must have direct regard 
to the purpose for which the classification is required. In as far as it serves
the purpose, the classification is a good classification, however ‘artificial’ 
it may be. In as far as it does not serve this purpose, it is a bad 
classification, however ‘natural’ it may be." 
`r tufte::quote_footer('@mercier1912formal')`

The purpose of the analysis is inherently subjective; whether this subjectivity
enters the analysis through the analyst, the domain in question or the 
combination of these is immaterial - it is always present. A clear analysis
plan stated before the data is seen (i.e. explicit aims, model choice, data
pre-processing, choice of priors, etc.), helps to alleviate some problems 
associated with subjecitivty, improving the recreatibility and significance of
a cluster analysis. In practice, however, there are many decisions that will not 
be forseeable. The large degrees of freedom a researcher has in making decisions
in the course of the analysis is problematic. It contributes to the importance 
of avoiding exagerrating the meaning of the clusters found (the logic here is 
based upon that described by @gelman2013garden, but applied to cluster 
analysis). This does not mean the analysis is not useful or does not have 
implications beyond the dataset analysed, but a measure of the broader 
significance of results, often an aim in statistical analyses, is not possible.

## Mixture models

A way 

### Frequentist inference

F

#### Expectation maximisation

Perform inference

### Bayesian inference

Analytical vs sampling based.

Pior

Likelihood

Posterior

Use language of variables and distributions.

#### MCMC

Markov-Chain Monte Carlo methods are one way of performing Bayesian inference. These methods draw samples from Markov Chains constructed such that the stationary distribution is the posterior distribution. Samples are drawn to use Monte Carlo integration to describe the posterior distribution.

* Markov chains
* Monte carlo integration
* MCMCM
* Metropolis hastings & Gibbs

#### Gibbs sampling

Consider some vector of variables $\theta = (\theta_1, \ldots, \theta_q)$ that we wish to perform inference upon. One method of implementing this is to perform _Gibbs sampling_ [@geman1984stochastic]. This works by iterating over each variable, updating it based upon the current values of all the other variables and then repeating this a large number of times (ideally an infinite number of times). 

Let $\theta^{(j)} = (\theta^{(j)}_1, \ldots, \theta^{(j)}_q)$ be the sampled values of $\theta$ in the $j^{th}$ iteration of Gibbs sampling. Our update for $\theta^{(j)}_i$ is conditioned on all the current values for the other variables - this means that the first $(i - 1)$ variables
have already been updated $j$ times, but the remaining $q - i$ variables are still based upon the
$(j - 1)^{th}$ iteration, i.e. our update probability is of the form:

\begin{eqnarray}
p(\theta^{(j)}_i | \theta^{(j)}_1, \ldots, \theta^{(j)}_{i-1}, \theta^{(j)}_{i+1}, \ldots, \theta^{(j)}_q).
(\#eq:updateProbs)
\end{eqnarray}

Consider Gibbs sampling performed upon a mixture model of $K$ components clustering observed variables $X=(x_1, \ldots, x_N)$. In this case the unobserved variables present are the $N$ allocation variables, $z=(z_1, \ldots, z_N)$, $K$ component parameters $\theta=(\theta_1, \ldots, \theta_K)$ (possibly each $\theta_k$ is a vector of parameters) and $K$ component weights $\pi=(\pi_1, \ldots, \pi_K)$. I use the notation $x_{-i}=(x_1,\ldots, x_{i-1}, x_{i+1}, \ldots, x_N)$. The hierarchical model is described in figure \@ref(fig:hierarchical-model).

```{r, hierarchical-model, echo=FALSE, engine="tikz", out.width="90%", fig.cap="The mixture model we are interested in performing inference upon.", fig.align="center", cache=T}
 \usetikzlibrary{arrows}
\usetikzlibrary{fit,positioning}
\centering
\begin{tikzpicture}[scale=.7, auto,>=latex']
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 22mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
  \node[main] (pi) {$\pi_k$ };
  \node[main] (z_i) [below right=of pi] {$z_i$};
  \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
  \node[main] (theta) [above left=of x_i] {$\theta_k$};
  \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
  \node[rectangle, inner sep=-0.5mm, fit= (z_i) (x_i),label=above right:$N$, xshift=14mm] {};
  \node[rectangle, inner sep=4.8mm,draw=black!100, fill=blue! 25, fill opacity=0.2, fit= (z_i) (x_i)] {};
 \node[rectangle, inner sep=-0.8mm, fit= (theta) (pi),label=above right:$K$, xshift=14mm] {};
  \node[rectangle, inner sep=4.8mm, draw=black! 100, fit= (theta)(pi)] {};
  \node[main] (pi) {$\pi_k$ };
  \node[main, fill = white!1] (z_i) [below right=of pi] {$z_i$};
  \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
  \node[main] (theta) [above left=of x_i] {$\theta_k$};
  \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
\end{tikzpicture}

```

Before I begin the derivations, please recall:

* Conditional probability:

\begin{eqnarray}
p(A, B | C) &=& p(B | A, C) p (A | C)
(\#eq:condProb) 
\end{eqnarray}

* The law of total probability:

\begin{eqnarray}
p(A | C) &=& \int_B p(A | B', C) p(B' | C) dB' 
(\#eq:totalProb)
\end{eqnarray}

* Bayes' Theorem (combining \@ref(eq:condProb) and \@ref(eq:totalProb)):

\begin{eqnarray}
p(A|B, C) &=& \frac{p(A, B | C)}{P(B|C)} \notag \\
&=& \frac{p(B | A, C) p (A | C)}{P(B|C)}
(\#eq:bayesTheorem) 
\end{eqnarray}

In the case of a mixture model, often the *only* variables of interest are the unseen allocation variables, $z$. This means that we are not interested in the samples produced for the other variables.

Consider the sampling of $z_i$. As this can only hold a relatively small number of values we
can consider the probability for each possible $k$. Based upon the relationships depicted in figure \@ref(fig:hierarchical-model) and equations \@ref(eq:totalProb) and \@ref(eq:bayesTheorem):

\begin{eqnarray}
		p(z_i = k | x, z_{-i}, \pi) &=& \frac{p(z_i = k | \pi, x_{-i}, z_{-i}, \pi) p(x_i | z, x_{-i}, \pi)}{p(x_i | x_{-i}, z_{-i}, \pi)} \\
		&\propto& p(z_i = k | \pi_k) \int_{\theta} p(x_i | \theta,  z, x_{-i}, \pi) p(\theta | z, x_{-i}, \pi) d \theta \\
		&=& \pi_k \int_{\theta} p(x_i | \theta) p(\theta | z, x_{-i}) d \theta
	(\#eq:ziCondProb) 
\end{eqnarray}

Note that the term in the denominator, $p(x_i | x_{-i}, z_{-i}, \pi)$ is independent of $z_i$ and thus the same for all values of $k$. The integral in \@ref(eq:ziCondProb) is the posterior predictive probability for $x_i$ given the items in the component;  thus one may consider \@ref(eq:ziCondProb) as how well does each component predict $x_i$, or as a model comparison problem.

 An alternative way of describing this involves the ratio of marginal likelihoods. As we are component specific (given $z_i = k$), I drop the $z$ and $\pi$ from my conditional and assume we are referring only to the $x_j$ for which $z_j = k$.
    
\begin{eqnarray}
p(x_i | z, x_{-i}, \pi) &=& \frac{p(x | z)}{p(x_{-i} | z)} \\
&=& \frac{\int_\theta p(x | \theta) p(\theta) d\theta}{\int_\theta p(x_{-i} | \theta) p(\theta) d\theta}
\end{eqnarray}

Therefore we can write the posterior predictive distribution as this ratio of marginal likelihoods:
    
\begin{eqnarray}
p(z_i = k | x, z_{-i}, \pi) \propto \pi_k \frac{p(x)}{p(x_{-i})}
\end{eqnarray}

Thus we can create a $K$-vector of probabilities for the allocation of $x_i$ to each component by finding the ratio of marginal likelihoods for each component including and excluding $x_i$, and multiplying these by the associated component weight, $\pi_k$. One can normalise these by dividing by the sum of the members of this vector due to the independence of the normalising constant from $z_i$.

#### Example: Gaussian mixture models

In this section we derive the marginal likelihood for a component of the Gaussian mixture model assuming that the mean $\mu$ and the precision $\lambda$ are unknown. Before we can continue we state the associated probability density functions of the Normal and Gamma distributions:
	
\begin{eqnarray}
\mathcal{N}(x | \mu, \lambda^{-1}) &=& \sqrt{\frac{\lambda}{2\pi}} \exp \left(- \frac{\lambda}{2}(x - \mu) ^ 2\right) \\
Ga(x | \alpha, \mathrm{rate }= \beta) &=& \frac{\beta^\alpha}{\Gamma(\alpha)} x ^{\alpha - 1} \exp(-\beta x)
\end{eqnarray}

The model likelihood for $n$ observations is:
	
\begin{eqnarray}
p(x | \mu, \lambda) &=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \sum_{i=1}^n(x_i - \mu)^2 \right)
(\#eq:ngammaLikelihood1)
\end{eqnarray}

Considering specifically the sum within the exponent here in equation \@ref(eq:ngammaLikelihood1), and letting $\bar{x}$ be the sample mean:

\begin{eqnarray}
	\sum_{i=1}^n(x_i - \mu)^2 &=& \sum_{i=1}^n(x_i - \bar{x} + \bar{x} - \mu)^2 \\
	&=& \sum_{i=1}^n\left[(x_i - \bar{x})^2 + (\mu - \bar{x})^2 + 2(x_i \bar{x} - \bar{x}^2 - x_i \mu + \bar{x} \mu)\right] \\
	&=& n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2
\end{eqnarray}

Substituting this back into equation \@ref(eq:ngammaLikelihood1), we have:

\begin{eqnarray}
p(x | \mu, \lambda) &=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \left[n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2\right] \right) 
(\#eq:likelihood)
\end{eqnarray}
		
The conjugate prior for this model is the \emph{Normal-Gamma} distribution. This has the probability density function:
	
\begin{eqnarray}
	NG(\mu, \lambda | \mu_0, \kappa_0, \alpha_0, \beta_0) &\coloneqq& \mathcal{N}(\mu | \mu_0, (\kappa_0 \lambda)^{-1})Ga(\lambda | \alpha_0, \beta_0) \\
	&=&  \sqrt{\frac{\kappa_0 \lambda}{2\pi}} \exp \left(- \frac{\kappa_0 \lambda}{2}(\mu - \mu_0) ^ 2\right) \\
	& & \hspace{3mm} \times \hspace{3mm} \frac{\beta_0 ^{\alpha_0}}{\Gamma(\alpha_0)} \lambda ^{\alpha_0 - 1} \exp(-\beta_0 \lambda) \\
	&=& \sqrt{\frac{\kappa_0}{2\pi}} \frac{\beta_0 ^ {\alpha_0}}{\Gamma(\alpha_0)} \\
	& & \hspace{3mm} \times \hspace{3mm}  \lambda^{\alpha_0 - \frac{1}{2}} \exp\left(-\frac{\lambda}{2}\left[\kappa_0 (\mu - \mu_0)^2 + 2 \beta_0\right]\right) 
(\#eq:priorPDF)
\end{eqnarray}

Here the normalising constant is:
	
\begin{eqnarray}
Z_0^{-1} = \sqrt{\frac{\kappa_0}{2\pi}} \frac{\beta_0 ^ {\alpha_0}}{\Gamma(\alpha_0)}
\end{eqnarray}

One can see that this function in equation \@ref(eq:priorPDF) will naturally complement the likelihood described in equation \@ref(eq:likelihood).  

To derive the poisterior probability, we apply Bayes' theorem (equation \@ref(eq:bayesTheorem)):
	
\begin{eqnarray}
	p(\mu, \lambda | x) &\propto& p(x | \mu, \lambda) p(\mu, \lambda) \\
	&=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \left[n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2\right] \right) \\
	&& \hspace{3mm} \times \hspace{3mm} \lambda^{\alpha_0 - \frac{1}{2}} \exp\left(-\frac{\lambda}{2}\left[\kappa_0 (\mu - \mu_0)^2 + 2 \beta_0\right]\right) \\
	&\propto&  \lambda^{\alpha_0 + \frac{n}{2} - \frac{1}{2}} \exp \left\{-\frac{\lambda}{2}\left[n(\mu - \bar{x})^2 + \kappa_0(\mu - \mu_0)^2 + \sum_{i=1}^n(x_i - \bar{x})^2 + 2\beta_0\right]\right\} 
	(\#eq:posteriorStep1)
\end{eqnarray}
	
We try and anticipate the parameters of our posterior. 
In Bayesian inference there is no difference between the prior and the likelihood (bar that one corresponds to observed variables and the other to unobersved variables). Thus we would expect that if we wish to combine these quantities and we have conjugacy, that the intuition for what each parameter means within the observed part of our equation should help us to have an intution for how to think about combining the likelihood with the prior distribution. Thus if we think as the prior as being based upon some previously observed data, then the $\kappa_0$ parameter would be  the number of observations in this data and $\mu_0$ as the mean observed (based upon a correspondence between the likelihood and the prior). In this case we would now expect that the updated mean, $\mu_n$, should correspond to a weighted average of the sample mean of the data currently being observed ($\bar{x}$), and that in our prior ($\mu_0$). Similarly, the ``total'' number of observations  between the prior data and the current data, $\kappa_n$, should be the sum of the number of samples in the dataset under observation, $n$, and the number of samples in our prior dataset, $\kappa_0$. This logic implies that including a very small $\kappa_0$ is one way of including an uninformative prior. Describing this mathematically:
	
\begin{eqnarray}
	\kappa_n &=& \kappa_0 + n \\
	\mu_n &=& \frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}
\end{eqnarray}
	
If this is the case we would expect a term in the exponent:
	
\begin{eqnarray}
	\kappa_n (\mu - \mu_n)^2 &=& (\kappa_0 + n)\left[\mu - \left(\frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}\right)\right]^2 \\
	&=& (\kappa_0 + n) \left(\mu^2 - 2\mu(\kappa_0 \mu_0 + n \bar{x}) + \left(\frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}\right)^2\right) %\\
	%&=& \kappa_0 \mu^2 + n\mu^2 - 2\mu (\kappa_0 + n) (\kappa_0 \mu_0 + n \bar{x}) - \frac{(\kappa_0\mu_0)^2 + 2n\kappa_0\mu_0 \bar{x} + (n\bar{x})^2}{\kappa_0 + n}
\end{eqnarray}

Returning to equation \@ref(eq:posteriorStep1) and considering the part of the exponent containing $\mu$:
	
\begin{eqnarray}
	\kappa_0 (\mu - \mu_0) ^ 2 + n (\mu - \bar{x}) ^2 &=& \mu^2 (\kappa_0 + n) + n\bar{x}^2 + \kappa_0 \mu_0^2 -2n\mu\bar{x} -2 \kappa_0 \mu \mu_0 \\
	&=& (\kappa_0 + n) \left[\mu^2 + \frac{n\bar{x}^2}{\kappa_0 + n} + \frac{\kappa_0 \mu_0^2}{\kappa_0 + n} - 2 \mu \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right] 
(\#eq:posteriorExponentExpansion)
\end{eqnarray}
	
Notice that several of the desired terms are present! If one focuses upon the components of this equation that do not fit the expected form:
	
\begin{eqnarray}
	\frac{n\bar{x}^2}{\kappa_0 + n} + \frac{\kappa_0 \mu_0^2}{\kappa_0 + n} &=& \frac{n(\kappa_0 + n)\bar{x}^2 + \kappa_0(\kappa_0 + n)\mu_0^2}{(\kappa_0 + n)^2} \\
	&=& \frac{n^2 \bar{x}^2 + 2\kappa_0 n \mu_0 \bar{x} + \kappa_0^ 2 \mu_0 ^ 2}{(\kappa_0 + n)^2} 2 \frac{\kappa_0 n \bar{x}^2 + \kappa_0 n\mu_0^2 - 2\kappa_0 n \mu_0 \bar{x}}{(\kappa_0 + n)^2} \\
	\\
	&=& \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right)^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{(\kappa_0 + n)^2}
\end{eqnarray}

Substituting this result into equation \@ref(eq:posteriorExponentExpansion) yields:
	
\begin{eqnarray}
\kappa_0 (\mu - \mu_0) ^ 2 + n (\mu - \bar{x}) ^2 &=& (\kappa_0 + n) \left[\mu + \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right]^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{\kappa_0 + n}
\end{eqnarray}

Returning to equation \@ref(eq:posteriorStep1):

\begin{eqnarray}
	p(\mu, \lambda | x) &\propto& \lambda^{\alpha_0 + \frac{n}{2} - \frac{1}{2}} \exp \left\{-\frac{\lambda}{2}\left[
	(\kappa_0 + n) \left(\mu + \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right)^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{\kappa_0 + n} + \sum_{i=1}^n(x_i - \bar{x})^2 + 2\beta_0\right]\right\} 
	(\#eq:posterior)
\end{eqnarray}

This is the probability density function of a Normal-Gamma distribution.

\begin{eqnarray}
	p(\mu, \lambda| x) &=& NG(\mu, \lambda | \mu_n, \kappa_n, \alpha_n, \beta_n) \\
	\mu_n &=& \frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n} \\
	\kappa_n &=& \kappa_0 + n \\
	\alpha_n &=& \alpha_0 + \frac{n}{2} \\
	\beta_n &=& \beta_0 + \frac{1}{2}\sum_{i=1}^n(x_i - \bar{x})^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{2(\kappa_0 + n)}
	\end{eqnarray}
	
As this posterior distribution is $NG(\mu_n,\kappa_n, \alpha_n, \beta_n)$, we know that the associated normalising constant is:

\begin{eqnarray}
Z_n &=& \frac{\Gamma(\alpha_n)}{\beta_n^{\alpha_n}}\sqrt{\frac{2\pi}{\kappa_n}}.
\end{eqnarray}

To derive the _Marginal Likelihood_, consider the posterior but track the normalising constants (such as the $(2\pi)^{-n/2}$ in the likelihood). Denote the prior, likelihood and posterior less their normalising constants by $p'(\mu, \lambda)$, $p(x | \mu, \lambda)$ and $p'(\mu, \lambda | x)$ respectively:

\begin{eqnarray}
\frac{1}{Z_n} p'(\mu, \lambda | x) &=& \frac{1}{p(x)}\frac{1}{Z_0} p'(\mu, \lambda)\left(\frac{1}{2\pi}\right)^{\frac{n}{2}} p'(x | \mu, \lambda)
\end{eqnarray}

We know that the product of the unnormalised prior and the unnormalised likelihood give the right hand side of equation \@ref(eq:posterior), and that this is our unnormalised posterior, $p'(\mu, \lambda | x)$. Thus:
	
\begin{eqnarray}
\frac{1}{Z_n} p'(\mu, \lambda | x) &=& \frac{1}{p(x)}\frac{1}{Z_0} \left(\frac{1}{2\pi}\right)^{\frac{n}{2}} p'(\mu, \lambda | x) \\
\implies p(x) &=& \frac{Z_n}{Z_0}(2\pi)^{-\frac{n}{2}} 
\end{eqnarray}

Thus our marginal likelihood is the ratio of the posterior normalising constants to the product of those of the likelihood and prior. Expanding this we finally have:
	
\begin{eqnarray}
	p(x) &=& \frac{\Gamma(\alpha_n)}{\beta_n^{\alpha_n}} \sqrt{\frac{2\pi}{\kappa_n}} \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \sqrt{\frac{\kappa_0}{2\pi}} (2\pi)^{-\frac{n}{2}} \\
	&=& \frac{\Gamma(\alpha_n)}{\Gamma(\alpha_0)}\frac{\beta_0^{\alpha_0}}{\beta_n^{\alpha_n}} \sqrt{\frac{\kappa_0}{\kappa_n}}(2\pi)^{-\frac{n}{2}}
\end{eqnarray}

#### Dirichlet process

Dirichlet process [@ferguson1973bayesian]

Multinomical dirichlet allocation model

Current Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference in Dirichlet process (DP) mixture models (see e.g. @neal2000markov; @jain2004split) are computationally costly, and often infeasible for large datasets [@crook2019fast].
	
#### Variational inference

@blei2006variational and @blei2017variational

## Consensus clustering 

@monti2003consensus and @wilkerson2010consensusclusterplus

## Integrative clustering

sequential vs post-hoc vs simultaneous, local vs global

### iCluster
@shen2009integrative and @mo2018fully

### Bayesian integrative clustering


#### Multiple Dataset Integration

@kirk2012bayesian and @mason2016mdi

#### Clusternomics

@gabasova2017clusternomics

#### Bayesian Consensus Clustering

@lock2013bayesian

#### MOFA

@argelaguet2019mofaplus and @argelaguet2018multi

### COCA

@cancer2012comprehensive

#### KLIC

@cabassi2019multiple

## Problems