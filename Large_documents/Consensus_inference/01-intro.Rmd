# Theory {#introduction}

* Clustering
 * Mixture models
 * Frequentist 
 * EM
 * Bayesian
 * Gibbs sampling
 * Consensus clustering
 * Integrative clustering
 * Integrative Bayesian clustering 
* Problems

In this section I explain some of the reasons one may wish to do a cluster 
analysis. I then explain what mixture models are and how Bayesian and 
Frequentist inference of these models works. Following from this I introduce the
concept of integrative clustering as an extension of cluster analysis and give 
some examples of relevant methods and the results they have delivered. This 
background is necessary to understand some of the problems that currently exist, 
and why I am suggesting Consensus inference as a solution to these.

## Cluster analysis {#clusterAnalysis}

Cluster analysis – also known as unsupervised learning – consists of defining
partitions of the items within a dataset such that the resulting sets are 
homogeneous and distinct from one another. These groups are known as clusters. 
Clustering is used in multivariate statistics to uncover _useful_ latent groups
suspected in the data or to discover _interesting_ groups of homogeneous 
observations. In large datasets, such as modern 'omics datasets, estimation of
clusters is often necessary for improved understanding and interpretation [@meinshausen2010stability] - reducing a complex, $P$-dimensional dataset to 
a 1-dimensional, discrete variable can significantly aid interpretation.

```{definition, clustering, name="Clustering"}
If one has some collection of data $X=\left(x_1,\ldots,x_N\right)$, let a _clustering_ or partition of the data be defined by:

\begin{align}
	Y &=& \left\{Y_1,\ldots,Y_K\right\} \\
	Y_k &=& \left\{x_i : c_i = k \right\}  \\
	Y_i \cap Y_j &=& \emptyset \hspace{4 pt} \forall \hspace{4 pt} i,j \in \{1,\ldots,K\},  i \neq j \\
	n_k & = & \mid Y_k \mid \hspace{4 pt} \geq 1 \hspace{4 pt} \forall \hspace{4 pt} k \in \{1,\ldots,K\} \\
	\sum_{k=1}^Kn_k &=& n
\end{align}
	
In short there are $K$ non-empty disjoint sets of data, each of which is referred to as a _cluster_, the set of which form a _clustering_. This clustering may be described by a collection of allocation labels, $c=(c_1, \ldots, c_N)$. For an item $x_i$, the label $c_i=k$ states that point $x_i$ is assigned to cluster $Y_k$.

```

Cluster analysis has a myriad of applications. These include data mining, 
which started from the search for groupings of customers and products in massive
retail datasets [@fraley2002model]; document clustering and the analysis of 
internet use data; spatial proteomics [@crook2018bayesian] where subcellular
localisation of proteins is predicted by clustering-based methods; and image
analysis, where clustering is used for image segmentation and quantization. The
area of interest for me is in clustering 'omics data to 
improve understanding of disease aetiology and (ideally) help clinical practice
through identifying driving biological mechanisms or in defining subtypes within
given diseases. Defining subtypes based upon behaviour at the 
subcellular level rather than on phenotypic patterns can enable more effective 
treatment. Examples where cluster analysis has been used to propose new subtypes
include such a diverse range of diseases as breast cancer [@cancer2012comprehensive; @berger2018comprehensive], Coronary artery disease [@guo2017cluster], tinnitus [@van2017cluster] and asthma [@ortega2014cluster]. Another interesting 
application is the defining of subpopulations of patients; this application
is integral to _Precision medicine_, which relies upon "the classification of 
people into subpopulations using their common genetic patterns, lifestyles, 
drug responses, and environmental and cultural factors" [@gameiro2018precision].

Within cluster analysis there exists a large number of possible methods. 
Traditional approaches [as taught in many textbooks, see for e.g. 
@fidell2001using; @manly2016multivariate; @stevens2012applied] include 
hierarchical and non-hierarchical (normally $k$-means) clustering. These
methods are heuristic in nature; they are not based upon a formal model that 
explicitly models the data-generating process and what guidance there is for 
solving important practical questions that arise in every cluster analysis (
such as the number of clusters present, the measure of distance / similarity
between points, which solution is optimal, etc.) is subjective and informal. 
This has consequences for the interpretation of the results, as evaluation of
the significance of results is difficult [@monti2003consensus].

Despite this, most of the clustering that is done in practise is based upon these 
methods; they are intuitively reasonable and, perhaps more importantly, most 
clustering methods available in commercial software are of this type 
[@fraley2002model]. However, I am interested in _model-based cluster analysis_. 
This embeds the cluster analysis within a statistical framework and offers a
principled approach to many issues associated with cluster analysis. 
Speicifcally, my interest is in _mixture models_ where each subpopulation is 
described within the model by a probability distribution. In this setting, the
problems of determining the number of clusters and of choosing an appropriate 
clustering method can be recast as statistical model choice problems, and 
models that differ in numbers of components and/or in component distributions 
can be compared. Outliers are handled by adding one or more components representing a different distribution for outlying data [@fraley2002model; see an example in @crook2018bayesian].

```{definition, mixtureModels, name="Finite mixture models"}

If one is given some data $X = (x_1, \ldots, x_N)$, we assume $K$ unobserved 
subpopulations generate the data and that insights into these sub-populations 
can be revealed by imposing a clustering $Y = \left\{Y_1,\ldots,Y_K\right\}$ on
the data. It is assumed that each of the $K$ clusters can be modelled by a 
parametric distribution, $f_k(\cdot)$ with parameters $\theta_k$. Normally a
common distribution, $f(\cdot)$, is assumed across all components. We let 
membership in the $k^{th}$ cluster for the $i^{th}$ individual be denoted by 
$c_i = k$. The full model density is then the weighted sum of the probability 
density functions where the weights, $\pi_k$, are the proportion of the total 
population assigned to the $k^{th}$ cluster. Then for item $x_i$ in a _finite 
mixture model_:

\begin{align}
	p(x_i|c_i = k) &= \pi_k f(x_i | \theta_k) \\
	p(x_i) &= \sum_{k=1}^K \pi_k f(x_i | \theta_k)
\end{align}

```

The flexibility in choice of $f(\cdot)$ means that these models can be applied in many different scenarios. 

<!-- The unobserved variable $c$ is (normally) the object we are most interested in performing inference upon. Thus clustering can be seen as a _latent variable analysis_. -->

### Frequentist inference

The most common form of inference, traditionally, is Frequentist; specifically Maximum Likelihood (*ML*) based. In mixture models, the model log-likelihood is:

\begin{align}
l(\pi, \theta)=\sum_{i=1}^N \ln p(x_i | \theta, \pi)
\end{align}

The likelihood equations based upon the derivative of this object with respect to the parameters $(\theta, \pi)$ are too complicated for the normal methods of maximisation [@stahl2012model] and thus inference is normally performed using Expectation-Maximisation (*EM*), a two-step iterative process. 

This inference is very quick but suffers from several problems:

1. Difficult to combine multiple sources of information [see @singh2005combining for an  example of how to consider this under the Frequentist paradigm];
2. Sensitive to initialisation, prone to finding local maxima due to the nature of the EM algorithm - normally these models are run from multiple diffferent initialisations. In this case observing the same log‐likelihood values from multiple starting points increases confidence that the solution is a global maximum. Using a some heuristic clustering method to initialise the method [such as in @mclust2016] also offers stability; and
3. Difficulty in converging - singularities can occur in the likelihood function; these are points where the likelihood becomes infinite, resulting in degenerate distributions. Singularities occur when the number of parameters to be estimated is large in relation to the sample size [@stahl2012model], models with unrestrained covariances and large numbers of components are prone to this problem. @fraley2007bayesian suggest using Bayesian inference as a means of overcoming this issue, reccomending use of a _maximum a posteriori_ (*MAP*) estimate instead of an ML estimate.

#### Expectation maximisation

Perform inference

### Bayesian inference

An alternative to the Frequentist paradigm is Bayesian inference. This relies upon a number of statements:

```{definition, condProb, name = "Conditional probability"}

For events $A$, $B$ and $C$:

\begin{eqnarray}
p(A, B | C) &=& p(B | A, C) p (A | C).
(\#eq:condProb) 
\end{eqnarray}

```

```{definition, totalProb, name = "Law of total probability"}

For a countable partition of the sample space $\mathcal{B}$, $\{B_q\}_{q=1}^Q$, and events $A$ and $C$:
  
\begin{eqnarray}
p(A | C) &=& \sum_{q=1}^Q p(A | B_q, C) p(B_q | C).
\end{eqnarray}

If it is the case that each member of the partition has a measure of 0 (i.e. the partition is uncountable), then:


\begin{eqnarray}
p(A | C) &=& \int_{\mathcal{B}} p(A | B_q, C) p(B_q | C) dB_q.
(\#eq:totalProb)
\end{eqnarray}

```

```{definition, bayesTheorem, name = "Bayes' Theorem"}

From definition \@ref(def:condProb):

\begin{eqnarray}
p(A|B, C) &=& \frac{p(A, B | C)}{P(B|C)} \\
&=& \frac{p(B | A, C) p (A | C)}{P(B|C)} 
(\#eq:bayesTheorem)
\end{eqnarray}

Now applying definition \@ref(def:totalProb):

\begin{eqnarray}
\frac{p(B | A, C) p (A | C)}{P(B|C)}  &=& \frac{p(B | A, C) p (A | C)}{\int_A P(B|A', C) dA'}
(\#eq:bayesTheoremIntegral)
\end{eqnarray}

```

<!-- * Bayes' Theorem (combining \@ref(eq:condProb) and \@ref(eq:totalProb)): -->

<!-- \begin{eqnarray} -->
<!-- p(A|B, C) &=& \frac{p(A, B | C)}{P(B|C)} \notag \\ -->
<!-- &=& \frac{p(B | A, C) p (A | C)}{P(B|C)} -->
<!-- (\#eq:bayesTheorem) \\ -->
<!-- &=& \frac{p(B | A, C) p (A | C)}{\int_A P(B|A', C) dA'} -->
<!-- \end{eqnarray} -->

Bayesian inference is focuses upon the concept of updating one's beliefs via equation \@ref(eq:bayesTheorem). If one has a prior belief about the behaviour of some 
parameter $\theta$, captured in the distribution $p(\theta)$, and then
observe some data $X$, then one describes the change in one's belief about $\theta$ given the observed variable $X$ by combining the distributions associated with the two sources of information (one currently observed, the other currently unobserved):

\begin{eqnarray}
p(\theta | X) = \frac{p(X | \theta) p(\theta)}{\int_{\Theta} p(X | \theta') d \theta'}.
(\#eq:posteriorDistribution)
\end{eqnarray}

In this case the following terminology is used:

* $p(\theta)$ is the _prior distribution_ of $\theta$ (so-called as it encompasses one beliefs prior to observing $X$);
* $p(X | \theta)$ is the _likelihood_ of $X$;
* $\int_{\Theta} p(X | \theta') d \theta' = p(X)$ is the _marginal likelihood_ of $X$ as it is the likelihood with $\theta$ marginalised out; and
* $p(\theta | X)$ is the _posterior distribution_ of $\theta$ (as it denotes the belief about $\theta$ one has _a posteriori_, that is after observing empirical fact).

Normally the posterior distribution is too difficult to solve analytically and one must use either _sampling based_ or _variational inference_ methods to solve this.

A common class of sampling based methods are the Markov-Chain Monte Carlo methods. These methods draw samples from Markov Chains constructed such that the stationary distribution is the posterior distribution. Samples are drawn to use Monte Carlo integration to describe the posterior distribution.

#### Monte Carlo integration
The original Monte Carlo method was developed as a means to solving integrals by use of random number generation [@metropolis1949monte].

```{definition, monteCarloIntegration, name = "Monte Carlo integration"}

Suppose there is some complex integral one wishes to solve on some interval, $(a,b)$:
  
\begin{align}
\int_a^b h(x) dx
\end{align}

If one can decompose $h(x)$ into the product of some more simple function $f(x)$ and a probability density $p(x)$ where both are defined over $(a,b)$, it can then be stated:
  
\begin{align}
\int_a^bh(x)dx = \int_a^bf(x)p(x)dx = \mathbb{E}_{p(x)}\left[f(x)\right]
(\#eq:monteCarloBasis)
\end{align}
  
By the Law of Large Numbers one can approximate this expectation of $f(x)$ over $p(x)$ by drawing $N$ random variables $x = (x_1,\ldots,x_N)$ from $p(x)$ for some large $N$; thus \@ref(eq:monteCarloBasis) becomes:
  
\begin{align}
\int_a^bh(x)dx = \mathbb{E}_{p(x)}\left[f(x)\right] \approx \frac{1}{N}\sum_{i=1}^Nf(x_i)
(\#eq:monteCarloIntegration)
\end{align}
  
This is Monte Carlo integration.

```

#### Markov chains

For the following definitions, consider some process $X$ which generates observations $x_1, \ldots, x_N$ which are observed at discrete timepoints $T=(t_1, \ldots, t_N)$ (where $t_i < t_j \iff i < j$). Let $S=\{s_q\}_{q=1}^Q$ be the sample space of $X$ and let $p_{(i,j)}=p(X_{t+1}=s_i | X_t = s_j)$ be the transition probability from state $s_j$ to $s_i$ in a single time step.

```{definition, markovProperty, name = "Markov property"}
$X$ is said to have the _Markov property_ if for a time point $t_{j+1}$, having observed all $j$ previous states, the transition probability is defined only by the preceding state, i.e. for any states $s_i, s_j, s_k \in S$:
  
\begin{align}
p(x_{t_{j+1}} = s_j | x_{t_1} = s_k, \ldots, x_{t_i} = s_i) = p(x_{t_{j+1})} = s_j | x_{t_i} = s_i) \hspace{4pt} \forall \hspace{4pt} i,j \in {1, \ldots, N}
\end{align}

Thus the future state depends only upon the present state. The process is said to be memoryless as past states do not affect future outcomes, given the current state. 

```

```{definition, markovProcess, name = "Markov process"}
If a process $X$ has the Markov property (\@ref(def:markovProperty)), then it is referred to as a _Markov process_. 

```

```{definition, markovChain, name = "Markov chain"}
If $X$ is a Markov process, than the sequence of variables $(x_1,\ldots, x_N)$ generated from $X$ form a _Markov chain_.
```

```{definition, nStepTransition, name = "$n$-step transition probability"}
Let $\mathbf{P}$ be an $|S| \times |S|$ matrix composed of the transition probabilities $p_{(i,j)}$ for all $i,j \in {1,\ldots, |S|}$. Define the $n$-step transition probability $p_{(i,j)}^n$ as the probability that the process is in state $s_j$ given it was in state $s_i$ at a remove of $n$ steps, i.e.:
  
\begin{align}
p_{(i,j)}^n = p(x_{t_{i+n}} = s_j | x_{t_i} = s_i)
\end{align}

```

```{definition, irreducible, name = "Irreducible"}
A Markov chain is said to be _irreducible_ if $p_{ij}^n > 0 \; \forall \; i,j \in \mathbb{N}$.
```

```{definition, communicate, name = "Communicate"}
If a Markov chain is irreducible, this means that there always exists a possible path from any state $s_i$ to every other state $s_j$. If this is true the states are said to _communicate_. 
```

```{definition, aperiodic, name = "Aperiodic"}
For a Markov chain, if the number of steps between two states is not required to be the multiple of some integer than the chain is considered _aperiodic_.

```

```{definition, detailedBalance, name = "Detailed balance"}
Let $X$ be a Markov process. The constraint:

\begin{align} 
p(x_{t_{n+1}} = s_i | x_{t_n} = s_j)  p(x_{t_{n+1}} = s_i)  =  p(x_{t_{n+1}} = s_j | x_{t_n} = s_i)  p(x_{t_{n+1}} = s_j)
(\#eq:detailedBalance)
\end{align}

is known as the _detailed balance_.

```


```{definition, reversible, name="Reversible"}

The chain has the _reversible_ property if the detailed balance holds for all states.

```

Reversibility is sufficient condition for a unique, stationary distribution. This means that the probability of being in any given state for the process is independent of the starting conditions given sufficient time and that the transition probabilities have approached some limiting value. 

```{definition, stationaryDistribution, name = "Staitonary distribution"}

A stationary distribution, $\pi$, is a $|S|$-vector where the $i^{th}$ entry is the probability $\pi_i = p(x_{t_{n+1}} = s_i)$. This means that:

\begin{align}
\pi_i &\geq 0 \: \forall \: i \in (1,\ldots,|S|) \\
\sum_{i=1}^{|S|}\pi_i &= 1
\end{align}

Furthermore the stationary distribution is invariant under the operation of the transition matrix $\mathbf{P}$ upon it:

\begin{align}
\pi = \pi\mathbf{P}.
(\#eq:stationaryDistribution)
\end{align}

Thus the distribution, $\pi$, remains unchanged as time progresses and more states are observed.

If $\pi$ is a stationary distirbution associated with a reversible Markov process $X$, then for any initial distribution, $\pi_0$, and transition probability matrix $P$,

\begin{align}
\lim_{n\to\infty} \pi_0 \mathbf{P}^n = \pi.
\end{align}

In this case the chain is said to converge to $\pi$. Once the chain is sampling from $\pi$, then as this distribution is stationary, the samples drawn should be independent of the previous draws as can be seen in \@ref(eq:stationaryDistribution).

```

For this reason auto-correlation is one of the measures of convergence within a Markov chain.

#### Markov-Chain Monte Carlo methods

Markov-Chain Monte Carlo (MCMC) methods developed as a method to obtain samples from some complex distribution $p(x)$ for the decomposition suggested in \@ref(eq:monteCarloBasis). The goal in the following is to draw samples from some distribution $p(\theta)$ where there is some distribution $f(\theta)$ such that:
  
\begin{align}
p(\theta) = \frac{f(\theta)}{K} 
\end{align}

For some constant $K$ where $K$ may not be known and is often difficult to compute.

The Metropolis algorithm [@metropolis1949monte; @metropolis1953equation] generates a sequence of draws from $p(\theta)$ using the following steps:
  
1. Initialise with some arbitrary value $\theta_0$ with the condition that $f(\theta_0) > 0$ and also choose some probability density $q(\theta_1|\theta_2)$ as the _jumping distribution_ or _proposal density_. For the Metropolis algorithm one demands that this is symmetric (i.e. $q(\theta_1 | \theta_2) = q(\theta_2 | \theta_1)$).
2. For each iteration, $t$:
  1. Using the current value $\theta_{t-1}$, sample a candidate point, $\theta^*$, from  $q(\theta^* | \theta_{t-1})$.
  2. Calculate the _acceptance ratio_ for the new value $\theta^*$:
  
\begin{align}
\alpha = \frac{p(\theta^*)}{p(\theta_{t-1})} = \frac{f(\theta^*)}{f(\theta_{t-1})}
\end{align}
  
Note that as the proportionality constant is the same for all $\theta$ that this
is an equivalence rather than proportional relationship.
3. Accept the new value $\theta^*$ with probability equal to $\min(\alpha, 1)$. 
Generate a number $u$ from the uniform distribution on $[0,1]$ and accept if 
$\alpha \geq u$, otherwise reject and continue with the previously accepted 
value.

This generates a Markov chain $(\theta_0,\ldots,\theta_k,\ldots, \theta_n)$ as 
each iteration is conditionally independent of all others given the sample from
the iteration preceding it. A stationary distribution is reached after a
_burn-in_ period of $n_{burn}$ steps (for some $n_{burn} \in \mathbb{N}$) and
all following samples come from the true $p(\theta)$. Knowing $n_{burn}$ is a 
non-trivial issue; there exist some heuristic methods are used to diagnose 
$n_{burn}$, but it remains a subjective choice.

The samples generated are highly correlated with other samples from within a 
close range of iterations. To avoid recording this duplicate information, often
only every $l$th sample is recorded for some small $l$ (this process is referred 
to as _thinning_).

@hastings1970monte extends this method to allow asymmetric proposal densities, 
in which case the acceptance ratio changes to:
  
\begin{align} 
\alpha = \min\left(\frac{f(\theta^*) q(\theta^*|\theta_{t-1})}{f(\theta_{t-1})q(\theta_{t-1}|\theta^*)}, 1\right)
(\#eq:metropolisHastingsAlpha)
\end{align}

This extension proposed is known as the Metropolis-Hastings algorithm. @geman1984stochastic use a special case of this, taking $\alpha = 1 \; \forall \; \theta^*$, accepting all proposed values. This is known as a _Gibbs sampler_.

These methods are useful in a Bayesian context as one is interested in a rather complex distribution, the posterior, and know two simpler quantities, the prior and the likelihood, that the posterior is proportional to (as shown in equation \@ref(eq:bayesTheorem)). Thus one can use MCMC methods to sample directly from the posterior distribution without directly calculating the normalising constant.

#### Gibbs sampling

Consider some vector of variables $\theta = (\theta_1, \ldots, \theta_q)$ that we wish to perform inference upon. One method of implementing this is to perform _Gibbs sampling_ [@geman1984stochastic]. This works by iterating over each variable, updating it based upon the current values of all the other variables and then repeating this a large number of times (ideally an infinite number of times). 

Let $\theta^{(j)} = (\theta^{(j)}_1, \ldots, \theta^{(j)}_q)$ be the sampled values of $\theta$ in the $j^{th}$ iteration of Gibbs sampling. Our update for $\theta^{(j)}_i$ is conditioned on all the current values for the other variables - this means that the first $(i - 1)$ variables
have already been updated $j$ times, but the remaining $q - i$ variables are still based upon the
$(j - 1)^{th}$ iteration, i.e. our update probability is of the form:

\begin{eqnarray}
p(\theta^{(j)}_i | \theta^{(j)}_1, \ldots, \theta^{(j)}_{i-1}, \theta^{(j)}_{i+1}, \ldots, \theta^{(j)}_q).
(\#eq:updateProbs)
\end{eqnarray}

Consider Gibbs sampling performed upon a mixture model of $K$ components clustering observed variables $X=(x_1, \ldots, x_N)$. In this case the unobserved variables present are the $N$ allocation variables, $z=(z_1, \ldots, z_N)$, $K$ component parameters $\theta=(\theta_1, \ldots, \theta_K)$ (possibly each $\theta_k$ is a vector of parameters) and $K$ component weights $\pi=(\pi_1, \ldots, \pi_K)$. I use the notation $x_{-i}=(x_1,\ldots, x_{i-1}, x_{i+1}, \ldots, x_N)$. The hierarchical model is described in figure \@ref(fig:hierarchicalModelFiniteMixture).

```{r, hierarchicalModelFiniteMixture, echo=FALSE, engine="tikz", out.width="90%", fig.cap="The mixture model we are interested in performing inference upon.", fig.align="center", cache=T}
 \usetikzlibrary{arrows}
\usetikzlibrary{fit,positioning}
\centering
\begin{tikzpicture}[scale=.7, auto,>=latex']
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 22mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
 \node[main] (pi) {$\pi_k$ };
 \node[main] (z_i) [below right=of pi] {$z_i$};
 \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
 \node[main] (theta) [above left=of x_i] {$\theta_k$};
 \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
 \node[rectangle, inner sep=-0.5mm, fit= (z_i) (x_i),label=above right:$N$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm,draw=black!100, fill=blue! 25, fill opacity=0.2, fit= (z_i) (x_i)] {};
 \node[rectangle, inner sep=-0.8mm, fit= (theta) (pi),label=above right:$K$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm, draw=black! 100, fit= (theta)(pi)] {};
 \node[main] (pi) {$\pi_k$ };
 \node[main, fill = white!1] (z_i) [below right=of pi] {$z_i$};
 \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
 \node[main] (theta) [above left=of x_i] {$\theta_k$};
 \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
\end{tikzpicture}

```

In the case of a mixture model, often the *only* variables of interest are the unseen allocation variables, $z$. This means that we are not interested in the samples produced for the other variables.

Consider the sampling of $z_i$. As this can only hold a relatively small number of values we
can consider the probability for each possible $k$. Based upon the relationships depicted in figure \@ref(fig:hierarchicalModelFiniteMixture) and equations \@ref(eq:totalProb) and \@ref(eq:bayesTheorem):

\begin{eqnarray}
		p(z_i = k | x, z_{-i}, \pi) &=& \frac{p(z_i = k | \pi, x_{-i}, z_{-i}, \pi) p(x_i | z, x_{-i}, \pi)}{p(x_i | x_{-i}, z_{-i}, \pi)} \\
		&\propto& p(z_i = k | \pi_k) \int_{\theta} p(x_i | \theta, z, x_{-i}, \pi) p(\theta | z, x_{-i}, \pi) d \theta \\
		&=& \pi_k \int_{\theta} p(x_i | \theta) p(\theta | z, x_{-i}) d \theta
	(\#eq:ziCondProb) 
\end{eqnarray}

Note that the term in the denominator, $p(x_i | x_{-i}, z_{-i}, \pi)$ is independent of $z_i$ and thus the same for all values of $k$. The integral in \@ref(eq:ziCondProb) is the posterior predictive probability for $x_i$ given the items in the component; thus one may consider \@ref(eq:ziCondProb) as how well does each component predict $x_i$, or as a model comparison problem.

 An alternative way of describing this involves the ratio of marginal likelihoods. As we are component specific (given $z_i = k$), I drop the $z$ and $\pi$ from my conditional and assume we are referring only to the $x_j$ for which $z_j = k$.
 
\begin{eqnarray}
p(x_i | z, x_{-i}, \pi) &=& \frac{p(x | z)}{p(x_{-i} | z)} \\
&=& \frac{\int_\theta p(x | \theta) p(\theta) d\theta}{\int_\theta p(x_{-i} | \theta) p(\theta) d\theta}
\end{eqnarray}

Therefore we can write the posterior predictive distribution as this ratio of marginal likelihoods:
 
\begin{eqnarray}
p(z_i = k | x, z_{-i}, \pi) \propto \pi_k \frac{p(x)}{p(x_{-i})}
\end{eqnarray}

Thus we can create a $K$-vector of probabilities for the allocation of $x_i$ to each component by finding the ratio of marginal likelihoods for each component including and excluding $x_i$, and multiplying these by the associated component weight, $\pi_k$. One can normalise these by dividing by the sum of the members of this vector due to the independence of the normalising constant from $z_i$.

#### Example: Gaussian mixture models

In this section we derive the marginal likelihood for a component of the Gaussian mixture model assuming that the mean $\mu$ and the precision $\lambda$ are unknown. Before we can continue we state the associated probability density functions of the Normal and Gamma distributions:
	
\begin{eqnarray}
\mathcal{N}(x | \mu, \lambda^{-1}) &=& \sqrt{\frac{\lambda}{2\pi}} \exp \left(- \frac{\lambda}{2}(x - \mu) ^ 2\right) \\
Ga(x | \alpha, \mathrm{rate }= \beta) &=& \frac{\beta^\alpha}{\Gamma(\alpha)} x ^{\alpha - 1} \exp(-\beta x)
\end{eqnarray}

The model likelihood for $n$ observations is:
	
\begin{eqnarray}
p(x | \mu, \lambda) &=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \sum_{i=1}^n(x_i - \mu)^2 \right)
(\#eq:ngammaLikelihood1)
\end{eqnarray}

Considering specifically the sum within the exponent here in equation \@ref(eq:ngammaLikelihood1), and letting $\bar{x}$ be the sample mean:

\begin{eqnarray}
	\sum_{i=1}^n(x_i - \mu)^2 &=& \sum_{i=1}^n(x_i - \bar{x} + \bar{x} - \mu)^2 \\
	&=& \sum_{i=1}^n\left[(x_i - \bar{x})^2 + (\mu - \bar{x})^2 + 2(x_i \bar{x} - \bar{x}^2 - x_i \mu + \bar{x} \mu)\right] \\
	&=& n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2
\end{eqnarray}

Substituting this back into equation \@ref(eq:ngammaLikelihood1), we have:

\begin{eqnarray}
p(x | \mu, \lambda) &=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \left[n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2\right] \right) 
(\#eq:likelihood)
\end{eqnarray}
		
The conjugate prior for this model is the \emph{Normal-Gamma} distribution. This has the probability density function:
	
\begin{eqnarray}
	NG(\mu, \lambda | \mu_0, \kappa_0, \alpha_0, \beta_0) &\coloneqq& \mathcal{N}(\mu | \mu_0, (\kappa_0 \lambda)^{-1})Ga(\lambda | \alpha_0, \beta_0) \\
	&=& \sqrt{\frac{\kappa_0 \lambda}{2\pi}} \exp \left(- \frac{\kappa_0 \lambda}{2}(\mu - \mu_0) ^ 2\right) \\
	& & \hspace{3mm} \times \hspace{3mm} \frac{\beta_0 ^{\alpha_0}}{\Gamma(\alpha_0)} \lambda ^{\alpha_0 - 1} \exp(-\beta_0 \lambda) \\
	&=& \sqrt{\frac{\kappa_0}{2\pi}} \frac{\beta_0 ^ {\alpha_0}}{\Gamma(\alpha_0)} \\
	& & \hspace{3mm} \times \hspace{3mm} \lambda^{\alpha_0 - \frac{1}{2}} \exp\left(-\frac{\lambda}{2}\left[\kappa_0 (\mu - \mu_0)^2 + 2 \beta_0\right]\right) 
(\#eq:priorPDF)
\end{eqnarray}

Here the normalising constant is:
	
\begin{eqnarray}
Z_0^{-1} = \sqrt{\frac{\kappa_0}{2\pi}} \frac{\beta_0 ^ {\alpha_0}}{\Gamma(\alpha_0)}
\end{eqnarray}

One can see that this function in equation \@ref(eq:priorPDF) will naturally complement the likelihood described in equation \@ref(eq:likelihood). 

To derive the poisterior probability, we apply Bayes' theorem (equation \@ref(eq:bayesTheorem)):
	
\begin{eqnarray}
	p(\mu, \lambda | x) &\propto& p(x | \mu, \lambda) p(\mu, \lambda) \\
	&=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \left[n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2\right] \right) \\
	&& \hspace{3mm} \times \hspace{3mm} \lambda^{\alpha_0 - \frac{1}{2}} \exp\left(-\frac{\lambda}{2}\left[\kappa_0 (\mu - \mu_0)^2 + 2 \beta_0\right]\right) \\
	&\propto& \lambda^{\alpha_0 + \frac{n}{2} - \frac{1}{2}} \exp \left\{-\frac{\lambda}{2}\left[n(\mu - \bar{x})^2 + \kappa_0(\mu - \mu_0)^2 + \sum_{i=1}^n(x_i - \bar{x})^2 + 2\beta_0\right]\right\} 
	(\#eq:posteriorStep1)
\end{eqnarray}
	
We try and anticipate the parameters of our posterior. 
In Bayesian inference there is no fundamental difference between the prior and the likelihood - both are distributions (bar that one corresponds to observed variables and the other to unobersved variables). Thus we would expect that if we wish to combine these quantities and we have conjugacy, that the intuition for what each parameter means within the observed part of our equation should translate onto the unobserved distribution and in this way help us to have an intution for how to combine the two quantities. Thus if we think as the prior as being based upon some previously observed data, then the $\kappa_0$ parameter would be the number of observations in this data and $\mu_0$ the mean observed. In this case we would expect that the updated mean, $\mu_n$, should correspond to a weighted average of the sample mean of the data currently being observed ($\bar{x}$), and that in our prior ($\mu_0$). Similarly, the ``total'' number of observations between the prior data and the current data, $\kappa_n$, should be the sum of the number of samples in the dataset under observation, $n$, and the number of samples in our prior dataset, $\kappa_0$. (This logic implies that including a very small $\kappa_0$ is one way of including an uninformative prior.) Describing this mathematically:
	
\begin{eqnarray}
	\kappa_n &=& \kappa_0 + n \\
	\mu_n &=& \frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}
\end{eqnarray}
	
If this is the case we would expect a term in the exponent:
	
\begin{eqnarray}
	\kappa_n (\mu - \mu_n)^2 &=& (\kappa_0 + n)\left[\mu - \left(\frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}\right)\right]^2 \\
	&=& (\kappa_0 + n) \left(\mu^2 - 2\mu(\kappa_0 \mu_0 + n \bar{x}) + \left(\frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}\right)^2\right) %\\
	%&=& \kappa_0 \mu^2 + n\mu^2 - 2\mu (\kappa_0 + n) (\kappa_0 \mu_0 + n \bar{x}) - \frac{(\kappa_0\mu_0)^2 + 2n\kappa_0\mu_0 \bar{x} + (n\bar{x})^2}{\kappa_0 + n}
\end{eqnarray}

Returning to equation \@ref(eq:posteriorStep1) and considering the part of the exponent containing $\mu$:
	
\begin{eqnarray}
	\kappa_0 (\mu - \mu_0) ^ 2 + n (\mu - \bar{x}) ^2 &=& \mu^2 (\kappa_0 + n) + n\bar{x}^2 + \kappa_0 \mu_0^2 -2n\mu\bar{x} -2 \kappa_0 \mu \mu_0 \\
	&=& (\kappa_0 + n) \left[\mu^2 + \frac{n\bar{x}^2}{\kappa_0 + n} + \frac{\kappa_0 \mu_0^2}{\kappa_0 + n} - 2 \mu \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right] 
(\#eq:posteriorExponentExpansion)
\end{eqnarray}
	
Notice that several of the desired terms are present! If one focuses upon the components of this equation that do not fit the expected form:
	
\begin{eqnarray}
	\frac{n\bar{x}^2}{\kappa_0 + n} + \frac{\kappa_0 \mu_0^2}{\kappa_0 + n} &=& \frac{n(\kappa_0 + n)\bar{x}^2 + \kappa_0(\kappa_0 + n)\mu_0^2}{(\kappa_0 + n)^2} \\
	&=& \frac{n^2 \bar{x}^2 + 2\kappa_0 n \mu_0 \bar{x} + \kappa_0^ 2 \mu_0 ^ 2}{(\kappa_0 + n)^2} 2 \frac{\kappa_0 n \bar{x}^2 + \kappa_0 n\mu_0^2 - 2\kappa_0 n \mu_0 \bar{x}}{(\kappa_0 + n)^2} \\
	\\
	&=& \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right)^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{(\kappa_0 + n)^2}
\end{eqnarray}

Substituting this result into equation \@ref(eq:posteriorExponentExpansion) yields:
	
\begin{eqnarray}
\kappa_0 (\mu - \mu_0) ^ 2 + n (\mu - \bar{x}) ^2 &=& (\kappa_0 + n) \left[\mu + \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right]^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{\kappa_0 + n}
\end{eqnarray}

Returning to equation \@ref(eq:posteriorStep1):

\begin{eqnarray}
	p(\mu, \lambda | x) &\propto& \lambda^{\alpha_0 + \frac{n}{2} - \frac{1}{2}} \exp \left\{-\frac{\lambda}{2}\left[
	(\kappa_0 + n) \left(\mu + \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right)^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{\kappa_0 + n} + \sum_{i=1}^n(x_i - \bar{x})^2 + 2\beta_0\right]\right\} 
	(\#eq:posterior)
\end{eqnarray}

This is the probability density function of a Normal-Gamma distribution.

\begin{eqnarray}
	p(\mu, \lambda| x) &=& NG(\mu, \lambda | \mu_n, \kappa_n, \alpha_n, \beta_n) \\
	\mu_n &=& \frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n} \\
	\kappa_n &=& \kappa_0 + n \\
	\alpha_n &=& \alpha_0 + \frac{n}{2} \\
	\beta_n &=& \beta_0 + \frac{1}{2}\sum_{i=1}^n(x_i - \bar{x})^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{2(\kappa_0 + n)}
	\end{eqnarray}
	
As this posterior distribution is $NG(\mu_n,\kappa_n, \alpha_n, \beta_n)$, we know that the associated normalising constant is:

\begin{eqnarray}
Z_n &=& \frac{\Gamma(\alpha_n)}{\beta_n^{\alpha_n}}\sqrt{\frac{2\pi}{\kappa_n}}.
\end{eqnarray}

To derive the _Marginal Likelihood_, consider the posterior but track the normalising constants (such as the $(2\pi)^{-n/2}$ in the likelihood). Denote the prior, likelihood and posterior less their normalising constants by $p'(\mu, \lambda)$, $p(x | \mu, \lambda)$ and $p'(\mu, \lambda | x)$ respectively:

\begin{eqnarray}
\frac{1}{Z_n} p'(\mu, \lambda | x) &=& \frac{1}{p(x)}\frac{1}{Z_0} p'(\mu, \lambda)\left(\frac{1}{2\pi}\right)^{\frac{n}{2}} p'(x | \mu, \lambda)
\end{eqnarray}

We know that the product of the unnormalised prior and the unnormalised likelihood give the right hand side of equation \@ref(eq:posterior), and that this is our unnormalised posterior, $p'(\mu, \lambda | x)$. Thus:
	
\begin{eqnarray}
\frac{1}{Z_n} p'(\mu, \lambda | x) &=& \frac{1}{p(x)}\frac{1}{Z_0} \left(\frac{1}{2\pi}\right)^{\frac{n}{2}} p'(\mu, \lambda | x) \\
\implies p(x) &=& \frac{Z_n}{Z_0}(2\pi)^{-\frac{n}{2}} 
\end{eqnarray}

Thus our marginal likelihood is the ratio of the posterior normalising constants to the product of those of the likelihood and prior. Expanding this we finally have:
	
\begin{eqnarray}
	p(x) &=& \frac{\Gamma(\alpha_n)}{\beta_n^{\alpha_n}} \sqrt{\frac{2\pi}{\kappa_n}} \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \sqrt{\frac{\kappa_0}{2\pi}} (2\pi)^{-\frac{n}{2}} \\
	&=& \frac{\Gamma(\alpha_n)}{\Gamma(\alpha_0)}\frac{\beta_0^{\alpha_0}}{\beta_n^{\alpha_n}} \sqrt{\frac{\kappa_0}{\kappa_n}}(2\pi)^{-\frac{n}{2}}
\end{eqnarray}

#### Dirichlet process

I stated in section \@ref(clusterAnalysis) that the problem of choosing $K$ is a 
non-trivial issue. All of the previously stated methods demand that the user 
choose $K$; different criteria might be offered to help choose $K$, but it 
remains a subjective choice (as the choie of criteria is itself subjective).
A method that does not suffer from this problems is the Dirichlet process 
[**DP**, @ferguson1973bayesian], which is a form of _non-parametric_ Bayesian
model. A non-parametric model, despite the wording, contains inifinite 
parameters. These methods are computationally expensive due to the MCMC 
algorithms for Bayesian inference in DP mixture models [see e.g. @neal2000markov; @jain2004split], and often infeasible for large datasets [@crook2019fast].
A computationally tractable approximation is the  _Dirichlet-Multinomial Allocation_ (**DMA**) mixture model [@green2001modelling; see @savage2013identifying for an 
example].

```{definition, DMAmodel, name = "Dirichlet-multinomial allocation mixture model"}

Begin with the definition of a finite mixture model for a dataset 
$X=(x_1, \ldots, x_N)$ and $P$ features as described in definition 
\@ref(def:mixtureModels). For a given item $x_i$ we have allocation probability:

\[
p(x_i| c_i = k) = \pi_k f(x_i \theta_k),
\]

and marginal likelihood:

\[
p(x_i) = \sum_{k=1}^K\pi_k f(x_1 | \theta_k)
\]

In the DMA mixture model, the mixture weights, $\pi=(\pi_1, \ldots, \pi_K)$, 
are given a Dirichlet prior, often with a symmetric concentration parameter, 
$\alpha$. The hierarchical model is:

\[
\begin{aligned}
\pi &\sim \textrm{Dirichlet}\left(\frac{\alpha_0}{K},\ldots,\frac{\alpha_0}{K}\right), \\
c_i &\sim \textrm{Categorical}(\pi), \\
\theta_k &\sim h(\cdot), \\
x_i | c_i = k &\sim f(x_i | \theta_k).
\end{aligned}
\]

This is also represented in figure \@ref(fig:hierarchicalModelDMA). 
Here $h(\cdot)$ is some distribution, often with additional hyperparameters.
```

```{r, hierarchicalModelDMA, echo=FALSE, engine="tikz", out.width="90%", fig.cap="The hierarchical model for the DMA mixture model.", fig.align="center", cache=T}
 \usetikzlibrary{arrows}
\usetikzlibrary{fit,positioning}
\centering
\begin{tikzpicture}[scale=.7, auto,>=latex']
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 22mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
 \node[main] (pi) {$\pi_k$ };
 \node[main] (a) [left=of pi] {$\alpha_0$ };
 \node[main] (z_i) [below right=of pi] {$z_i$};
 \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
 \node[main] (theta) [above left=of x_i] {$\theta_k$};
 \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i)
		(a) edge [connect] (pi);
 \node[rectangle, inner sep=-0.5mm, fit= (z_i) (x_i),label=above right:$N$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm,draw=black!100, fill=blue! 25, fill opacity=0.2, fit= (z_i) (x_i)] {};
 \node[rectangle, inner sep=-0.8mm, fit= (theta) (pi),label=above right:$K$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm, draw=black! 100, fit= (theta)(pi)] {};
 \node[main] (pi) {$\pi_k$ };
 \node[main, fill = white!1] (z_i) [below right=of pi] {$z_i$};
 \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
 \node[main] (theta) [above left=of x_i] {$\theta_k$};
 \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
\end{tikzpicture}

```

$K$ is learnt from the data as components are allowed to become empty. If a 
component does become empty than it retain a a non-zero probability of being
allocated a member; thus by initialising with a large value of $K$ 
[@kirk2012bayesian recommend setting $K=\frac{N}{2}$] and allowing the model
to itself empty and fill components, the number of components actually used is
learnt. If one does let $K \to \infty$ then this model becomes a Dirichlet
process; to be approximating this well, one must have chosen a $K$ sufficiently
large that there are empty clusters within the model (although this is not 
sufficient cirterion to be sure that $K$ was initialised with a large enough 
value). 



#### Variational inference

@blei2006variational and @blei2017variational

## Consensus clustering 

Consensus Clustering [@monti2003consensus] was proposed as a method for _cluster validation_, specifically in the context of gene expression data. It is a 
general, model-independent re-sampling based method that may be applied to any
clustering method that is sensitive to initialisation. 

For a given model (such as $k$-means clustering), multiple random restarts are
used to asses cluster stability. The cluster stability is itself used to decide
upon $K$, the number of clusters present. For each restart a random 
initialisation is used and the dataset is perturbed using resampling techniques.
The consensus between the restarts can be then assesed using the 
_consensus matrix_. 

```{definition, consensusMatrix, name = "Consensus matrix"}
A _consensus matrix_ is an $(N \times N)$ matrix that stores, for each pair of items, the proportion of clustering runs in which two items are clustered together. The consensus matrix is obtained by taking the average over the coclustering matrices of models restart.

Let B^{(r)} be the coclustering matrix associated with the $r^{th}$ model run. Then, for $R$ model runs, let the consensus matrix, $M$, be defined:
  
\begin{align}
M = \frac{1}{B}\sum_{r=1}^RZ^{(r)}
\end{align}
```

The driving logic of this method is that if a sub-population does exist, then if 
a different sent of items had been observed, drawn from the same sub-populations, 
the clustering produced should be quite similar. Therefore, the more robust 
clusters are to sampling variability, the more possible it is that these 
clusters represent real structure.

> "The underlying assumption is that the more stable the results are with respect 
  to the simulated perturbations, the more these results are to be trusted."
`r tufte::quote_footer('@monti2003consensus')`

@monti2003consensus recommend applying Consensus clustering for various values 
of $K$ and selecting that which gives a consensus matrix with the most values 
close to 0 or 1.

This method is implemented in ``R`` in the ``ConsensusClusterPlus`` package [@wilkerson2010consensusclusterplus], available on Bioconductor.

## Integrative methods


required to explain complex disease [@hasin2017multi; @integrative2014integrative]

Parkinson's disease [@markello2020integrated]

Cancer has been particularly successful [@wang2016integrative; @rappoport2018multi]

autoencoder based methods [@seal2020estimating; @lemsara2020pathme]

Network based: [@rappoport2020monet, MONET allows outliers, missing data. Subsets of patients are clustered and may use a subset of 'omics to define this cluster]

``mergeTrees`` [@R-mergeTrees] implements the algorithm described by @hulot2020fast; this is a quick, heuristic-based method of combining local hierarchical clustering models to create a global dendrogram from which a clustering may be created.

sequential vs post-hoc vs joint, local vs global

Joint, sequential, post-hoc
Local vs global



### Bayesian integrative clustering

Bayesian integrative models tend either to be an extension of the MDA 
model, or else an extension of factor analysis where clustering is performed 
_post-hoc_ within the latent variable space. The prior approach is the main 
object of interest here, but I will include an example of a factor analysis 
model as this approach is often included in the multi-omics literature (); 
however when I refer to Bayesian integrative clustering I will be referring to 
methods that are clustering methods rather than factor analysis methods.

There are 3 basic approaches to Bayesian integrative clustering:

1. assume no global clustering structure;
2. assume the global clustering structure has _less_ clusters than the local clusterings;
3. assume the global clustering structure has _more_ clusters than the local clusterings;

I give an example of each of these, 1) Multiple Dataset Integration [*MDI*, @kirk2012bayesian], 2) Bayesian Consensus Clustering [*BCC*, @lock2013bayesian] and 3) Clusternomics [@gabasova2017clusternomics]. In each case I will describe the model briefly and explain how information is shared across datasets. All of these models are extensions of the MDA model to the multiple dataset setting.



<!-- In this case we model the latent structure in the $lth$ dataset using a mixture  -->
<!-- of $K_l$ components. This means that the full model density is the weighted  -->
<!-- sum of the probability density functions associated with each component where -->
<!-- the weights, $\pi_{lk}$, are the proportion of the total population assigned to the  -->
<!-- $kth$ components: -->

<!-- $$p(X_{li}| c_{li} = k) = \pi_{lk} f(X_{li} | \theta_{lk}),$$ -->
<!-- $$p(X_{li}) = \sum_{k=1}^{K_l} \pi_{lk} f_l(X_{li} | \theta_{lk}).$$ -->
<!-- Here $K_l$ is the total number of clusters present and $\theta_{lk}$ are the  -->
<!-- parameters defining the $kth$ distribution in the $lth$ dataset. -->

<!-- The weights, $\pi_l = (\pi_{l1},\ldots,\pi_{lK_l})$, follow a Dirichlet  -->
<!-- distribution with concentration parameter $\alpha_0$. -->

<!-- The distributions in the mixture model for each dataset are: -->

<!-- \[ -->
<!-- \begin{aligned} -->
<!-- \pi_l &\sim \textrm{Dirichlet}\left(\frac{\alpha_0}{K_l},\ldots,\frac{\alpha_0}{K_l}\right) \\ -->
<!-- c_{li} &\sim \textrm{Categorical}(\pi_l) \\ -->
<!-- \theta_{lk} &\sim h_l \\ -->
<!-- X_{li} | c_{li} = k &\sim f_l(X_{li} | \theta_{lk}) -->
<!-- \end{aligned} -->
<!-- \] -->


<!-- where $H_l$ is some prior distribution for parameteters for each mixture -->
<!-- component; $F_l$ is a probability distribution for samples given the parameters -->
<!-- $\theta_{lk}$. Note that each context may have different distributions and -->
<!-- hyperparameters. -->


#### Multiple Dataset Integration

@kirk2012bayesian and @mason2016mdi

@afrin2020directionally attempt to add directionality to information sharing.

#### Clusternomics

Before I explain Clusternomics, two definitions are required.


```{definition, outerProduct, name = "Outer product"}
For two vectors, $x=(x_1, \ldots, x_N)$ and $y=(y_1,\ldots,y_M)$, the outer
product is defined:

\[
\begin{aligned}
x \otimes y &= \begin{pmatrix}
  x_1 y_1 & x_1 y_2 & \cdots & x_1 y_M \\
  x_2 y_1 & x_2 y_2 & \cdots & x_2 y_M \\
  \vdots & \vdots & \ddots & \vdots \\
  x_N y_1 & x_N y_2 & \cdots & x_N y_M \\
  \end{pmatrix}
\end{aligned}
\]

```

```{definition, vectorFunction, name = "$vec(\\cdot)$"}
For a $N \times M$ matrix $A$, the $vec(\cdot)$ function is defined:

\[
\begin{aligned}
vec(A) &= \begin{pmatrix}
  A_{11} \\
  \vdots \\
  A_{N1} \\
  A_{12} \\
  \vdots \\
  A_{N2} \\
  A_{13} \\
  \vdots \\
  A_{NM}
\end{pmatrix}
\end{aligned}
\]

```

<!-- assumes more global clusters than local -->

Clusternomics [@gabasova2017clusternomics] identifies structure on both the 
local and the global level. The model does not assume that cluster behaviour
will be consistent across heterogeneous datasets. This is not to assume that the
clustering structure uncovered in one dataset should not inform the clustering 
in another dataset. This can be summarised as so:

1. Clustering structure in one dataset should inform the clustering in another 
dataset. If two points are clustered together in one context they should be 
more inclined to cluster together in other contexts.
2. Different degrees of dependence should be allowed between clusters across
contexts. The model should work when datasets have the same underlying structure
and also when each dataset is effecitvely independent of all others. Fundamental 
to this is allowing datasets to have different numbers of clusters.

To enable these modelling aims, Clusternomics explicitly represents the local 
clusters and the global structure that emerges when considering the combination
of the datasets. The global clusters are defined by combinations of local 
clusters. Consider the case where 3 clusters emerge in Context 1 (denoted by 
labels $\{1, 2, 3\}$) and 2 clusters emerge in Context 2 (denoted by labels 
$\{A, B\}$). In this case our global structure has the possibile form:

$$\{(1, A), (2, A), (3, A), (1, B), (2, B), (3, B)\}$$

Thus if an item is assigned a label of 1 in Context 1 and a label of $A$ in 
Context 2 it increases the probability of other items being assigned to cluster 
$(1, A)$ at the global level. However, it is possible that some of the possible 
global clusters described above are not realised as some local clusters overlap 
across datasets. Consider the case that labellings 1 and 2 from the first 
context are captured entirely by label $A$ in the second context with a label of 
3 corresponding perfectly to label $B$. In this case our global structure would 
take the form:

$$\{(1, A), (2, A), (3, B)\}$$

The original paper introduces two models that are "asymptotically equivalent". 
The first is easier to develop an intuition for, but it is the second that is 
implmented in the ``clusternomics`` R package [@R-clusternomics] as it is more 
computationally efficient.

I begin by describing the intuitive model. For ease of understanding, let $L=2$.
Then each context has its own mixture weights with symmetric Dirichlet priors:

\[
\begin{aligned}
\pi_1 &\sim \textrm{Dirichlet}\left(\frac{\alpha_1}{K_1},\ldots,\frac{\alpha_1}{K_1}\right) \\
\pi_2 &\sim \textrm{Dirichlet}\left(\frac{\alpha_2}{K_2},\ldots,\frac{\alpha_2}{K_2}\right)
\end{aligned}
\]

These weights form the basis of the local clustering within each dataset. A 
third mixure distribution is used to link the two local clusters. This has a 
Dirichlet prior over the global mixture weights, $\varpi$, which is defined over 
the outer product of the local weights:

\[
\varpi \sim \textrm{Dirichlet}\left(\gamma \textrm{ vec}(\pi_1 \otimes \pi_2)\right).
\]


This is the basis for the non-symmetric concentration parameter of the 
Dirichlet distribution over the global mixture weights, $\varpi$. 

A global membership variable, $\varsigma$, is drawn from a Catgeorical distribution with
concentration parameter $\varpi$:

\[
\begin{aligned}
\varsigma_i &= (c_{1i}, c_{2i}) \\
\varsigma &\sim \textrm{Categorical}(\varpi) \\
X_{li} | c_{li} = k &\sim f_l(X_{li} | \theta_{lk}), l \in \{1, 2\}
\end{aligned}
\]

In this way the local clusters, $c_{li}$, in the $lth$ context are projections 
of the global clustering, $c_i$, onto this space.

The model achieves the property that the local assignment should affect the 
global assignment posterior probability. This can be seen as local assignments 
affect the posterior probability of the context-specific weights, $\pi_l$, which
in turn define the probabilities of the global combinatorial clusters through 
the hierarchical model (a change in the membership of the $kth$ component in the
$lth$ dataset affects an entire slice of column vectors in the tensors defining 
the concentration parameter for the global component weights, $\varpi$).

The model also represents different degrees of dependence by allowing any 
combination of cluster assignments across contexts. When there is a single 
common cluster structure across the two contexts, the occupied clusters will be concentrated along the diagonal of the probability matrix of $\varpi$.

When $L > 2$ then the outer product of the local componenet weights generalises 
to a tensor product. In this case the hierarchical model is given by:

\[
\begin{aligned}
\pi_l | \alpha_0 &\sim \textrm{Dirichlet}\left(\frac{\alpha_0}{K_l},\ldots,\frac{\alpha_0}{K_l}\right) \\
\varpi | \gamma, \{\pi_1\, \ldots, \pi_L\} & \sim \textrm{Dirichlet}\left(\gamma \left\{\bigotimes_{l=1}^L \pi_l \right\} \right)  \\
\varsigma_i | \varpi &\sim \textrm{Categorical}(\varpi), \varsigma_i = \left(c_{1i},\ldots,c_{Li}\right)\\
\theta_{lk} &\sim h_l \\
X_{li} | c_{li} = k &\sim f_l(X_{li} | \theta_{lk})
\end{aligned}
\]

The model for $L$ contexts has a scaling issue as each additional dataset brings
an additional layer of calculations in the probability tensor. The second model 
is designed to reduce this cost by avoiding calculations for uninhabited 
components.

The second formulation, the quick model, attempts to reduce the number of
combinations required. This requires decoupling the number of local clusters and
the number of global clusters. A mixture over $S$ global clusters is defined:

\[
\begin{aligned}
\varpi | \gamma_0 & \sim \textrm{Dirichlet}\left(\frac{\gamma_0}{S}, \ldots, \frac{\gamma_0}{S} \right) \\
\varsigma_i | \varpi &\sim \textrm{Categorical}(\varpi)
\end{aligned}
\]

where $\varpi$ is the global mixture weights and $\varsigma_i$ is the components 
assignment indivator variable as before. A variable $c_{ls}$ is then defined 
that associates the $sth$ global cluster with context specific clusters:

\[
\begin{aligned}
\pi_l | \alpha_0 & \sim \textrm{Dirichlet}\left(\frac{\alpha_0}{K_l},\ldots,\frac{\alpha_0}{K_l}\right), \\
c_{ls} | \pi_l & \sim \textrm{Categorical}(\pi_l).
\end{aligned}
\]

Here $c_{ls} \in \{1, \ldots, K_l\}$ assignes the $sth$ global cluster to the a 
specific local cluster in the $lth$ dataset. One may consider $(c_{ls})_{s=1}^S$ 
as the coordinates of the global clusters in the $lth$ dataset. This link 
means that the $varsigma_i$ maps a point to a global cluster as well as the local 
clusters.

In the previous model the mapping of global clusters to local clusters was
implicit, because each combination of context clusters mapped to a unique 
global cluster. In this model, the mapping is probabilistic and forms a part of
the model. One may now state $S$, the number of global components, and thus 
limit the number of computations required in contrast to the preceding model.

This hierarchial model is defined by:

\[
\begin{aligned}
\varpi | \gamma_0 & \sim \textrm{Dirichlet}\left(\frac{\gamma_0}{S},\ldots,\frac{\gamma_0}{S}\right), \\
\varsigma_i | \varpi & \sim \textrm{Categorical}(\varpi), \\
\pi_l | \alpha_0 & \sim \textrm{Dirichlet}\left(\frac{\alpha_0}{K_l},\ldots,\frac{\alpha_0}{K_l}\right), \\
c_{ls} | \pi_l & \sim \textrm{Categorical}(\pi_l) \\
\theta_{kl} | h_l & \sim h_l \\
X_{li} | \varsigma_i, (c_{sl})_{s=1}^S, (\theta_{lk})_{k=1}^K &\sim f_l(X_{li} | \theta_{lk_{c_i}})
\end{aligned}
\]

This is represented in figure \@ref(fig:hierarchicalModelClusternomics).

```{r, hierarchicalModelClusternomics, echo=FALSE, engine="tikz", out.width="90%", fig.cap="The hierarchical model for Clusternomics.", fig.align="center", cache=T}
 \usetikzlibrary{arrows}
\usetikzlibrary{fit,positioning}
\centering
\begin{tikzpicture}[scale=.7, auto,>=latex']
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 22mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
 \node[main] (pi) {$\pi_{lk}$ };
 \node[main] (a) [left=of pi] {$\alpha_0$ };
 \node[main] (ci) [below right=of pi] {$c_{ls}$};
 \node[main, fill = black!10] (xi) [right=of ci] {$x_{li}$};
 \node[main] (theta) [above left=of xi] {$\theta_{lk}$};
 \node[main] (varsig) [below=of xi] {$\varsigma_i$};
 \node[main] (varpi) [left=of varsig] {$\varpi_s$};
 \node[main] (gamma0) [left=of varpi] {$\gamma_0$};
 \node[main] (hl) [right=of theta] {$h_l$};
 \path (pi) edge [connect] (ci)
		(ci) edge [connect] (xi)
		(theta) edge [connect] (xi)
		(a) edge [connect] (pi);
 \node[rectangle, inner sep=-0.5mm, fit= (varsig) (xi),label=above right:$N$, xshift=0mm, yshift=16mm] {};
 \node[rectangle, inner sep=4.8mm,draw=black!100, fit= (varsig) (xi)] {};
 \node[rectangle, inner sep=-0.8mm, fit= (theta) (pi),label=above right:$K_l$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm, draw=black! 100, fit= (theta)(pi)] {};
\node[rectangle, inner sep=4.8mm, draw=black! 100, fit= (theta)(pi)] {};
 \node[rectangle, inner sep=-0.8mm, fit= (ci) (varpi),label=above right:$S$, xshift=0mm, yshift=16mm] {};
 \node[rectangle, inner sep=4.8mm, draw=black! 100, fit= (ci) (varpi)] {};
 \node[rectangle, inner sep=-0.8mm, fit= (ci) (xi) (theta) (pi) (hl),label=above right:$L$, xshift=20mm] {};
 \node[rectangle, inner sep=6.0mm, draw=black! 100, fit= (ci) (xi) (theta) (pi) (hl)] {};

 \path (pi) edge [connect] (ci)
		(ci) edge [connect] (xi)
		(theta) edge [connect] (xi)
		(hl) edge [connect] (theta)
		(varsig) edge [connect] (xi)
		(varpi) edge [connect] (varsig)
		(gamma0) edge [connect] (varpi);
\end{tikzpicture}

```

The quick, decoupled model is implemented in the R package `clusternomics`
[@R-clusternomics], available on Github, and uses Gibbs sampling to perform the 
inference.

#### Bayesian Consensus Clustering

@lock2013bayesian; assumes less gloabl clusters than local clusters

The dependence between the datasets are captured by defining a parameter that 
controls the adherence of the source-specific clustering to the global 
clustering. 

Where MDI defines a parameter to share information between each
pariwise local clustering, BCC shares information between the local and
global clusterings. This $\alpha$ parameter defines how much the local 
clustering informs the global clustering and vice-versa. In this approach, each
local clustering is assumed to be conditionally independent of all others given
the global clustering.

BCC [@lock2013bayesian] is extension of the DMA mixture model. Consider the case
of $L$ datasets describing the same $N$ samples with different measurements or under different experimental conditions (e.g. gene expression data in one dataset, copy number variation of the same genes in
another). In this case there are context-specific models similar to that 
described above with a common number of components, $K$, in each model. There is
then an additional *global clustering*, $C=(C_1, \ldots, C_N)$, where 
$C_i \in \{1, \ldots, K\} \forall i \in [1, N]$. In this model it is the local
allocations that are dependent upon the global membership. For the $lth$ 
context, given the global clustering $C$, the context-specific concentration 
parameter, $\alpha_l$:

\[
P(c_{li} = k | C_i) = \nu(k, C_i, \alpha_l)
\]

where $\alpha_l$ changes the magnitude of dependence of the local clustering 
about the global clustering. The conditional model is:

\[
P(c_{li} = k | x_{li}, C_i, \theta_{lk}) \propto \nu(k, C_i, \alpha_l) f_L(x_{li} | \theta_{lk}).
\]

@lock2013bayesian assume the following form for $\nu(\cdot)$:

\[
\nu(c_{li}, C_i, \alpha_l) = \alpha_l  \mathbb{I}(c_{li} = C_i) + \frac{1 - \alpha_l}{K - 1} (1 - \mathbb{I}(c_{li} = C_i))
\]

where $\alpha_l \in [\frac{1}{K}, 1]$ controls the similarity of the clustering 
in context $l$ to the global clustering and $\mathbb{I}(\cdot)$ is the indicator
function. If $\alpha_l=1$ then $c_l = C$. The $\alpha_l$ is a random variable 
inferred from the data. Let $\alpha = (\alpha_1, \ldots, \alpha_L)$.

Let $\Pi=(\Pi_1, \ldots, \Pi_K)$ be the component weights at the global level.
Then the conditional probability of the context-specific clustering given the 
component weights is defined to be:

\[
P(c_{li} = k | \Pi ) = \alpha_l \Pi_k + (1 - \Pi_k) \frac{1 - \alpha_l}{K - 1}
\]

Note that if $kth$ global mixture weight is 0 (and hence the associated 
component has a total membership of 0) then the probability of assignment to the
$kth$ local cluster is 0 if and only if $\alpha_l = 1$. This is unlikely to 
happen (as 1 is the upper bound on $\alpha_l$), and therefore it is normal that
there are *more* local clusters than global clusters in stark contrast to the
Clusternomics model.

The probability of being allocated to the $kth$ global component is:

\[
P(C_i = k | c, \Pi, \alpha) \propto \Pi_k \prod_{l=1}^L \nu(c_{li}, k, \alpha_l).
\]

This model is implemented within the ``bayesCC`` R package [@R-bayesCC],
available on Github.

### Integrative factor analysis methods

I also briefly describe iCluster [@shen2009integrative], Joint and Individual 
Variation Explained [*JIVE*, @lock2013joint] and Multi-Omics Factor Analysis 
[*MOFA*, @argelaguet2018multi]. All of these methods apply a flavour of factor
analysis model; they are not clustering tools _per se_, instead they map the 
datasets to a shared set of latent factors which one may then perform clustering
upon (this is part of the iCluster pipeline). However, Factor Analysis is 
another prevalent integrative technique and some examples seem pertinent. 
Furthermore, MOFA is a Beysian method that uses VI to perform inference.

#### iCluster
@shen2009integrative and @mo2018fully

#### JIVE

JIVE [@lock2013joint] produces a decomposition of multiple datasets in three
terms: a low-rank approximation of variation for the integrated analysis, 
low-rank approximations of varation for the individual datasets and residual 
noise.

The model assumes that there will exist some global structure (joint structure
in the words ofthe original paper) and some local structure or individual 
structure unique to each dataset. The local component of the model is intended 
to account for structure that may be be artifacts of the process used to 
generate the data or of biological interest. This dataset-level variation can
interfere with finding global signal, just as joint structure can obscure 
important signal that is individual to a data type.

In the authors own words:

> "Analysis of individual structure provides a way to identify potentially 
   useful information that exists in one data type, but not others. Accounting
   for individual structure also allows for more accurate estimation of what is
   common between data types."
`r tufte::quote_footer('@lock2013joint')`

Some attractive featurs of JIVE include that it:

* may be used regardless of whether the dimension of a dataset exceeds the 
sample size;
* is applicable to datasets with more than two data types; and 
* has a simple algebraic interpretation.

This method describes the data as a sum of global factors, local factors and 
local noise. If one thinks of it in this way (i.e. related to factor
analysis), JIVE avoids computation and considers only the matrix that would 
normally be considered the product of the loadings matrix and the factors matrix.

Consider $L$ datasets denoted 
$X_1,\ldots,X_L$ for some $L \geq 2$. Each dataset **must** have a common number
of columns representing $N$ objects. Each dataset may have a unique number of 
rows (let $P_l$ be the number of rows for the $lth$ dataset $X_l$). Let:

\[
\begin{aligned}
P &= \sum_{l=1}^L P_l \\
X &= \begin{bmatrix}
  X_1 \\ \vdots \\ X_L
\end{bmatrix}
\end{aligned}
\]

In this case $X$ is a $P \times N$ matrix. @lock2013joint recommend 
the mean-centreing of the data by row, and scaleing the individual datasets by their 
individual variation to avoid dominance by anyone dataset. Let:

\[
\begin{aligned}
X_l^{scaled} &= \frac{X_l}{||X_l||} \\
||X_l||^2 &= \sum_{i,j} x_{lij} \\
\therefore ||X_l^{scaled}|| &= 1 \forall l
\end{aligned}
\]

Now:

\[
X^{scaled} = \begin{bmatrix}
  X_1^{scaled} \\ \vdots \\ X_L^{scaled}
\end{bmatrix}
\]

and each dataset contributes equally to the total variation of the concatenated 
matrix, $X$.

For this section let $X_1, \ldots, X_L$ be data matrices scaled as described 
above. Let $J_1, \ldots, J_L$ and $A_1, \ldots, A_L$ be the joint and individual 
structure matrices. Then the full model is given by:

\[
\begin{aligned}
X_1 & = J_1 + A_1 + \epsilon_1 \\
\vdots \\
X_L &= J_L + A_L + \epsilon_L
\end{aligned}
\]

where $\epsilon_l$ are $P_l \times N$ error matrices with independet entries and
an expectation of 0. Let $J$ be the stacked joint structure matrices. This model
imposes rank constraints:

Let:

\[ 
\begin{aligned}
rank(J) &= r < rank(X) \\
rank(A_l) &= r_l < rank(X_l) \textrm{ for } l \in \{1,\ldots,L\}
\end{aligned}
\]

A further constrain that the joint and individual structure matrices are 
orthogonal is imposed:

\[
J A_l^T = 0_{P \times P} \textrm{ for } l \in \{1,\ldots,L\}
\]

The purpose of this is to ensure that patterns in the samples that are informing
the global structure are unrelated to those responsible for the local structure.

For fixed ranks $r, r_1, \ldots, r_L$, the global and local structure captured 
in $J, A_1, \ldots, A_L$ is estimaed by minimising the sum of squared error for 
the given ranks. Let:

\[
R = \begin{bmatrix}
  R_1 \\ \vdots \\ R_L
\end{bmatrix} = \begin{bmatrix}
  X_1 - J_1 - A_1 \\ \vdots \\ X_L - J_L - A_L
\end{bmatrix}.
\]

The minimisation process is achieved iteratively by repeating two steps until 
convergence is achieved:

* Given the current $J$, find $A_1, \ldots, A_L$ to minimise $||R||$; and
* Given the current $A_1, \ldots, A_L$, find $J$ to minimise $||R||$.

The joint structure $J$ minimizing $||R||$ is equal to the first $r$ terms in 
the singular value decomposition (SVD) of $X$ with individual structure removed.

This method has been extended to consider the case where the data does not share 
only common items or features but allows for both [@park2020integrative; @lock2020bidimensional].

#### MOFA

@argelaguet2019mofaplus and @argelaguet2018multi

### Sequential analysis methods

These methods are performed to uncover a _global_ clustering structure given the
results of local clustering analyses for multiple related datasets.

Kernel Learning Integrative Clustering (_KLIC_) is a **sequential analysis** method of integrative clustering. This in comparison to **post-processing** or **joint** methods.

#### Cluster-Of-Cluster Analysis

@cancer2012comprehensive
@R-coca

#### Kernel Learning Integrative Clustering

@cabassi2019multiple
@R-klic

**Elevator pitch**: Takes independent clusterings of the individuals (i.e. local clusterings) and combines these for a global clustering. Allows different local clusterings to contribute with different strenghts to the final clustering. No specifications on the type of model used to create the original clusterings.


KLIC is an extension of Cluster-of-Cluster Analysis (COCA). If one has $L$ different datasets of measurements for the same $N$ individuals to which one applies independent clustering methods, COCA then turns the similarity matrices that result from this into a global clustering by combining the matrices in a method similar to Consensus Clustering (the original paper). KLIC extends this by allowing different similarity matrices to have different weights in how they contribute to the global clustering. In short, KLIC applies multiple kernel k-means clustering to similarity matrices generated for individual datasets.

To understand KLIC one must understand the following:

* COCA;
* the kernel trick;
* $k$-means clustering; and
* multiple kernel $k$-means clustering.

##### The kernel trick

This is a computational trick to avoid operations. It aims to do an analysis in a high-dimensional space while only considering calculations in the original space.

```{definition, kenrel, name = "Positive-definite kernel"}
A _positive-definite kernel_ (or simply a _kernel_), $\delta$, is a symmetric map:

\[
\delta : \mathcal{X} \times \mathcal{X} \to \mathbb{R}
\]

which for all $x_1, \ldots, x_N \in \mathcal{X}$, the matrix $\mathbf{\Delta}$ defined by entries $\mathbf{\Delta}_{ij} = \delta(x_i, x_j)$, is positive semi-definite.
```

```{definition, kenrelMatrix, name = "Kernel matrix"}
A _kernel matrix_ or __Gram matrix__, $\mathbf{\Delta}$, is the positive semi-definite matrix defined by a kernel $\delta$ applied to data $\mathcal{X} = (x_1, \ldots, x_N)$ with entries $\mathbf{\Delta}_{ij} = \delta(x_i, x_j)$.
```

```{definition, featureMap, name = "Feature map"}
For each kernel $\delta$ there exists a _feature map_ $\phi(\cdot)$, which maps the original data $\mathcal{X}=(x_1, \ldots, x_N)$ to some new feature space taking values in some inner product of $\mathcal{X}$ defined by:

\[
\delta(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle.
\]

Thus if one is interested in working in some feature space that can be defined in terms of inner products, one may use kernels to avoid computations in analysing the data in said space.
```

<!-- Sometimes one will see kernels defined by their feature map - indeed this can be a more intuitive way to approach the problem. Knowing that we wish to include higher order terms of $x_i$, for example: -->

<!-- \[ -->
<!-- \phi(x_i) = \begin{bmatrix} -->
<!--   x_i \\ x_i^2 \\ x_i^3 -->
<!--   \end{bmatrix}. -->
<!-- \] -->

<!-- In this case we know the feature space we wish to use and can create an inner product: -->

<!-- \[ -->
<!-- \phi(x_i)^T \phi(x_j) = \sum_{k=1}^3 x_i^k x_j^k. -->
<!-- \] -->

##### $k$-means clustering

$k$-means clustering assigns each of $N$ points to *one* of $K$ different clusters. For cluster means $\mu = (\mu_1, \ldots, \mu_K)$, data $X=(x_1, \ldots, x_n)$ and cluster allocation matrix $\mathbf{Z}$, the objective function for $k$-means clustering is:

\[
f_{Obj}(\mathbf{Z}, \mu) = \sum_{i=1}^N \sum_{k=1}^K z_{ik}||x_i - \mu_k||^2.
\]

One can solve for $f_{Obj}$ by iterating over:

\[
\begin{aligned}
z_{ik} &= \mathbb{I}(k = \underset{\mathbf{k'}}{\operatorname{argmin}}|| x_i - \mu_{k'} ||_2^2), \\
\mu_k &= \frac{1}{N_k} \sum_{i = 1}^N z_{ik} x_i.
\end{aligned}
\]

##### Kernel $k$-means clustering

If we redefine this problem in some feature space defined by the feature map $\phi(\cdot)$, we can use the kernel trick in this context. First we write the problem in terms of the feature map:

\[
f_{Obj}(\mathbf{Z}, \mu) = \sum_{i=1}^N \sum_{k=1}^K z_{ik}||\phi(x_i) - \mu_k||^2.
\]

Allocation and cluster means are as previously stated, except $x_i$ is replaced with the features $\phi(x_i)$.

\[
\begin{aligned}
& \underset{\mathbf{Z}}{\operatorname{argmin}} \sum_{i=1}^N \sum_{k=1}^K z_{ik}||\phi(x_i) - m^*_k ||_2^2 \\
& m^*_k = \frac{1}{N_k} \sum_{i=1}^N z_{ik} \phi(x_i)
\end{aligned}
\]

Define the $K \times K$ matrix $\mathbf{L}$ with $(k, k)th$ entries of $1 / N_k$ and 0's elsewhere (i.e. the $kth$ diagonal correspond the inverse of the number of points assigned to the $kth$ cluster). Define also the gram matrix $\mathbf{\Delta}$ as the matrix with $(i,j)th$ entries $\delta(x_i, x_j)$ and the matrix $\Phi$ where $\Phi_{ij} = \phi_i(x_j)$. Define also the matrix $\mathbf{M}$:

\[
\mathbf{M} = \Phi \mathbf{Z} L \mathbf{Z}^T.
\]

\[
\begin{aligned}
L\mathbf{Z}^T &= \begin{bmatrix} 
  \frac{1}{N_1} & &  \\
  & \ddots & \\
  & & \frac{1}{N_K}
\end{bmatrix} \begin{bmatrix}
  z_{11} & \cdots & z_{N1} \\
  \vdots & \ddots & \vdots \\
  z_{K1} & \cdots & z_{NK}
\end{bmatrix} \\
 &= \begin{bmatrix}
 \frac{z_{11}}{N_1} & \cdots & \frac{z_{N1}}{N_1} \\
 \vdots & \ddots & \vdots \\
 \frac{z_{1K}}{N_K} & \cdots & \frac{z_{NK}}{{N_K}}
\end{bmatrix}, \\
\mathbf{Z} L \mathbf{Z}^T &= \begin{bmatrix}
  \sum_{k=1}^K \frac{z_{1k}z_{1k}}{N_k} & \cdots & \sum_{k=1}^K \frac{z_{1k}z_{Nk}}{N_k} \\
  \vdots & \ddots & \vdots \\
  \sum_{k=1}^K \frac{z_{Nk}z_{1k}}{N_k} & \cdots & \sum_{k=1}^K \frac{z_{Nk}z_{Nk}}{N_k}
\end{bmatrix}
\end{aligned}
\]

As $z_{ik}$ is a binary variable with the constraint that $\sum_{k=1}^Kz_{ik} = 1$, this means that the only non-zero entries are the entries for which there is a common allocation. If one re-arranges the rows of $\mathbf{Z} L \mathbf{Z}^T$ such that points assigned to the same cluster are contiguous, i.e. into a block diagonal matrix, then the matrix has the form:

\[
\mathbf{Z} L \mathbf{Z}^T = \begin{bmatrix}
  \frac{1}{N_1} & \cdots & \frac{1}{N_1} \\
  \vdots & \ddots & \vdots \\
  \frac{1}{N_1} & \cdots & \frac{1}{N_1} &  \\
  & & & \frac{1}{N_2} & \cdots & \frac{1}{N_2} \\
  & & & \vdots & \ddots & \vdots & & \\
  & & & \frac{1}{N_2} & \cdots & \frac{1}{N_2} & & \\
  & & & & & & \ddots & & & \\
  & & & & & & & \frac{1}{N_K} & \cdots & \frac{1}{N_K} \\
  & & & & & & & \vdots & \ddots & \vdots \\
  & & & & & & & \frac{1}{N_K} & \cdots & \frac{1}{N_K}
\end{bmatrix}
\]

We consider the order of the points as stored in $X$ to be independent of the analysis and consider the rows of each matrix rearranged to give the above block diagonal structure.

This means that multiplying by $X^T$ or $\Phi$ gives a $p \times N$ matrix with the $jth$ column being the mean of the cluster the $jth$ point is assigned to. The $jth$ column is of the form:

\[
\mathrm{col}_j(\Phi Z L Z^T) = \begin{bmatrix}
  \frac{1}{N_{c_j}} \sum_{i=1}^N z_{ic_j}\phi_j(x_i) \\
  \vdots \\
  \frac{1}{N_{c_j}} \sum_{i=1}^N z_{ic_j}\phi_j(x_i) 
\end{bmatrix} = \mu_{c_j}
\]

This allows us to write the objective function in the form $f_{Obj} = \mathbf{tr}[(\Phi - M) (\Phi - M)^T]$[@gonen2014localized].

The $jth$ diagonal entry of this matrix is given by:

\[
f_{Obj_{jj}} = \sum_{j=1}^p(x_{ij} - \mu_{c_i j})^2.
\]

Thus, one can see how the statement of the function as a trace minimisation problem is merely a restatement of our original objective function.

Note that:

\[
\begin{aligned}
Z^TZ &= \begin{bmatrix}
  \sum_{i=1}^N z_{i1}z_{i1} & \cdots & \sum_{i=1}^N z_{i1}z_{iK} \\
  \vdots & \ddots & \vdots \\
  \sum_{i=1}^N z_{i1}z_{iK} & \cdots & \sum_{i=1}^N z_{iN}z_{iK}
\end{bmatrix} \\
  &= \begin{bmatrix}
  N_1 \\
  & \ddots \\
  & & N_K
\end{bmatrix} \\
  &= L^{-1}
\end{aligned}
\]

This means that $(ZLZ^T)^2 = ZLZ^T$ which is the definition of a projection. Similarly $I - ZLZ^T$ is a projection on the complement. This combined with the fact that $\mathbf{tr}[AB]=\mathbf{tr}[BA]$ means that we can simplify the objective function:

\[
\begin{aligned}
f_{Obj} &= \mathbf{tr}[\Phi(I - ZLZ^T)^2\Phi^T] \\
  &= \mathbf{tr}[\Phi(I - ZLZ^T)\Phi^T] \\
  &= \mathbf{tr}[\Phi\Phi^T] - \mathbf{tr}[\Phi Z L^{1/2} L^{1/2} Z^T \Phi^T] \\
  &= \mathbf{tr}[\Phi^T \Phi] - \mathbf{tr}[L^{1/2}Z^T\Phi^T \Phi Z L^{1/2}] \\
  &= \mathbf{tr}[\Delta] - \mathbf{tr}[L^{1/2}Z^T\Delta Z L^{1/2}]
\end{aligned}
\]

Here we have let $L^{1/2}$ be the matrix with entries of the square root of the diagonal entries of $L$.

As it is only the second part of the equation that depends upon the clustering matrix $Z$, one can formulate the equivalent maximisation problem:

\[
\max \mathbf{tr}[L^{\frac{1}{2}}Z^T\Delta Z L^{\frac{1}{2}}],
\]

subject to the contraints:

\[
\begin{aligned}
\mathbf{Z} \mathbf{1}_K &= \mathbf{1}_N \\
z_{ik} &\in \left\{0, 1 \right\}
\end{aligned}
\]

The binary variables, $z_{ik}$, make this problem very difficult to solve [@gonen2014localized]. By setting $H = ZL^{1/2}$, we can restate the problem with matrix $H$ which is no longer constrained to be binary, but as $Z^TZ = L^{-1}$ does have the constraint that $H^T H = I$. One hopes that a clustering solution may then be derived as an additional step after the optimisation problem is solved. Thus the relaxed formulation of the problem is:

\[
\begin{aligned}
& \max_H \mathbf{tr}[H^T \Delta H] \\
& \textrm{subject to }H^TH = I
\end{aligned}
\]

One can solve this by performing Kernel-PCA on the Gram matrix $\mathbf{\Delta}$ and setting $\mathbf{H}$ to the $K$ largest eigevalues. To fianlly acquire a clustering solution, one can normalise all rows of $\mathbf{H}$ to be on the unit sphere. This is done as so:

\[\hat{H}= \frac{H_{ik}}{\sqrt{\sum_{k=1}^K H_{ik}^2}}\]

One can then implements $k$-means clustering on this normalised matrix.

##### Multiple kernel $k$-means clustering

@gonen2014localized extend this concept of kernel $k$-means clustering to multiple kernel $k$-means clustering. Considered in the context of *multiview learning*, this method assumes we have $L$ different feature representations each with its own mapping function, i.e. $\{\Phi_m(\cdot)\}_{l=1}^L$. The aim is to combine these different views in a non-naive way; we wish to avoid simple concatenation. If instead we use a weighted sum such that the views that contribute the most signal are given the greatest weights with the restriction that weights are positive and sum to 1. This corresponds to replacing $\phi(x_i)$ with:

\[
\phi_\theta(x_i) = \begin{bmatrix}
  \theta_1 \phi_1(x_i)^T \\ \vdots \\ \theta_L \phi_L (x_i)^T
\end{bmatrix}.
\]

Here $\theta \in \mathbb{R}^L_+$ is the kernel weights that need to be optimised. The kernel function over the weighted mapping function becomes:

\[
\begin{aligned}
\delta_\theta (x_i, x_j) &= \langle \phi_\theta(x_i), \phi_\theta(x_j) \rangle \\
  &= \sum_{l=1}^L \langle \theta_l \phi_l(x_i), \theta_l \phi_l(x_j) \rangle \\
  &= \sum_{l=1}^L \theta_l^2 \delta_l(x_i, x_j)
\end{aligned}
\]

Where previously we could discard the $\mathbf{tr}[\Delta]$ component of the trace minimisation problem as being independant of the clustering matrix, it will now be dependent upon the kernel weights. Letting $\Delta_\theta= \sum_{l+1}^L \theta_l^2 \Delta_l$, our objective function becomes:

\[
\begin{aligned}
& f_{Obj} = \mathbf{tr}[\Delta_\theta] - \mathbf{tr}[L^{1/2}Z^T\Delta_\theta Z L^{1/2}] \\
& \textrm{subject to }H^TH = I, \sum_{l=1}^L \theta_l = 1
\end{aligned}
\]

## Problems