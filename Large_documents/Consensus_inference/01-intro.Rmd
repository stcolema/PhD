# Theory {#introduction}

* Clustering
 * Mixture models
 * Frequentist 
 * EM
 * Bayesian
 * Gibbs sampling
 * Consensus clustering
 * Integrative clustering
 * Integrative Bayesian clustering 
* Problems

In this section I explain some of the reasons one may wish to do a cluster 
analysis. I then explain what mixture models are and how Bayesian and 
Frequentist inference of these models works. Following from this I introduce the
concept of integrative clustering as an extension of cluster analysis and give 
some examples of relevant methods and the results they have delivered. This 
background is necessary to understand some of the problems that currently exist, 
and why I am suggesting Consensus inference as a solution to these.

## Cluster analysis {#clusterAnalysis}

Cluster analysis – also known as unsupervised learning – consists of defining
partitions of the items within a dataset such that the resulting sets are 
homogeneous and distinct from one another. These groups are known as clusters. 
Clustering is used in multivariate statistics to uncover _useful_ latent groups
suspected in the data or to discover _interesting_ groups of homogeneous 
observations. In large datasets, such as modern 'omics datasets, estimation of
clusters is often necessary for improved understanding and interpretation [@meinshausen2010stability]. 

```{definition, clustering, name="Clustering"}
If one has some collection of data $X=\left(x_1,\ldots,x_N\right)$, let a _clustering_ or partition of the data be defined by:

\begin{align}
	Y &=& \left\{Y_1,\ldots,Y_K\right\} \\
	Y_k &=& \left\{x_i : c_i = k \right\}  \\
	Y_i \cap Y_j &=& \emptyset \hspace{4 pt} \forall \hspace{4 pt} i,j \in \{1,\ldots,K\},  i \neq j \\
	n_k & = & \mid Y_k \mid \hspace{4 pt} \geq 1 \hspace{4 pt} \forall \hspace{4 pt} k \in \{1,\ldots,K\} \\
	\sum_{k=1}^Kn_k &=& n
\end{align}
	
In short there are $K$ non-empty disjoint sets of data, each of which is referred to as a _cluster_, the set of which form a _clustering_. This clustering may be described by a collection of allocation labels, $c=(c_1, \ldots, c_N)$. For an item $x_i$, the label $c_i=k$ states that point $x_i$ is assigned to cluster $Y_k$.

```

Cluster analysis has a myriad of applications. These include data mining, 
which started from the search for groupings of customers and products in massive
retail datasets [@fraley2002model]; document clustering and the analysis of 
internet use data; spatial proteomics [@crook2018bayesian] where subcellular
localisation of proteins is predicted by clustering-based methods; and image
analysis, where clustering is used for image segmentation and quantization. The
area of interest for me is in clustering 'omics data to 
improve understanding of disease aetiology and (ideally) help clinical practice
through identifying driving biological mechanisms or in defining subtypes within
given diseases. Defining subtypes based upon behaviour at the 
subcellular level rather than on phenotypic patterns can enable more effective 
treatment. Examples where cluster analysis has been used to propose new subtypes
include such a diverse range of diseases as Breast cancer [@cancer2012comprehensive; @berger2018comprehensive], 
Coronary artery disease [@guo2017cluster], tinnitus [@van2017cluster] and asthma
[@ortega2014cluster]. Another interesting application is the defining of subpopulations of 
patients; this application is integral to _Precision medicine_, which relies 
upon "the classification of people into subpopulations using their common 
genetic patterns, lifestyles, drug responses, and environmental and cultural
factors" [@gameiro2018precision].

Within cluster analysis there exists a large number of possible methods. 
Traditional approaches [as taught in many textbooks, see for e.g. @fidell2001using; @manly2016multivariate; @stevens2012applied] include hierarchical and non-hierarchical (normally $k$-means) 
clustering. These methods are heuristic in nature; they are not based upon a 
formal model and what guidance there is for solving important practical 
questions that arise in every cluster analysis (such as the number of clusters
present, the measure of distance / similarity between points, which solution is optimal, etc.) is subjective and informal. Despite 
this, most of the clustering that is done in practise is based upon these 
methods; they are intuitively reasonable and, perhaps more importantly, most 
clustering methods available in commercial software are of this type 
[@fraley2002model]. However, I am interested in _model-based cluster analysis_. 
This embeds the cluster analysis within a statistical framework and offers a
principled approach to many issues associated with cluster analysis. 
Speicifcally, my interest is in _mixture models_ where each subpopulation is 
described within the model by a probability distribution. In this setting, the
problems of determining the number of clusters and of choosing an appropriate 
clustering method can be recast as statistical model choice problems, and 
models that differ in numbers of components and/or in component distributions 
can be compared. Outliers are handled by adding one or more components representing a different distribution for outlying data [@fraley2002model; see an example in @crook2018bayesian].

```{definition, mixtureModels, name="Finite mixture models"}

If one is given some data $X = (x_1, \ldots, x_N)$, we assume $K$ unobserved 
subpopulations generate the data and that insights into these sub-populations 
can be revealed by imposing a clustering $Y = \left\{Y_1,\ldots,Y_K\right\}$ on
the data. It is assumed that each of the $K$ clusters can be modelled by a 
parametric distribution, $f_k(\cdot)$ with parameters $\theta_k$. Normally a
common distribution, $f(\cdot)$, is assumed across all components. We let 
membership in the $k^{th}$ cluster for the $i^{th}$ individual be denoted by 
$c_i = k$. The full model density is then the weighted sum of the probability 
density functions where the weights, $\pi_k$, are the proportion of the total 
population assigned to the $k^{th}$ cluster. Then for item $x_i$ in a _finite 
mixture model_:

\begin{align}
	p(x_i|c_i = k) &= \pi_k f(x_i | \theta_k) \\
	p(x_i) &= \sum_{k=1}^K \pi_k f(x_i | \theta_k)
\end{align}

```

The flexibility in choice of $f(\cdot)$ means that these models can be applied in many different scenarios. 

<!-- The unobserved variable $c$ is (normally) the object we are most interested in performing inference upon. Thus clustering can be seen as a _latent variable analysis_. -->

### Frequentist inference

The most common form of inference, traditionally, is Frequentist; specifically Maximum Likelihood (*ML*) based. In mixture models, the model log-likelihood is:

\begin{align}
l(\pi, \theta)=\sum_{i=1}^N \ln p(x_i | \theta, \pi)
\end{align}

The likelihood equations based upon the derivative of this object with respect to the parameters $(\theta, \pi)$ are too complicated for the normal methods of maximisation [@stahl2012model] and thus inference is normally performed using Expectation-Maximisation (*EM*), a two-step iterative process. 

This inference is very quick but suffers from several problems:

1. Difficult to combine multiple sources of information [see @singh2005combining for an  example of how to consider this under the Frequentist paradigm];
2. Sensitive to initialisation, prone to finding local maxima due to the nature of the EM algorithm - normally these models are run from multiple diffferent initialisations. In this case observing the same log‐likelihood values from multiple starting points increases confidence that the solution is a global maximum. Using a some heuristic clustering method to initialise the method [such as in @mclust2016] also offers stability; and
3. Difficulty in converging - singularities can occur in the likelihood function; these are points where the likelihood becomes infinite, resulting in degenerate distributions. Singularities occur when the number of parameters to be estimated is large in relation to the sample size [@stahl2012model], models with unrestrained covariances and large numbers of components are prone to this problem. @fraley2007bayesian suggest using Bayesian inference as a means of overcoming this issue, reccomending use of a _maximum a posteriori_ (*MAP*) estimate instead of an ML estimate.

#### Expectation maximisation

Perform inference

### Bayesian inference

An alternative to the Frequentist paradigm is Bayesian inference. This relies upon a number of statements:

```{definition, condProb, name = "Conditional probability"}

For events $A$, $B$ and $C$:

\begin{eqnarray}
p(A, B | C) &=& p(B | A, C) p (A | C).
(\#eq:condProb) 
\end{eqnarray}

```

```{definition, totalProb, name = "Law of total probability"}

For a countable partition of the sample space $\mathcal{B}$, $\{B_q\}_{q=1}^Q$, and events $A$ and $C$:
  
\begin{eqnarray}
p(A | C) &=& \sum_{q=1}^Q p(A | B_q, C) p(B_q | C).
\end{eqnarray}

If it is the case that each member of the partition has a measure of 0 (i.e. the partition is uncountable), then:


\begin{eqnarray}
p(A | C) &=& \int_{\mathcal{B}} p(A | B_q, C) p(B_q | C) dB_q.
(\#eq:totalProb)
\end{eqnarray}

```

```{definition, bayesTheorem, name = "Bayes' Theorem"}

From definition \@ref(def:condProb):

\begin{eqnarray}
p(A|B, C) &=& \frac{p(A, B | C)}{P(B|C)} \\
&=& \frac{p(B | A, C) p (A | C)}{P(B|C)} 
(\#eq:bayesTheorem)
\end{eqnarray}

Now applying definition \@ref(def:totalProb):

\begin{eqnarray}
\frac{p(B | A, C) p (A | C)}{P(B|C)}  &=& \frac{p(B | A, C) p (A | C)}{\int_A P(B|A', C) dA'}
(\#eq:bayesTheoremIntegral)
\end{eqnarray}

```

<!-- * Bayes' Theorem (combining \@ref(eq:condProb) and \@ref(eq:totalProb)): -->

<!-- \begin{eqnarray} -->
<!-- p(A|B, C) &=& \frac{p(A, B | C)}{P(B|C)} \notag \\ -->
<!-- &=& \frac{p(B | A, C) p (A | C)}{P(B|C)} -->
<!-- (\#eq:bayesTheorem) \\ -->
<!-- &=& \frac{p(B | A, C) p (A | C)}{\int_A P(B|A', C) dA'} -->
<!-- \end{eqnarray} -->

Bayesian inference is focuses upon the concept of updating one's beliefs via equation \@ref(eq:bayesTheorem). If one has a prior belief about the behaviour of some 
parameter $\theta$, captured in the distribution $p(\theta)$, and then
observe some data $X$, then one describes the change in one's belief about $\theta$ given the observed variable $X$ by combining the distributions associated with the two sources of information (one currently observed, the other currently unobserved):

\begin{eqnarray}
p(\theta | X) = \frac{p(X | \theta) p(\theta)}{\int_{\Theta} p(X | \theta') d \theta'}.
(\#eq:posteriorDistribution)
\end{eqnarray}

In this case the following terminology is used:

* $p(\theta)$ is the _prior distribution_ of $\theta$ (so-called as it encompasses one beliefs prior to observing $X$);
* $p(X | \theta)$ is the _likelihood_ of $X$;
* $\int_{\Theta} p(X | \theta') d \theta' = p(X)$ is the _marginal likelihood_ of $X$ as it is the likelihood with $\theta$ marginalised out; and
* $p(\theta | X)$ is the _posterior distribution_ of $\theta$ (as it denotes the belief about $\theta$ one has _a posteriori_, that is after observing empirical fact).

Normally the posterior distribution is too difficult to solve analytically and one must use either _sampling based_ or _variational inference_ methods to solve this.

A common class of sampling based methods are the Markov-Chain Monte Carlo methods. These methods draw samples from Markov Chains constructed such that the stationary distribution is the posterior distribution. Samples are drawn to use Monte Carlo integration to describe the posterior distribution.

#### Monte Carlo integration
The original Monte Carlo method was developed as a means to solving integrals by use of random number generation [@metropolis1949monte].

```{definition, monteCarloIntegration, name = "Monte Carlo integration"}

Suppose there is some complex integral one wishes to solve on some interval, $(a,b)$:
  
\begin{align}
\int_a^b h(x) dx
\end{align}

If one can decompose $h(x)$ into the product of some more simple function $f(x)$ and a probability density $p(x)$ where both are defined over $(a,b)$, it can then be stated:
  
\begin{align}
\int_a^bh(x)dx = \int_a^bf(x)p(x)dx = \mathbb{E}_{p(x)}\left[f(x)\right]
(\#eq:monteCarloBasis)
\end{align}
  
By the Law of Large Numbers one can approximate this expectation of $f(x)$ over $p(x)$ by drawing $N$ random variables $x = (x_1,\ldots,x_N)$ from $p(x)$ for some large $N$; thus \@ref(eq:monteCarloBasis) becomes:
  
\begin{align}
\int_a^bh(x)dx = \mathbb{E}_{p(x)}\left[f(x)\right] \approx \frac{1}{N}\sum_{i=1}^Nf(x_i)
(\#eq:monteCarloIntegration)
\end{align}
  
This is Monte Carlo integration.

```

#### Markov chains

For the following definitions, consider some process $X$ which generates observations $x_1, \ldots, x_N$ which are observed at discrete timepoints $T=(t_1, \ldots, t_N)$ (where $t_i < t_j \iff i < j$). Let $S=\{s_q\}_{q=1}^Q$ be the sample space of $X$ and let $p_{(i,j)}=p(X_{t+1}=s_i | X_t = s_j)$ be the transition probability from state $s_j$ to $s_i$ in a single time step.

```{definition, markovProperty, name = "Markov property"}
$X$ is said to have the _Markov property_ if for a time point $t_{j+1}$, having observed all $j$ previous states, the transition probability is defined only by the preceding state, i.e. for any states $s_i, s_j, s_k \in S$:
  
\begin{align}
p(x_{t_{j+1}} = s_j | x_{t_1} = s_k, \ldots, x_{t_i} = s_i) = p(x_{t_{j+1})} = s_j | x_{t_i} = s_i) \hspace{4pt} \forall \hspace{4pt} i,j \in {1, \ldots, N}
\end{align}

Thus the future state depends only upon the present state. The process is said to be memoryless as past states do not affect future outcomes, given the current state. 

```

```{definition, markovProcess, name = "Markov process"}
If a process $X$ has the Markov property (\@ref(def:markovProperty)), then it is referred to as a _Markov process_. 

```

```{definition, markovChain, name = "Markov chain"}
If $X$ is a Markov process, than the sequence of variables $(x_1,\ldots, x_N)$ generated from $X$ form a _Markov chain_.
```

```{definition, nStepTransition, name = "$n$-step transition probability"}
Let $\mathbf{P}$ be an $|S| \times |S|$ matrix composed of the transition probabilities $p_{(i,j)}$ for all $i,j \in {1,\ldots, |S|}$. Define the $n$-step transition probability $p_{(i,j)}^n$ as the probability that the process is in state $s_j$ given it was in state $s_i$ at a remove of $n$ steps, i.e.:
  
\begin{align}
p_{(i,j)}^n = p(x_{t_{i+n}} = s_j | x_{t_i} = s_i)
\end{align}

```

```{definition, irreducible, name = "Irreducible"}
A Markov chain is said to be _irreducible_ if $p_{ij}^n > 0 \; \forall \; i,j \in \mathbb{N}$.
```

```{definition, communicate, name = "Communicate"}
If a Markov chain is irreducible, this means that there always exists a possible path from any state $s_i$ to every other state $s_j$. If this is true the states are said to _communicate_. 
```

```{definition, aperiodic, name = "Aperiodic"}
For a Markov chain, if the number of steps between two states is not required to be the multiple of some integer than the chain is considered _aperiodic_.

```

```{definition, detailedBalance, name = "Detailed balance"}
Let $X$ be a Markov process. The constraint:

\begin{align} 
p(x_{t_{n+1}} = s_i | x_{t_n} = s_j)  p(x_{t_{n+1}} = s_i)  =  p(x_{t_{n+1}} = s_j | x_{t_n} = s_i)  p(x_{t_{n+1}} = s_j)
(\#eq:detailedBalance)
\end{align}

is known as the _detailed balance_.

```


```{definition, reversible, name="Reversible"}

The chain has the _reversible_ property if the detailed balance holds for all states.

```

Reversibility is sufficient condition for a unique, stationary distribution. This means that the probability of being in any given state for the process is independent of the starting conditions given sufficient time and that the transition probabilities have approached some limiting value. 

```{definition, stationaryDistribution, name = "Staitonary distribution"}

A stationary distribution, $\pi$, is a $|S|$-vector where the $i^{th}$ entry is the probability $\pi_i = p(x_{t_{n+1}} = s_i)$. This means that:

\begin{align}
\pi_i &\geq 0 \: \forall \: i \in (1,\ldots,|S|) \\
\sum_{i=1}^{|S|}\pi_i &= 1
\end{align}

Furthermore the stationary distribution is invariant under the operation of the transition matrix $\mathbf{P}$ upon it:

\begin{align}
\pi = \pi\mathbf{P}.
(\#eq:stationaryDistribution)
\end{align}

Thus the distribution, $\pi$, remains unchanged as time progresses and more states are observed.

If $\pi$ is a stationary distirbution associated with a reversible Markov process $X$, then for any initial distribution, $\pi_0$, and transition probability matrix $P$,

\begin{align}
\lim_{n\to\infty} \pi_0 \mathbf{P}^n = \pi.
\end{align}

In this case the chain is said to converge to $\pi$. Once the chain is sampling from $\pi$, then as this distribution is stationary, the samples drawn should be independent of the previous draws as can be seen in \@ref(eq:stationaryDistribution).

```

For this reason auto-correlation is one of the measures of convergence within a Markov chain.

#### Markov-Chain Monte Carlo methods

Markov-Chain Monte Carlo (MCMC) methods developed as a method to obtain samples from some complex distribution $p(x)$ for the decomposition suggested in \@ref(eq:monteCarloBasis). The goal in the following is to draw samples from some distribution $p(\theta)$ where there is some distribution $f(\theta)$ such that:
  
\begin{align}
p(\theta) = \frac{f(\theta)}{K} 
\end{align}

For some constant $K$ where $K$ may not be known and is often difficult to compute.

The Metropolis algorithm [@metropolis1949monte; @metropolis1953equation] generates a sequence of draws from $p(\theta)$ using the following steps:
  
1. Initialise with some arbitrary value $\theta_0$ with the condition that $f(\theta_0) > 0$ and also choose some probability density $q(\theta_1|\theta_2)$ as the _jumping distribution_ or _proposal density_. For the Metropolis algorithm one demands that this is symmetric (i.e. $q(\theta_1 | \theta_2) = q(\theta_2 | \theta_1)$).
2. For each iteration, $t$:
  1. Using the current value $\theta_{t-1}$, sample a candidate point, $\theta^*$, from  $q(\theta^* | \theta_{t-1})$.
  2. Calculate the _acceptance ratio_ for the new value $\theta^*$:
  
  \begin{align}
  \alpha = \frac{p(\theta^*)}{p(\theta_{t-1})} = \frac{f(\theta^*)}{f(\theta_{t-1})}
  \end{align}
  
  Note that as the proportionality constant is the same for all $\theta$ that this is an equivalence rather than proportional relationship.
  3. Accept the new value $\theta^*$ with probability equal to $\min(\alpha, 1)$. Generate a number $u$ from the uniform distribution on $[0,1]$ and accept if $\alpha \geq u$, else reject.

This generates a Markov chain $(\theta_0,\ldots,\theta_k,\ldots, \theta_n)$ as each iteration is conditionally independent of all others given the sample from the iteration preceding it. A stationary distribution is reached after a \emph{burn-in} period of $k$ steps (for some $k \in \mathbb{N}$) and all following samples come from $p(\theta)$ (i.e. the vector $(\theta_{k+1},\ldots,\theta_n)$ are samples from $p(\theta)$). Knowing $k$ is a non-trivial issue; some arbitrary large number of burn-in iterations is often assumed erring on the side of caution, although there exists techniques to help in diagnosing what value this should be (see section \ref{sec:additional_theory:sub_sec:convergence:sub_sub_sec:ess}). The samples generated are highly correlated with other samples from within a close range of iterations. To avoid recording this duplicate information, often only every $l$th sample is recorded for some small $l$ (this process is referred to as _thinning_).

@hastings1970monte extends this method to allow asymmetric proposal densities, in which case the acceptance ratio changes to:
  
\begin{align} 
\alpha = \min\left(\frac{f(\theta^*) q(\theta^*|\theta_{t-1})}{f(\theta_{t-1})q(\theta_{t-1}|\theta^*)}, 1\right)
(\#eq:metropolisHastingsAlpha)
\end{align}

This extension proposed is known as the Metropolis-Hastings algorithm. @geman1984stochastic use a special case of this, taking $\alpha = 1 \; \forall \; \theta^*$, accepting all proposed values. This is known as a _Gibbs sampler_.

These methods are useful in a Bayesian context as one is interested in a rather complex distribution, the posterior, and know two simpler quantities, the prior and the likelihood, that the posterior is proportional to (as shown in equation \@ref(eq:bayesTheorem)). Thus one can use MCMC methods to sample directly from the posterior distribution without directly calculating the normalising constant.

#### Gibbs sampling

Consider some vector of variables $\theta = (\theta_1, \ldots, \theta_q)$ that we wish to perform inference upon. One method of implementing this is to perform _Gibbs sampling_ [@geman1984stochastic]. This works by iterating over each variable, updating it based upon the current values of all the other variables and then repeating this a large number of times (ideally an infinite number of times). 

Let $\theta^{(j)} = (\theta^{(j)}_1, \ldots, \theta^{(j)}_q)$ be the sampled values of $\theta$ in the $j^{th}$ iteration of Gibbs sampling. Our update for $\theta^{(j)}_i$ is conditioned on all the current values for the other variables - this means that the first $(i - 1)$ variables
have already been updated $j$ times, but the remaining $q - i$ variables are still based upon the
$(j - 1)^{th}$ iteration, i.e. our update probability is of the form:

\begin{eqnarray}
p(\theta^{(j)}_i | \theta^{(j)}_1, \ldots, \theta^{(j)}_{i-1}, \theta^{(j)}_{i+1}, \ldots, \theta^{(j)}_q).
(\#eq:updateProbs)
\end{eqnarray}

Consider Gibbs sampling performed upon a mixture model of $K$ components clustering observed variables $X=(x_1, \ldots, x_N)$. In this case the unobserved variables present are the $N$ allocation variables, $z=(z_1, \ldots, z_N)$, $K$ component parameters $\theta=(\theta_1, \ldots, \theta_K)$ (possibly each $\theta_k$ is a vector of parameters) and $K$ component weights $\pi=(\pi_1, \ldots, \pi_K)$. I use the notation $x_{-i}=(x_1,\ldots, x_{i-1}, x_{i+1}, \ldots, x_N)$. The hierarchical model is described in figure \@ref(fig:hierarchical-model).

```{r, hierarchical-model, echo=FALSE, engine="tikz", out.width="90%", fig.cap="The mixture model we are interested in performing inference upon.", fig.align="center", cache=T}
 \usetikzlibrary{arrows}
\usetikzlibrary{fit,positioning}
\centering
\begin{tikzpicture}[scale=.7, auto,>=latex']
\tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 22mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
 \node[main] (pi) {$\pi_k$ };
 \node[main] (z_i) [below right=of pi] {$z_i$};
 \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
 \node[main] (theta) [above left=of x_i] {$\theta_k$};
 \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
 \node[rectangle, inner sep=-0.5mm, fit= (z_i) (x_i),label=above right:$N$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm,draw=black!100, fill=blue! 25, fill opacity=0.2, fit= (z_i) (x_i)] {};
 \node[rectangle, inner sep=-0.8mm, fit= (theta) (pi),label=above right:$K$, xshift=14mm] {};
 \node[rectangle, inner sep=4.8mm, draw=black! 100, fit= (theta)(pi)] {};
 \node[main] (pi) {$\pi_k$ };
 \node[main, fill = white!1] (z_i) [below right=of pi] {$z_i$};
 \node[main, fill = black!10] (x_i) [right=of z_i] {$x_i$};
 \node[main] (theta) [above left=of x_i] {$\theta_k$};
 \path (pi) edge [connect] (z_i)
		(z_i) edge [connect] (x_i)
		(theta) edge [connect] (x_i);
\end{tikzpicture}

```

In the case of a mixture model, often the *only* variables of interest are the unseen allocation variables, $z$. This means that we are not interested in the samples produced for the other variables.

Consider the sampling of $z_i$. As this can only hold a relatively small number of values we
can consider the probability for each possible $k$. Based upon the relationships depicted in figure \@ref(fig:hierarchical-model) and equations \@ref(eq:totalProb) and \@ref(eq:bayesTheorem):

\begin{eqnarray}
		p(z_i = k | x, z_{-i}, \pi) &=& \frac{p(z_i = k | \pi, x_{-i}, z_{-i}, \pi) p(x_i | z, x_{-i}, \pi)}{p(x_i | x_{-i}, z_{-i}, \pi)} \\
		&\propto& p(z_i = k | \pi_k) \int_{\theta} p(x_i | \theta, z, x_{-i}, \pi) p(\theta | z, x_{-i}, \pi) d \theta \\
		&=& \pi_k \int_{\theta} p(x_i | \theta) p(\theta | z, x_{-i}) d \theta
	(\#eq:ziCondProb) 
\end{eqnarray}

Note that the term in the denominator, $p(x_i | x_{-i}, z_{-i}, \pi)$ is independent of $z_i$ and thus the same for all values of $k$. The integral in \@ref(eq:ziCondProb) is the posterior predictive probability for $x_i$ given the items in the component; thus one may consider \@ref(eq:ziCondProb) as how well does each component predict $x_i$, or as a model comparison problem.

 An alternative way of describing this involves the ratio of marginal likelihoods. As we are component specific (given $z_i = k$), I drop the $z$ and $\pi$ from my conditional and assume we are referring only to the $x_j$ for which $z_j = k$.
 
\begin{eqnarray}
p(x_i | z, x_{-i}, \pi) &=& \frac{p(x | z)}{p(x_{-i} | z)} \\
&=& \frac{\int_\theta p(x | \theta) p(\theta) d\theta}{\int_\theta p(x_{-i} | \theta) p(\theta) d\theta}
\end{eqnarray}

Therefore we can write the posterior predictive distribution as this ratio of marginal likelihoods:
 
\begin{eqnarray}
p(z_i = k | x, z_{-i}, \pi) \propto \pi_k \frac{p(x)}{p(x_{-i})}
\end{eqnarray}

Thus we can create a $K$-vector of probabilities for the allocation of $x_i$ to each component by finding the ratio of marginal likelihoods for each component including and excluding $x_i$, and multiplying these by the associated component weight, $\pi_k$. One can normalise these by dividing by the sum of the members of this vector due to the independence of the normalising constant from $z_i$.

#### Example: Gaussian mixture models

In this section we derive the marginal likelihood for a component of the Gaussian mixture model assuming that the mean $\mu$ and the precision $\lambda$ are unknown. Before we can continue we state the associated probability density functions of the Normal and Gamma distributions:
	
\begin{eqnarray}
\mathcal{N}(x | \mu, \lambda^{-1}) &=& \sqrt{\frac{\lambda}{2\pi}} \exp \left(- \frac{\lambda}{2}(x - \mu) ^ 2\right) \\
Ga(x | \alpha, \mathrm{rate }= \beta) &=& \frac{\beta^\alpha}{\Gamma(\alpha)} x ^{\alpha - 1} \exp(-\beta x)
\end{eqnarray}

The model likelihood for $n$ observations is:
	
\begin{eqnarray}
p(x | \mu, \lambda) &=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \sum_{i=1}^n(x_i - \mu)^2 \right)
(\#eq:ngammaLikelihood1)
\end{eqnarray}

Considering specifically the sum within the exponent here in equation \@ref(eq:ngammaLikelihood1), and letting $\bar{x}$ be the sample mean:

\begin{eqnarray}
	\sum_{i=1}^n(x_i - \mu)^2 &=& \sum_{i=1}^n(x_i - \bar{x} + \bar{x} - \mu)^2 \\
	&=& \sum_{i=1}^n\left[(x_i - \bar{x})^2 + (\mu - \bar{x})^2 + 2(x_i \bar{x} - \bar{x}^2 - x_i \mu + \bar{x} \mu)\right] \\
	&=& n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2
\end{eqnarray}

Substituting this back into equation \@ref(eq:ngammaLikelihood1), we have:

\begin{eqnarray}
p(x | \mu, \lambda) &=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \left[n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2\right] \right) 
(\#eq:likelihood)
\end{eqnarray}
		
The conjugate prior for this model is the \emph{Normal-Gamma} distribution. This has the probability density function:
	
\begin{eqnarray}
	NG(\mu, \lambda | \mu_0, \kappa_0, \alpha_0, \beta_0) &\coloneqq& \mathcal{N}(\mu | \mu_0, (\kappa_0 \lambda)^{-1})Ga(\lambda | \alpha_0, \beta_0) \\
	&=& \sqrt{\frac{\kappa_0 \lambda}{2\pi}} \exp \left(- \frac{\kappa_0 \lambda}{2}(\mu - \mu_0) ^ 2\right) \\
	& & \hspace{3mm} \times \hspace{3mm} \frac{\beta_0 ^{\alpha_0}}{\Gamma(\alpha_0)} \lambda ^{\alpha_0 - 1} \exp(-\beta_0 \lambda) \\
	&=& \sqrt{\frac{\kappa_0}{2\pi}} \frac{\beta_0 ^ {\alpha_0}}{\Gamma(\alpha_0)} \\
	& & \hspace{3mm} \times \hspace{3mm} \lambda^{\alpha_0 - \frac{1}{2}} \exp\left(-\frac{\lambda}{2}\left[\kappa_0 (\mu - \mu_0)^2 + 2 \beta_0\right]\right) 
(\#eq:priorPDF)
\end{eqnarray}

Here the normalising constant is:
	
\begin{eqnarray}
Z_0^{-1} = \sqrt{\frac{\kappa_0}{2\pi}} \frac{\beta_0 ^ {\alpha_0}}{\Gamma(\alpha_0)}
\end{eqnarray}

One can see that this function in equation \@ref(eq:priorPDF) will naturally complement the likelihood described in equation \@ref(eq:likelihood). 

To derive the poisterior probability, we apply Bayes' theorem (equation \@ref(eq:bayesTheorem)):
	
\begin{eqnarray}
	p(\mu, \lambda | x) &\propto& p(x | \mu, \lambda) p(\mu, \lambda) \\
	&=& \left(\frac{\lambda}{2\pi}\right)^{\frac{n}{2}}\exp\left(-\frac{\lambda}{2} \left[n(\mu - \bar{x})^2 + \sum_{i=1}^n (x_i - \bar{x})^2\right] \right) \\
	&& \hspace{3mm} \times \hspace{3mm} \lambda^{\alpha_0 - \frac{1}{2}} \exp\left(-\frac{\lambda}{2}\left[\kappa_0 (\mu - \mu_0)^2 + 2 \beta_0\right]\right) \\
	&\propto& \lambda^{\alpha_0 + \frac{n}{2} - \frac{1}{2}} \exp \left\{-\frac{\lambda}{2}\left[n(\mu - \bar{x})^2 + \kappa_0(\mu - \mu_0)^2 + \sum_{i=1}^n(x_i - \bar{x})^2 + 2\beta_0\right]\right\} 
	(\#eq:posteriorStep1)
\end{eqnarray}
	
We try and anticipate the parameters of our posterior. 
In Bayesian inference there is no fundamental difference between the prior and the likelihood - both are distributions (bar that one corresponds to observed variables and the other to unobersved variables). Thus we would expect that if we wish to combine these quantities and we have conjugacy, that the intuition for what each parameter means within the observed part of our equation should translate onto the unobserved distribution and in this way help us to have an intution for how to combine the two quantities. Thus if we think as the prior as being based upon some previously observed data, then the $\kappa_0$ parameter would be the number of observations in this data and $\mu_0$ the mean observed. In this case we would expect that the updated mean, $\mu_n$, should correspond to a weighted average of the sample mean of the data currently being observed ($\bar{x}$), and that in our prior ($\mu_0$). Similarly, the ``total'' number of observations between the prior data and the current data, $\kappa_n$, should be the sum of the number of samples in the dataset under observation, $n$, and the number of samples in our prior dataset, $\kappa_0$. (This logic implies that including a very small $\kappa_0$ is one way of including an uninformative prior.) Describing this mathematically:
	
\begin{eqnarray}
	\kappa_n &=& \kappa_0 + n \\
	\mu_n &=& \frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}
\end{eqnarray}
	
If this is the case we would expect a term in the exponent:
	
\begin{eqnarray}
	\kappa_n (\mu - \mu_n)^2 &=& (\kappa_0 + n)\left[\mu - \left(\frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}\right)\right]^2 \\
	&=& (\kappa_0 + n) \left(\mu^2 - 2\mu(\kappa_0 \mu_0 + n \bar{x}) + \left(\frac{\kappa_0 \mu_0 + n \bar{x}}{\kappa_0 + n}\right)^2\right) %\\
	%&=& \kappa_0 \mu^2 + n\mu^2 - 2\mu (\kappa_0 + n) (\kappa_0 \mu_0 + n \bar{x}) - \frac{(\kappa_0\mu_0)^2 + 2n\kappa_0\mu_0 \bar{x} + (n\bar{x})^2}{\kappa_0 + n}
\end{eqnarray}

Returning to equation \@ref(eq:posteriorStep1) and considering the part of the exponent containing $\mu$:
	
\begin{eqnarray}
	\kappa_0 (\mu - \mu_0) ^ 2 + n (\mu - \bar{x}) ^2 &=& \mu^2 (\kappa_0 + n) + n\bar{x}^2 + \kappa_0 \mu_0^2 -2n\mu\bar{x} -2 \kappa_0 \mu \mu_0 \\
	&=& (\kappa_0 + n) \left[\mu^2 + \frac{n\bar{x}^2}{\kappa_0 + n} + \frac{\kappa_0 \mu_0^2}{\kappa_0 + n} - 2 \mu \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right] 
(\#eq:posteriorExponentExpansion)
\end{eqnarray}
	
Notice that several of the desired terms are present! If one focuses upon the components of this equation that do not fit the expected form:
	
\begin{eqnarray}
	\frac{n\bar{x}^2}{\kappa_0 + n} + \frac{\kappa_0 \mu_0^2}{\kappa_0 + n} &=& \frac{n(\kappa_0 + n)\bar{x}^2 + \kappa_0(\kappa_0 + n)\mu_0^2}{(\kappa_0 + n)^2} \\
	&=& \frac{n^2 \bar{x}^2 + 2\kappa_0 n \mu_0 \bar{x} + \kappa_0^ 2 \mu_0 ^ 2}{(\kappa_0 + n)^2} 2 \frac{\kappa_0 n \bar{x}^2 + \kappa_0 n\mu_0^2 - 2\kappa_0 n \mu_0 \bar{x}}{(\kappa_0 + n)^2} \\
	\\
	&=& \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right)^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{(\kappa_0 + n)^2}
\end{eqnarray}

Substituting this result into equation \@ref(eq:posteriorExponentExpansion) yields:
	
\begin{eqnarray}
\kappa_0 (\mu - \mu_0) ^ 2 + n (\mu - \bar{x}) ^2 &=& (\kappa_0 + n) \left[\mu + \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right]^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{\kappa_0 + n}
\end{eqnarray}

Returning to equation \@ref(eq:posteriorStep1):

\begin{eqnarray}
	p(\mu, \lambda | x) &\propto& \lambda^{\alpha_0 + \frac{n}{2} - \frac{1}{2}} \exp \left\{-\frac{\lambda}{2}\left[
	(\kappa_0 + n) \left(\mu + \left(\frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n}\right) \right)^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{\kappa_0 + n} + \sum_{i=1}^n(x_i - \bar{x})^2 + 2\beta_0\right]\right\} 
	(\#eq:posterior)
\end{eqnarray}

This is the probability density function of a Normal-Gamma distribution.

\begin{eqnarray}
	p(\mu, \lambda| x) &=& NG(\mu, \lambda | \mu_n, \kappa_n, \alpha_n, \beta_n) \\
	\mu_n &=& \frac{n\bar{x} + \kappa_0 \mu_0}{\kappa_0 + n} \\
	\kappa_n &=& \kappa_0 + n \\
	\alpha_n &=& \alpha_0 + \frac{n}{2} \\
	\beta_n &=& \beta_0 + \frac{1}{2}\sum_{i=1}^n(x_i - \bar{x})^2 + \frac{n\kappa_0\left(\bar{x} - \mu_0 \right)^2}{2(\kappa_0 + n)}
	\end{eqnarray}
	
As this posterior distribution is $NG(\mu_n,\kappa_n, \alpha_n, \beta_n)$, we know that the associated normalising constant is:

\begin{eqnarray}
Z_n &=& \frac{\Gamma(\alpha_n)}{\beta_n^{\alpha_n}}\sqrt{\frac{2\pi}{\kappa_n}}.
\end{eqnarray}

To derive the _Marginal Likelihood_, consider the posterior but track the normalising constants (such as the $(2\pi)^{-n/2}$ in the likelihood). Denote the prior, likelihood and posterior less their normalising constants by $p'(\mu, \lambda)$, $p(x | \mu, \lambda)$ and $p'(\mu, \lambda | x)$ respectively:

\begin{eqnarray}
\frac{1}{Z_n} p'(\mu, \lambda | x) &=& \frac{1}{p(x)}\frac{1}{Z_0} p'(\mu, \lambda)\left(\frac{1}{2\pi}\right)^{\frac{n}{2}} p'(x | \mu, \lambda)
\end{eqnarray}

We know that the product of the unnormalised prior and the unnormalised likelihood give the right hand side of equation \@ref(eq:posterior), and that this is our unnormalised posterior, $p'(\mu, \lambda | x)$. Thus:
	
\begin{eqnarray}
\frac{1}{Z_n} p'(\mu, \lambda | x) &=& \frac{1}{p(x)}\frac{1}{Z_0} \left(\frac{1}{2\pi}\right)^{\frac{n}{2}} p'(\mu, \lambda | x) \\
\implies p(x) &=& \frac{Z_n}{Z_0}(2\pi)^{-\frac{n}{2}} 
\end{eqnarray}

Thus our marginal likelihood is the ratio of the posterior normalising constants to the product of those of the likelihood and prior. Expanding this we finally have:
	
\begin{eqnarray}
	p(x) &=& \frac{\Gamma(\alpha_n)}{\beta_n^{\alpha_n}} \sqrt{\frac{2\pi}{\kappa_n}} \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \sqrt{\frac{\kappa_0}{2\pi}} (2\pi)^{-\frac{n}{2}} \\
	&=& \frac{\Gamma(\alpha_n)}{\Gamma(\alpha_0)}\frac{\beta_0^{\alpha_0}}{\beta_n^{\alpha_n}} \sqrt{\frac{\kappa_0}{\kappa_n}}(2\pi)^{-\frac{n}{2}}
\end{eqnarray}

#### Dirichlet process

Dirichlet process [@ferguson1973bayesian]

Multinomical dirichlet allocation model

Current Markov chain Monte Carlo (MCMC) algorithms for Bayesian inference in Dirichlet process (DP) mixture models (see e.g. @neal2000markov; @jain2004split) are computationally costly, and often infeasible for large datasets [@crook2019fast].
	
#### Variational inference

@blei2006variational and @blei2017variational

## Consensus clustering 

@monti2003consensus and @wilkerson2010consensusclusterplus

## Integrative clustering

sequential vs post-hoc vs simultaneous, local vs global

### iCluster
@shen2009integrative and @mo2018fully

### Bayesian integrative clustering


#### Multiple Dataset Integration

@kirk2012bayesian and @mason2016mdi

#### Clusternomics

@gabasova2017clusternomics

#### Bayesian Consensus Clustering

@lock2013bayesian

#### MOFA

@argelaguet2019mofaplus and @argelaguet2018multi

### COCA

@cancer2012comprehensive

#### KLIC

@cabassi2019multiple

## Problems