# Metrics {#metrics}

In this study we will be considering both within-simulation and across-simulation performance for the $N_{sim}$ simulations run under each scenario.

The aim of each model is uncovering the true clustering of the data, $C'$. The performance of each model may be judged by compaing the predicted clustering, $C^*$, to $C'$ using the adjusted rand index (**${ARI(C^*, C')}$**).

Robustness may be judged by the uncertainty on this estimate. I compare the Frobenius product between the coclustering matrix of the true clustering with the posterior similarity matrix (from the Bayesian inferene) and the consensus matrix (from the Consensus inference). For the Bayesian models I also will record the Gelman-Rubin shrinkage parameter ($\hat{R}$) and the Geweke statistic as measures of model convergence.

Each model will be timed in milliseconds.

## Clustering performance

### Rand Index

A popular metric for comparing the similarity of two clusterings of the data is the _Rand index_ [@rand1971objective]. Let $X=(x_1,\ldots,x_N)$ be a dataset of $N$ items. Define two partitions of $X$ into subsets, $Y=\{Y_1,\ldots,Y_K\}$ and $Y'=\{Y_1',\ldots, Y_{K'}'\}$, let $C=(c_1,\ldots,c_N)$ and $C'=(c_1'\ldots,c_N')$ denote the allocation of each item in $X$ to one of these subsets; then each allocation label pair, $(c_i, c_i') \in [(1, 1),\ldots, (1, K'), (2, 1), \ldots, (K, K')] \forall i \in [1, N]$. For any two points $x_i$ and $x_j$ there can exist one of three possible scenarios regarding their allocation under each partition. Let $\gamma_{ij}$ be a measure between the two points $x_i$ and $x_j$. For the two points, they can have 
  
1. The same label in both clusterings ($c_i = c_j \land c'_i = c'_j$) ($\gamma_{ij}=1$);
2. Different labels in both ($c_i \neq c_j \land c'_i \neq c'_j$) ($\gamma_{ij}=1$); or 
3. The same label in one but not in the other ($c_i \neq c_j \land c'_i = c'_j \lor c_i = c_j \land c'_i \neq c'_j$) ($\gamma_{ij}=0$).

Thus @rand1971objective proposed counting the number of times any two points have the relationship described by points 1 and 2 from the above list and finding the proportion of these compared to the number of all possible combinations. More formally, this is:
	
\begin{eqnarray}
A \binom{N}{2}^{-1} & = & \frac{1}{\binom{N}{2}} \sum_{i=1}^{N-1}\sum_{j=i + 1}^N\gamma_{ij}
(\#eq:randIndex)
\end{eqnarray}
	
<!-- This is the quantity of points in agreement between the two clusters. -->
This can be envisioned as a $K \times K'$ contingency table of the count of overlapping points, as shown in table \@ref(tab:randContingency). Table \@ref(tab:randContingency) uses the following notation:

1. $n_{ij}$ is the number of points that have membership in $Y_i$ in clustering $Y$ and $Y'_j$ in clustering $Y'$;
2. $n_{\cdot j}$ is the number of points in cluster $Y'_j$ in clustering $Y'$;
3. $n_{i \cdot}$ is the number of points in cluster $Y_i$ in clustering $Y$; and
4. $n_{\cdot \cdot} = n$ is the number of points in clusterings $Y$ and $Y'$.


<!-- |          | $Y_1'$       | $Y_2'$       | $\cdots$ | $Y_{K'}'$      | Sums              | -->
<!-- | -------- | :----------: | :----------: | :------: | :------------: | :---------------: | -->
<!-- | $Y_1$    | $n_{11}$     | $n_{12}$     | $\cdots$ | $n_{1K'}$      | $n_{1\cdot}$      | -->
<!-- | $Y_1$    | $n_{21}$     | $n_{22}$     | $\cdots$ | $n_{2K'}$      | $n_{2\cdot}$      | -->
<!-- | $\vdots$ | $\vdots$     | $\vdots$     | $\ddots$ | $\vdots$       | $\vdots$          | -->
<!-- | $Y_K$    | $n_{K1}$     | $n_{K2}$     | $\cdots$ | $n_{KK'}$      | $n_{K\cdot}$      | -->
<!-- | **Sums** | $n_{\cdot1}$ | $n_{\cdot2}$ | $\cdots$ | $n_{\cdot K'}$ | $n_{\cdot\cdot}$ | -->

<!-- $ {{} \atop Y}  \!\diagdown\! ^{Y'}$	& $Y'_1$	& $Y'_2$	& $\cdots$	& $Y'_{K'}$	& Sums	\\ -->
<!-- \hline -->
<!-- $Y_1$		& $n_{11}$	& $n_{12}$	& $\cdots$	& $n_{1K'}$	& $n_{1 \cdot}$	\\ -->
<!-- $Y_2$		& $n_{21}$	& $n_{22}$	& $\cdots$	& $n_{2K'}$	& $n_{2 \cdot}$	\\ -->
<!-- $\vdots$	& $\vdots$	& $\vdots$	& $\ddots$	& $\vdots$	& $\vdots$		 \\ -->
<!-- $Y_{K}$	& $n_{K1}$	& $n_{K2}$	& $\cdots$	& $n_{KK'}$	& $n_{K \cdot}$	\\ -->
<!-- \hline -->
<!-- Sums	& $n_{\cdot 1}$	&  $n_{\cdot 2}$	& $\cdots$	& $n_{\cdot K'}$	& $n_{\cdot \cdot} = n$ -->

```{r randContingency, tidy=FALSE, echo=F}
table_data <- data.frame(
  "$Y_1'$" = c("$n_{11}$", "$n_{21}$", "$\\vdots$", "$n_{K1}$", "$n_{\\cdot 1}$"),
  "$Y_2'$" = c("$n_{12}$", "$n_{22}$", "$\\vdots$", "$n_{K2}$", "$n_{\\cdot 2}$"),
  "$\\cdots$" = c("$\\cdots$", "$\\cdots$", "$\\ddots$", "$\\cdots$",  "$\\cdots$"),
  "$Y_{K'}'$"=c("$n_{1K'}$", "$n_{2K'}$", "$\\vdots$", "$n_{KK'}$", "$n_{\\cdot K'}$"),
  "Sums"=c("$n_{1 \\cdot}$", "$n_{2 \\cdot}$", "$\\vdots$", "$n_{K \\cdot}$", "$n_{\\cdot \\cdot}$")
)

row.names(table_data) <- c("$Y_1$", "$Y_2$", "$\\vdots$", "$Y_K$", "**Sums**")
colnames(table_data) <- c("$Y_1'$", "$Y_2'$", "$\\cdots$", "$Y_{K'}'$", "Sums")

knitr::kable(
  table_data, 
  caption = "Contingency table to calculate a measure of similarity between clusterings $Y$ and $Y'$ as used by @rand1971objective.",
  booktabs = TRUE,
  escape=FALSE
)
```

One can restate equation \@ref(eq:randIndex) in terms of the notation from table \@ref(tab:randContingency) [@brennan1974measuring]:

\begin{eqnarray}
	A &=& \binom{n}{2} + \sum_{i=1}^K\sum_{j=1}^{K'}n_{ij}^2 - \frac{1}{2}\left(\sum_{i=1}^K n_{i\cdot}^2 + \sum_{j=1}^{K'}n_{\cdot j}^2  \right) \\
	&=& \binom{n}{2} + 2 \sum_{i=1}^{K}\sum_{j=1}^{K'}\binom{n_{ij}}{2} - \left[\sum_{i=1}^{K}\binom{n_{i \cdot}}{2} + \sum_{j=1}^{K'}\binom{n_{\cdot j}}{2}\right]% \binom{n}{2} ^{-1}
	(\#eq:randIndex2)
\end{eqnarray}

<!-- This function, $c$, has three properties. -->

<!--   1. $c(\cdot, \cdot) \in [0,1]$ (0 for no similarity, 1 for high similarity); -->
<!--   2. $1 - c$ is a distance measure; -->
<!--   3. if one assumes a distribution to describe $X$, then $c$ is a random variable. -->

### The Adjusted Rand Index

@hubert1985comparing extend the Rand index to account for chance. They include a null hypothesis and assume that there is a probability of some points having a $\gamma$ value of 1 by chance. It can be shown that the expected number of points with common membership in both clusters is non-zero. Specifically:

\begin{eqnarray}
	\mathbb{E}\left[\sum_{i=1}^K \sum_{j=1}^K\binom{n_{ij}}{2}\right] = \frac{\sum_{i=1}^K \binom{n_{i\cdot}}{2} \sum_{j=1}^K \binom{n_{\cdot j}}{2}}{\binom{n}{2}}
\end{eqnarray}

This is the product of the number of distinct pairs that can be formed from rows and the number of distinct pairs that can be constructed from columns, divided by the total number of pairs.

The expected number of items with the same label in both partitions can be calculated from table \@ref(tab:randContingency). Say for paired labelling such that items belong to both $\{Y_i, Y_j'\}$ is the product of number of pairs the $i^{th}$ row and the $j^{th}$ column divided by the total number of possible pairings:

\begin{eqnarray} 
	\mathbb{E}\left[\binom{n_{ij}}{2}\right] = \frac{\binom{n_{i\cdot}}{2}\binom{n_{\cdot j}}{2}}{\binom{n}{2}}
	(\#eq:expectedNij)
	\end{eqnarray}
	
One can see that as each component of equation \@ref(eq:randIndex2) is some transformation of $\sum_{i,j}\binom{n_{ij}}{2}$, one can directly state the expected value of the Rand index by combining equations \@ref(eq:randIndex2) and \@ref(eq:expectedNij):
	
\begin{eqnarray}
	\mathbb{E}\left[A \binom{n}{2}^{-1}\right] = 1 + 2 \sum_{i=1}^{K} \binom{n_{i \cdot}}{2} \sum_{j=1}^{K'} \binom{n_{\cdot j}}{2} \binom{n}{2}^{-2} - \left[\sum_{i=1}^{K} \binom{n_{i \cdot}}{2} + \sum_{j=1}^{K'} \binom{n_{\cdot j}}{2}\right] \binom{n}{2}^{-1}
\end{eqnarray}

Defining an index corrected for chance as:
\begin{eqnarray}
	\text{Corrected index} = \frac{\text{Index} - \text{Expected index}}{\text{Maximum index} - \text{Expected index}}
\end{eqnarray}

Assuming a maximum value of 1 for the Rand index then gives a corrected Rand index:
\begin{eqnarray} 
	AR(Y, Y') &= \frac{\sum_{i=1}^{K}\sum_{j=1}^{K'} \binom{n_{ij}}{2} - \sum_{i=1}^{K} \binom{n_{i \cdot}}{2} \sum_{j=1}^{K'} \binom{n_{\cdot j}}{2} \binom{n}{2}^{-1}}{\frac{1}{2} \left[\sum_{i=1}^{K} \binom{n_{i \cdot}}{2} + \sum_{j=1}^{K'} \binom{n_{\cdot j}}{2}\right] - \sum_{i=1}^{K} \binom{n_{i \cdot}}{2} \sum_{j=1}^{K'} \binom{n_{\cdot j}}{2} \binom{n}{2}^{-1}}
	(\#eq:adjustedRandIndex)
\end{eqnarray}

This quantity is defined as the _adjusted Rand index_ and I use it as my measure of choice for similarity between clusterings.

### Posterior expected adjusted Rand index

Define this quantity

One can estimate the posterior expected adjusted Rand (**PEAR**) index from the recorded MCMC samples. @fritsch2009improved suggest choosing the clustering $c^*$ that maximises the posterior expected adjusted Rand index. This is approximated from $n_{iter}$ MCMC samples by:
	\begin{align}
	\frac{1}{n_{iter}}\sum_{i=1}^{n_{iter}} AR(c^*, c^i)
		(\#eq:PEAR)
	\end{align}

Where $c^i$ is the $i^{th}$ recorded clustering. The R package ``mcclust`` [@R-mcclust] includes a calculation of this quantity based upon the PSM.

### Frobenius product

For two $m \times n$ matrices $A$ and $B$, the _Frobenius inner product_ is defined as:

\begin{eqnarray}
\langle A, B \rangle _F = \sum_{i=1}^m \sum_{j=i}^n a_{ij}b_{ij}
(\#eq:frobeniusProduct)
\end{eqnarray}

We use this to compare the true coclustering matrix to both the posterior similarity matrix (**PSM**) produced by Bayesian inference and the consensus matrix (**CM**) produced by the Consensus inference. (Note: I will use the phrase _similarity matrix_ in the contexts that apply to both the PSM and CM).

Consider an item $x_i$ that truthfully has allocation label $c_i$. Now say that our similarity matrix has $x_i$ allocated correctly (i.e. with the other items that have allocation $c_i$) with a score of 0.4, but misallocated to some $c_j \neq c_i$ with a score of 0.6. In the predicted clustering calculated from our similarity matrix we will allocate $x_i$ to the wrong cluster and this will lessen the ARI between the truth and the predicted clustering. However, the model has been uncertain about $x_i$'s allocation. The Frobenius inner product will capture this uncertainty and (in this case) reward the model with a higher score. Thus the Frobenius product more accurately describes the model performance.

In practice I use a normalised score defined by the Frobenius product. For some similarity matrix $S$ and the true coclustering matrix $T$:

\begin{eqnarray}
f(S, T) = 1 - \frac{\langle S, T \rangle _F}{\langle T, T \rangle _F}
\end{eqnarray}

This is to give a common scale across all simulation scenairos. The change from a distance to a similarity measure is to ensure that a positive increase in the score represents a positive increase in performance. 

## Bayesian model convergence

### Geweke standardised scores

Everyone's favourite $Z$ [@geweke1992evaluating].

### Gelman-Rubin shrinkage factor

Eveyone's favourite $\hat{R}$ [@gelman1992inference].