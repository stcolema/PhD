---
output:
  pdf_document: default
  html_document: default
---
# Methods {#methods}

## Consensus clustering of Bayesian mixture models

As noted in the preceding section, Bayesian mixture models have many attractive features such as:

1. allowing inference of $K$;
2. allowing integration of prior information; and
3. extending naturally to the multiple dataset scenario.

Bayesian mixture models also extend naturally to the multiple dataset setting [see @kirk2012bayesian; @gabasova2017clusternomics; and @lock2013bayesian for
examples].

However, MCMC methods, the workhorse of Bayesian inference, have pathological
behaviour in a high dimensional setting; a setting which is prevalent in modern biomedical 
data. The sampler becomes very slow to perform a single iteration,
and can become trapped within a single mode for any realistic length of run time
[@ronan2016avoiding; @chandra2020bayesian; @robert2018accelerating].
However, MCMC samplers are often adept at finding _a_ mode. A small
number of iterations (relative to the number required for convergence) is often
sufficient to find a mode. Therefore, I propose extending Consensus clustering to
Bayesian mixture models. I use many short chains each with a random
initialisation. This ensemble approach is expected to represent more modes that are present in the data than would be investigated in any single chain. The density of each mode is estimated by the number of samples generated from that part of the space, as is standard in sampling-based density estimation. 

This application of consensus clustering allows one to perform inference upon a Bayesian clustering model in contexts where Bayesian inference is difficult. However, it must be stressed that although the models may be Bayesian, the inference is not. I aim to marry some of the attractions of Bayesian mixture models with those of the ensemble.

The algorithm is described below:

| **PROCEDURE:** Consensus clustering for Bayesian mixture models
|
| **Input:** 
| a dataset $X=(x_1, \ldots, x_N)$
| a non-parametric mixture model with membership vector $c=(c_1, \ldots, c_N)$
| an MCMC algorithm _Sample_
| the number of chains to run, $S$
| the number of iterations within each chain, $R$
| 
| **Method:** 
| $\mathbf{M} \leftarrow 0_{N \times N}$ \{initialise the consensus matrix.\}
| for $s$ in $1, 2, \ldots, S$ do
|   $set.seed(s)$ \{set the random seed that controls initialisation.\}
|   $Y_{(0, s)} \leftarrow$ _Initialise_$(X)$ \{initial random partition.\}
|   for $r$ in $1, 2, \ldots, R$ do
|      $Y_{(r, s)} \leftarrow$ _Sample_$(c)$ \{generate a Markov chain for the membership vector.\}
|   end
|   $B^{(s)} \leftarrow Y_{(R, s)}$ \{create a coclustering matrix using the $R^{th}$ sample.\}
|   $\mathbf{M} \leftarrow \mathbf{M} + B^{(s)}$ 
| end
| 
| $\mathbf{M} \leftarrow \frac{1}{S} \mathbf{M}$ \{normalise the consensus matrix.\}
| $\hat{Y} \leftarrow$ partition $X$ based upon $\mathbb{M}$
|
| **Output:** 
| consensus matrix $\mathbf{M}$
| partition $\hat{Y}$

In contrast to previosuly implemented versions of consensus clustering, one does not have to set $K$, as this is inferred within the non-parametric mixture model.

<!-- Through use of random initialisation one creates diversity in the coclustering  -->
<!-- matrices produced, avoiding the need for a sampling method. -->




<!-- ### Consensus inference -->

<!-- Consensus inference is an ensemble method such as _random forest_  -->
<!-- [@breiman2001random], and aggregates many (presumably) weak learners to improve  -->
<!-- ability to explore multiple modes and improve the model's ability to uncover  -->
<!-- structure. It follows a similar logic to  -->
<!-- _consensus clustering_ [@monti2003consensus], allowing many models with  -->
<!-- different random initialisations to contribute equally weighted  -->
<!-- votes on the clustering. In contrast to consensus clustering, consensus  -->
<!-- inference does not require model convergence. Furthermore, consensus clustering -->
<!-- is intended as a method to choose the number of clusters present based upon the -->
<!-- stability of clusters for different choice of $K$, whereas consensus inference  -->
<!-- infers $K$, and thus this uncertainty about $K$ is represented in the final  -->
<!-- consensus matrix. More precisely I define consensus inference as follows for -->
<!-- this report: -->

<!-- ```{definition, consensusInference, name="Consensus inference"} -->
<!-- A general method for performing inference upon clustering models using MCMC  -->
<!-- methods. Consensus inference runs $S$ independent chains of a collapsed Gibbs -->
<!-- sampler run for $R$ iterations saving the sample from the $R^{th}$ iteration.  -->
<!-- That is, from the $s^{th}$ chain a single value for each variable is recorded, -->
<!-- $\theta^{(s)}=(\theta_1^{(s)}, \ldots, \theta_Q^{(s)})$. -->

<!-- The samples are then compiled into a $S \times Q$ matrix (where the $(s, q)^{th}$  -->
<!-- entry is the sampled value for the $q^{th}$ parameter in the $s^{th}$ chain). -->

<!-- Inference is performed similarly to the case of a Bayesian inference using  -->
<!-- sampling methods, but the interpretation of results differs in that the  -->
<!-- uncertainty measured is not necessarily the posterior variance and we do not -->
<!-- assume that we are sampling from the posterior distribution. -->
<!-- ``` -->


<!-- As consensus inference uses MCMC methods, it may leverage some of the strengths  -->
<!-- of Bayesian inference (e.g. use of a prior, inference of $K$ through use of a  -->
<!-- _Dirichlet-Multinomial Allocation_ (**DMA**) mixture model [@green2001modelling; -->
<!-- see @savage2013identifying for an example], suitability for an  -->
<!-- integrative setting, and some quantification of uncertainty), but does not offer -->
<!-- the asymptotic guarantees of Bayesian inference.  -->

<!-- As each chain is independent of all others, this means that consensus inference  -->
<!-- can use a parallel environment to reduce computational time and improve  -->
<!-- scalability.  -->

<!-- Roughly summarising: -->

<!-- ```{r, my_table, echo = F} -->

<!-- col_names <- c("MLE", "Bayesian", "Consensus") -->
<!-- row_names <- c("Singularities in the likelihood",  -->
<!--   "Can infer $K$", -->
<!--   "Can use prior knowledge", -->
<!--   "Quantifies uncertainty", -->
<!--   "Robust to local maxima", -->
<!--   "Speed" -->
<!--   ) -->

<!-- scores <- c( -->
<!--   T, F, F, -->
<!--   F, T, T, -->
<!--   F, T, T, -->
<!--   F, T, T, -->
<!--   F, F, T, -->
<!--   "Fast", "Slow", "Fast" -->
<!-- ) %>%  -->
<!--   matrix(byrow = T, ncol = 3) %>% -->
<!--   data.frame() %>%  -->
<!--   set_rownames(row_names) %>%  -->
<!--   set_colnames(col_names) -->

<!-- knitr::kable(scores, row.names = T, escape = F) -->


<!-- ``` -->

<!-- ## Ensemble methods -->

<!-- I use the same definition of an _ensemble_ as @re2012ensemble.  --> 

<!-- ```{definition, ensemble, name = "Ensemble"} -->
<!-- A set of learning machines that work together to find some structure within data, -->
<!-- whether this be a supervised problem such as _classification_ or an unsupervised -->
<!-- problem such as cluster analysis. -->

<!-- ``` -->


<!-- <!-- There exists many other terms referring to the same basic concept, e.g. --> 
<!-- <!-- a fusion, combination, aggregation, committee of learners, but I will use the  -->
<!-- <!-- term ensemble throughout this report. --> 

<!-- The concept of an ensemble of learners and its improvements is an old one, and -->
<!-- not limited to machincal learners [@condorcet1785essay]. Much of the theoretical -->
<!-- underpinings of ensemble learning has focused upon resampling-based methods -->
<!-- and classification [@friedman2007bagging; @breiman1996bias; @schapire1998boosting], -->
<!-- in keeping with the  majority of algorithms, of which _Random forest_ -->
<!-- [@breiman2001random] is probably the most famous. There have been efforts to -->
<!-- provide a more general mathematical basis for ensembles; Eugene Kleinberg -->
<!-- deserves special mention for achievements based upon set theory and -->
<!-- combinatorics [@kleinberg1977infinitary; @kleinberg1990stochastic; -->
<!-- @kleinberg1996overtraining; @kleinberg2000mathematically]. More pertinently for -->
<!-- the practitioner, perhaps, these methods have also displayed great empricial -->
<!-- success, outperforming state-of-the-art methods in both simulations and -->
<!-- benchmark datasets [@breiman2001random; @monti2003consensus; -->
<!-- @sohn2007experimental; @afolabi2018ensemble]. Attractions of ensemble methods -->
<!-- include their ability to explore multiple modes [@ghaemi2011review]. Most -->
<!-- ensemble methods also enable use of a parallel enivronment to improve -->
<!-- computation speed [@ghaemi2009survey]. -->

<!-- For a more thorough review of ensemble methods with a focus on classification, -->
<!-- please see @re2012ensemble. -->

<!-- @monti2003consensus proposed a general frameworks for ensembles of clusterers, -->
<!-- "Consensus clustering". It is an model-independent, sampling-based method, that -->
<!-- attempts to provide a rule-based approach to choosing $K$ in the model and to -->
<!-- improve cluster stability. The individual-level algorithm investigated in the -->
<!-- original publication and implemented in the ``ConsensusClusterPlus`` R package -->
<!-- [@wilkerson2010consensusclusterplus] is $k$-means clustering. This flavour of -->
<!-- Consensus clustering has been successfully used in cancer subtyping -->
<!-- [@verhaak2010integrated; @marisa2013gene]. -->

<!-- I propose extending Consensus clustering to Bayesian models. As listed -->
<!-- previously, Bayesian mixture models have many attractive features such as: -->

<!-- 1. allowing inference of $K$; -->
<!-- 2. allowing integration of prior information; and -->
<!-- 3. extending naturally to the multiple dataset scenario. -->

<!-- However, MCMC methods, the workhorse of Bayesian inference, have pathological -->
<!-- behaviour in the context of modern biomedical data which is often of sufficient -->
<!-- dimensionality that the sampler is both very slow to perform a single iteration, -->
<!-- and can become trapped within a single mode for any realistic length of run time -->
<!-- [@ronan2016avoiding; @chandra2020bayesian; @robert2018accelerating]. -->
<!-- However, MCMC samplers are often adept at finding _a_ mode, therefore a small -->
<!-- number of iterations (relative to the number required for convergence) is often -->
<!-- sufficient to find a mode. Using many short chains each with a random -->
<!-- initialisation, one might believe that one will explore more modes that are -->
<!-- present in the data than would be investigated in any single chain. One may use -->
<!-- this approach to perform inference upon a Bayesian clustering model in contexts -->
<!-- where this is normally difficult or even impossible, although the inference -->
<!-- itself is not Bayesian. Thus one may marry the attractions of Bayesian _models_ -->
<!-- with those of the ensemble. -->

<!-- can be slow to  -->
<!-- explore the target distribution. In practice, particularly for complex,  -->
<!-- high-dimensional data, ensuring that the sampler has -->

<!-- 1. reached its stationary distribution; -->
<!-- 2. explored the full support of the posterior space; and -->
<!-- 3. converged to the expectation -->

<!-- is very difficult [@robert2018accelerating]. -->

<!-- Bayesian clustering models are therefore attractive, but  -->


Note that this does sacrifice certain qualities about MCMC methods that are attractive

* there are no aymptotic guarantees;
* the uncertainty described across the partitions is not the same as the
uncertainty described by Bayesian inference; and
* one cannot claim that the distribution of values generated for each variable
coincides with the posterior distribution.

In cluster analysis generally, but even more so in the context of biomedical
data and human health, any result has to be validated based upon knowledge of
the domain. This means that aymptotic guarantees are not sufficient to validate
a final partition; the defence and interpretation based upon the data is more
important. Thus, I argue that if consensus clustering of Bayesian mixture models
produces sensible, interpretable results, than the loss of the asymptotic
guarantees is insufficient reason to prevent use of this approach, particularly
as the asymptotic behaviour does not often emerge in practice.

There are three outstanding issues for any ensemble of clusterers
[@topchy2003combining] that need to be considered.

1. Consensus: How to combine different clusterings?
2. Diversity: How to generate different partitions?
3. Strength: How "weak" can each input partition be?

One may realise that many of the issues here are also present for sampling-based
inference of Bayesian mixture models. In this case, one could consider the MCMC
sampler as a special case of an clustering ensemble, being a set of many
partitions. In this case the problems of diversity and strength are mitigated
somewhat by mathematical properties of the Markov chain, but as the problems for
MCMC listed in section \ref(bayesian-inference) makes clear, these questions are
not answered as satisfactorily as one might hope.

For the problem of finding a consensus between partitions, as the labels
generated for each partition are symbolic, combining clusterings
can be a difficult problem [@strehl2002cluster]. Aggregating partitions into a 
Consensus matrix [as recommended by @monti2003consensus] offers a representation
of the partitions that overcomes this label-swapping problem. A point clustering
may then be estimated from these matrices using one of the methods listed by
@fritsch2009improved.

The source of diversity in my approach is the use random initialisation of the
sampler, in contrast to the original consensus clustering algorithm where 
diversity is introduced via peturbation of the dataset using some sampling
method. In my simulations I show that a single MCMC chain can struggle to 
produce a diverse range of partitions and that using many different 
initialisations helps to overcome this problem. I also display cases where the
diversity of partitions is very limited, but this is due to each chain finding 
the global maximum and thus a lack of diversity in the final samples is ideal.

I will investigate the strength of each individual in my simulation study,
showing emprical results for both the consensus and individual learners.
There already exist results in the literature that the ratio of the diversity
of partitions to the strength is more important than the strength alone,
with the caveat that each individual performs better than random
[@breiman2001random]. My prior expectation for a consensus of
Bayesian mixture models would be that within a single iteration
each Markov chain would be producing samples that are better than random as the
sampler proposes a more probable state.

## Simulation study

I test the quality of the inference offered by consensus clustering in a simulation study. This study has two goals:

1. absolute evaluation of consensus clustering as a inferential technique for mixture models; and 
2. comparative evaluation of consensus and Bayesian inference of mixture models and Mclust, a popular MLE-based mixture model R package.

To achieve these goals I am interested in evaluating:

* Predictive performance - how well each method predicts the true clustering (metric: *Adjusted Rand Index* [@hubert1985comparing] between the predicted clustering from each method and the true clustering);
* Uncertainty quantification - how well each method quantifies uncertainty about the true clustering (metric: *Frobenius norm* of the difference of the coclustering / consensus / posterior similarity matrices and the true coclustering matrix); and
* Speed - how long each method takes to run (metric: *seconds*).

### Data generating mechanism

I use the notation:

* $N$: the number of items generated;
* $P$: the number of features in the dataset;
* $P_n$: the number of irrelevant features used;
* $K$: the true number of clusters present;
* $X = (x_1, \ldots, x_N)$: the items generated;
* $\pi=(\pi_1, \ldots, \pi_K)$: the expected proportions of the population belonging to each cluster;
* $c=(c_1, \ldots, c_N)$: the allocation variable for each item;
* $\theta=(\theta_1, \ldots, \theta_K)$: the parameters associated with each component; and
* $\phi=(\phi_1,\ldots, \phi_P)$: the indicator variable of feature relevance.

The data generating model is a finite mixture model with independent features. Within this model there exist "irrelevant features" that have global parameters rather than component specific parameters:

\[
\begin{aligned}
p(x, c, \theta, \pi) &= \prod_{i=1}^N p(x_i | c_i, \theta_{c_i}) \prod_{i=1}^N p (c_i | \pi) p(\pi) p(\theta) \\
  &= \prod_{i=1}^N \prod_{p=1}^P p(x_{ip} | c_i, \theta_{c_ip})^{(1 - \phi_p)} p(x_{ip} | \theta_p) ^ {\phi_p} \prod_{i=1}^N p (c_i | \pi) p(\pi) p(\theta)
\end{aligned}
\]

In the simultatiuon study described here, the model is a mixture of *Gaussian* distribution and thus $\theta_{kp}=(\mu_{kp}, \sigma^2_{kp})$.

Within my generated datasets, there is a vector of means:

\begin{align}
\mu = (\mu_1, \ldots, \mu_K)
\end{align}

The indices here do not necessarily associate with the corresponding cluster (i.e. $\mu_1$ does not necessarily define the cluster labelled 1). Instead the mean vector is randomly permuted for each feature before generating the clusters; thus $\nu$, the vector of means in the $p^{th}$ feature, has the same values as $\mu$ but reordered. It is the $k^{th}$ value of this permuted vector that defines the values generated for the $k^{th}$ component in the $p^{th}$ feature, not the $k^{th}$ value from $\mu$ (although $\mu$ is one of the possible permutations of itself). 

As the mean of a cluster in a feature is sampled from $\mu$, this means that additional features imparts more information about the clustering structure. There is $K!$ possible unique permutations of $\mu$. This means that as the number of relevant features grows the clarity of the structure improves.

In my simulations there is a constant distance $\Delta_{\mu}$ between each consecutive entry of $\mu$, i.e.

\begin{align}
\mu = (\mu_1, \mu_1 + \Delta \mu, \ldots, \mu_1 + ( K - 1 ) \times \Delta \mu).
\end{align}

Thus $\Delta \mu$ may be considered a defining parameter for the scenario rather than $\mu$ itself.

| **PROCEDURE:** Data Generation
|
| **Input:** 
| a distance between means $\Delta_{\mu}$
| a common standard deviation $\sigma^2$
| number of clusters $K$
| number of items to generate in total $N$
| number of features to generate in total $P$
| number of irrelevant features to generate $P_n$
| expected proportion of items in each cluster $\pi=(\pi_1, \ldots, \pi_K)$
| a method for sampling $x$ times from the array $y$, with weights $\pi$: _Sample_$(y, x)$
| a method for permuting a vector $x$: _Permute_$(x)$
| a method for generating a value from a univariate Gaussian distribution with mean $\mu$ and standard deviation $\sigma^2$: _Gaussian_$(\mu, \sigma^2)$
| 
| **Method:** 
| $X \leftarrow 0_{N \times P}$ \{initialise the data matrix.\}
| $\mu \leftarrow (\Delta_{\mu}, \ldots, K\Delta_{\mu})$ \{create a matrix of $K$ means.\}
<!-- | $\mu \leftarrow$ _Mean centre_$(\mu)$ \{centre the mean vector about 0.\} -->
| $c \leftarrow$ _Sample_$(1:K, N, \pi)$ \{generate the allocation vector.\}
<!-- | for $k$ in 1 to $K$ do -->
<!-- |   $N_k \leftarrow \sum_{i=1}^N \mathbb{I}(c_i = k)$ \{find the number of items assigned to current cluster.\} -->
<!-- | end $k$ -->
| for $p$ in 1 to $P$ do
|   $\nu \leftarrow$ _Permute_$(\mu)$ \{permute the means associated with each cluster within the current feature to create independet features.\}
|   for $n$ in 1 to $N$ do
|     $X(n, p) \leftarrow$ _Gaussian_$(\nu_{c_i}, \sigma^2)$ \{generate data defined by the original label.}
|   end $n$
| end $p$
|
| **Output:** 
| data $X$
| allocation vector $c$

<!-- As each method of inference uses a common model, this data-generating mechanism is not expected to favour any one method over another. -->

### Study design

I test seven different scenarios that change various parameters in this model. The scenarios tested and there defining parameters are shown in table \ref{scenario_table}. In this case I have separated the number of relevant signals ($P_s$), such that $P=P_s + P_n$.s

<!-- 1. The 2D case; -->
<!-- 2. The lack-of-structure case in 2 dimensions; -->
<!-- 3. The base case for which scenarios 4s-6 are direct variations; -->
<!-- 4. Increasing $\sigma^2$; -->
<!-- 5. Increasing the number of irrelevant features; -->
<!-- 6. Varying the expected proportion of the total population assigned to each sub-population; -->
<!-- 7. The large $N$, small $P$ setting; and -->
<!-- 8. The small $N$, large $P$ case. -->

<!-- More details of each scenario are given in table \ref{scenario_table}. -->

<!-- A more detailed description of each scenario and various sub-scenarios is given in the below table. -->

```{r scenario_table, echo=F}
set.seed(1)
scn_table <- data.frame(
  Scenario = c("Simple 2D", "No structure", "Base Case", rep("Large N, small P", 3), rep("Large standard deviation", 3), rep("Irrelevant features", 5), rep("Small N, large P", 2), "Varying proportions"),
  N = c(100, 100, 2e2, 1e4, 1e4, 1e4, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 50, 50, 200),
  P_s = c(2, 0, 20, 4, 4, 4, 20, 20, 20, 20, 20, 20, 20, 20, 500, 500, 20),
  P_n = c(0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 10, 20, 100, 200, 0, 0, 0),
  K = c(5, 1, 5, 5, 50, 50, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5),
  Delta_mu = c(3, 0, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.2, 1),
  sigma2 = c(1, 1, 1, 1, 1, 1, 3, 5, 10, 1, 1, 1, 1, 1, 1, 1, 1),
  Pi = c(rep("vec(1/K)", 16), "(0.5, 0.25, 0.125, 0.0675, 0.0675)")
)

colnames(scn_table) <- c(
  "Scenario",
  "$N$",
  "$P_s$",
  "$P_n$",
  "$K$",
  "$\\Delta_{\\mu}$",
  "$\\sigma^2$",
  "$\\pi$"
)

sims_used <- scn_table[c(1:3, 7, 8, 11:13, 17, 15:16), ] %>%
  set_rownames(1:nrow(.))

knitr::kable(sims_used, row.names = T, escape = F) # , "latex", longtable = T, booktabs = T, caption = "Longtable")
```

<!-- \begin{center} -->
<!-- \captionof{table}{Multirow Table.} -->
<!-- \begin{tabular}{l|l|r} -->
<!-- \textbf{Value 1} & \textbf{Value 2} & \textbf{Value 3} \\ -->
<!-- $\alpha$ & $\beta$ & $\gamma$ \\ \hline -->
<!-- \multirow{2}{*}{12} & 1110.1 & a \\ -->
<!--  & 10.1 & b \\ \hline -->
<!-- 3 & 23.113231 & c \\ -->
<!-- 4 & 25.113231 & d -->
<!-- \end{tabular} -->
<!-- \end{center} -->

Each scenario is seen as testing certain concepts or else specific characteristics of real data.

#### 2D
This case is seen as an introduction to method performance. The small dimensionality and coherent structure means that I expect the collapsed Gibbs sampler to explore the target distribution quite thoroughly. This is not intended to be a realistic dataset; it is intended as a setting where both ``Mclust`` and the collapsed Gibbs sampler will perform as expected, displaying no patholgical behaviour.

<!-- providing an insight into how the sampling compares in consensus clustering and Bayesian inference.  -->

<!-- This scenario thus provides reassurance that the comparisons between the methods in more extreme scenarios are not representative of some underlying issue in the methods rather than  -->

<!-- This means that we can compare consensus clustering to Bayesian inference with a strong belief that the Markov chain has converged (given appropriate tests). This means that we can process how well consensus clustering performs with a deeper understanding of the context than might be possible in higher dimensional cases. -->

```{r, simple2dView, echo = F, cache = T}
set.seed(1)

scn <- "Simple 2D"
row_ind <- which(sims_used$Scenario == scn)
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}

# Generate dataset
simple2D_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(simple2D_gen$data), simple2D_gen$cluster_IDs,
  main = "2D data (seed 1)",
  show_rownames = F,
  show_colnames = F
)

# simple2D_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(simple2D_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_2, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: Simple 2D case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2",
#     subtitle = "Note that multiple clusters may overlap."
#   )
```

#### No structure

In this case all items are drawn from the same normal distribution. It is meant to be a test to see how the inference methods perform when there is no structure to find. It may be considered as analagous to attempting to quantify the false positive rate.

```{r, noStructureView, echo = F, cache = T}
scn <- "No structure"
row_ind <- which(sims_used$Scenario == scn)
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}

# Generate dataset
noStructure_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


no_structure_ph <- annotatedHeatmap(scale(noStructure_gen$data), noStructure_gen$cluster_IDs,
  main = "No structure (seed 1)",
  show_rownames = F,
  show_colnames = F
)

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Noise_1, y = Noise_2, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

#### Base case

This is the _base case_ which most of the following scenarios are some variation of. This is intended to be a benchmark against which performance may be judged. Due to the clarity of the structure, the lack of irrelevant features and the relatively low dimensionality each method is expected to perform well here across all metrics, predicting the true clustering correctly and with very little uncertainty.

```{r, baseCase, echo = F, cache = T}
scn <- "Base Case"
row_ind <- which(sims_used$Scenario == scn)[1]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
baseCase_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


base_case_ph <- annotatedHeatmap(scale(baseCase_gen$data), baseCase_gen$cluster_IDs,
  main = paste0(scn, " (seed 1)"),
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
# cluster_labels <- c(paste0("Cluster ", 1:K))
# names(cluster_labels) <- 1:K


# pc1 <- prcomp(baseCase_gen$data)$x
# pcaSeriesPlot(x = pc1, labels = baseCase_gen$cluster_IDs, n_comp = min(P_s, 6)) +
#   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
#   labs(
#     title = paste0("PCA series for ", scn, " scenario") #,
#     # subtitle = "Notice how the clusters separate across the components."
#   )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

#### Large standard deviation

This is to test how distinct the clusters must be for the inference to perform well. In many datasets there is no reason to believe that the clusters should be strongly separable; this setting is intended to better understand how sensitive the different approaches are to overlap of subpopulations.

<!-- _A priori_, I would think that Bayesian inference would perform optimally here. The reasonable number of features (20) means that the collapsed Gibbs sampler should explore the space well. The large variance within each cluster means that much of the signal is hidden and the data might appear more as a continuum with no clustering structure; but I hope that the Bayesian inference will correctly idenitfy some structure and capture a large amount of uncertainty. I think consensus clustering will perform quite well and do not have a strong intuition for how ``Mclust`` will perform. I think that the clever initialisation based upon ``hclust`` will help in unpicking much structure, but I suspect that the final clustering will be too certain. -->

```{r, largeStandardDeviation, echo = F, cache = T}
scn <- "Large standard deviation"
row_ind <- which(sims_used$Scenario == scn)[1]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
largeStadDev_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


large_std_dev_ph <- annotatedHeatmap(scale(largeStadDev_gen$data), largeStadDev_gen$cluster_IDs,
  main = "Large standard deviation (seed 1)",
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
# cluster_labels <- c(paste0("Cluster ", 1:K))
# names(cluster_labels) <- 1:K
# 
# 
# pc1 <- prcomp(largeStadDev_gen$data)$x
# suppressWarnings(
#   pcaSeriesPlot(x = pc1, labels = largeStadDev_gen$cluster_IDs, n_comp = min(P_s, 6)) +
#   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
#   labs(
#     title = "PCA series for large N, small P scenario",
#     subtitle = "Notice how the clusters separate across the components."
#   )
# )
# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

<!-- To highlight how strongly the clusters overlap within each feature I include a  -->
<!-- an example of the within-feature distributions for each case. -->

<!-- ```{r, example_large_std_dev, echo=F} -->


<!-- p1 <- lapply(1:5, function(x){rnorm(5000, x, 5)}) %>%  -->
<!--   data.frame() %>% -->
<!--   set_colnames(1:5) %>% -->
<!--   pivot_longer(everything(), names_to = "Cluster", values_to = "Value") %>%  -->
<!--   ggplot(aes(x = Value, colour = Cluster)) + -->
<!--   geom_density() + -->
<!--   scale_color_viridis_d() + -->
<!--   labs( -->
<!--     title = "Standard deviation 5" -->
<!--   ) + -->
<!--   xlim(-15, 25) -->


<!-- p2 <- lapply(1:5, function(x){rnorm(5000, x, 3)}) %>%  -->
<!--   data.frame() %>% -->
<!--   set_colnames(1:5) %>% -->
<!--   pivot_longer(everything(), names_to = "Cluster", values_to = "Value") %>%  -->
<!--   ggplot(aes(x = Value, colour = Cluster)) + -->
<!--   geom_density() + -->
<!--   scale_color_viridis_d() + -->
<!--   labs( -->
<!--     title = "Standard deviation 3" -->
<!--   ) + -->
<!--   xlim(-15, 25) -->

<!-- (p2 / p1) + plot_annotation(title = "Example of clusters within a feature for large standard deviation data") -->


<!-- ``` -->

<!-- One can see that the clusters overlap severaly within each feature and that unpicking any structure might be highly difficult, partiuclarly in the second case. -->

<!-- ### Large $N$, small $P$ -->

<!-- This is to test the _large $N$, small $P$_ scenario reminiscent of CytoSeq data. -->

<!-- This is a difficult case to -->
<!-- analyse as visualising the result is difficult for a PSM (due to the $N \times N$  -->
<!-- dimension of this), and obtaining a predicted clustering is also very slow. -->

<!-- ```{r, largeNsmallP, echo = F, cache = T} -->
<!-- scn <- "Large N, small P" -->
<!-- row_ind <- which(sims_used$Scenario == scn)[1] -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->


<!-- # Generate dataset -->
<!-- largeNsmallP_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(largeNsmallP_gen$data), largeNsmallP_gen$cluster_IDs, -->
<!--   main = "Large N, small P (seed 1)", -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->


<!-- # Set labels for facet wrapping -->
<!-- cluster_labels <- c(paste0("Cluster ", 1:K)) -->
<!-- names(cluster_labels) <- 1:K -->


<!-- pc1 <- prcomp(largeNsmallP_gen$data)$x -->
<!-- pcaSeriesPlot(x = pc1, labels = largeNsmallP_gen$cluster_IDs, n_comp = min(P_s, 6)) + -->
<!--   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) + -->
<!--   labs( -->
<!--     title = "PCA series for large N, small P scenario", -->
<!--     subtitle = "Notice how the clusters separate across the components." -->
<!--   ) -->

<!-- # noStructure_gen$data %>% -->
<!-- #   scale() %>% -->
<!-- #   data.frame() %>% -->
<!-- #   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>% -->
<!-- #   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) + -->
<!-- #   geom_point() + -->
<!-- #   scale_colour_viridis_d() + -->
<!-- #   labs( -->
<!-- #     title = "Example: No structure case (seed 1)", -->
<!-- #     x = "Gene 1", -->
<!-- #     y = "Gene 2" -->
<!-- #   ) -->
<!-- ``` -->


#### Small $N$, large $P$

This is to test the _small $N$, large $P$_ scenario so common in genetics. An 
example of this data would be in clustering patients based upon their gene 
expression data. Such datasets arise consistently in precision medicine.  
Many of the the Cancer Genome Atlas (**TCGA**) datasets have this characteristic.

<!-- I  -->
<!-- think all methods will succeed in unpicking structure in the first case, but as -->
<!-- the distance between means decreases it will be interesting to see how each  -->
<!-- method performs. If the distance becomes too small and the resulting mixture of  -->
<!-- Gaussians effectively merge such that unpicking anyone cluster from its  -->
<!-- neighbours is not feasible, than this will be similar to a high-dimensional  -->
<!-- extension to the _no structure_ case. -->

```{r, smallNlargeP, echo = F, cache = T}
scn <- "Small N, large P"
row_ind <- which(sims_used$Scenario == scn)[2]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
smallNlargeP_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


small_n_large_p_ph <- annotatedHeatmap(scale(smallNlargeP_gen$data), smallNlargeP_gen$cluster_IDs,
  main = "Small N, large P (seed 1)",
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
# cluster_labels <- c(paste0("Cluster ", 1:K))
# names(cluster_labels) <- 1:K


# pc1 <- prcomp(smallNlargeP_gen$data)$x
# pcaSeriesPlot(x = pc1, labels = smallNlargeP_gen$cluster_IDs, n_comp = min(P_s, 6)) +
#   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
#   labs(
#     title = "PCA series for small N, large P scenario",
#     subtitle = "The clusters appear to be separable, despite the wide range of loadings within clusters."
#   )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

#### Irrelevant features

This is to test how robust to noise the methods are. In many settings, such as 
gene expression data, many noisy features that contribute little 
or no information about underlying structure are expected to be present. 
Investigating how robust each method is to irrelevant features is important; if
methods are highly sensitive to noise, feature selection becomes a far more 
important part of the analysis. Feature selection is a highly difficult problem
and presents an opportunity to remove valuable information if done incorrectly. 
Thus, if the methods are both able to uncover the true structure despite noise
and unlikely to uncover false structure then one can reduce the degree of feature
selection required.


<!-- I expect that all methods  -->
<!-- will perform wll when the number of feautres containing signal is greater than  -->
<!-- those containing noise, but as the ratio flips and the number of irrelevant  -->
<!-- features dominates, I expect ``Mclust`` to struggle. I will be interested to -->
<!-- see how Bayesian inference and consensus clustering perform, hoping that the Markov chain  -->
<!-- does produce sensible samples; it might be deeper into the chain before the  -->
<!-- correct space of partitions is being explored. This is of interest in the  -->
<!-- context of gene expression data where many irrelevant features are present;  -->
<!-- ideally our method will be robust to this, as it would mean that a practitioner -->
<!-- could use a less intense feature selection as removing all noisy features  -->
<!-- would not be a requisite to avoid obscuring subpopulation signal. -->

```{r, irrelevantFeatures, echo = F, cache = T}
scn <- "Irrelevant features"
row_ind <- which(sims_used$Scenario == scn)[3]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
irrelevantFeatures_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


irr_features_ph <- annotatedHeatmap(scale(irrelevantFeatures_gen$data), irrelevantFeatures_gen$cluster_IDs,
  main = paste0("Irrelevant features (seed 1, ", P_n, " irrelevant features)"),
  show_rownames = F,
  show_colnames = F
)

# 
# # Set labels for facet wrapping
# cluster_labels <- c(paste0("Cluster ", 1:K))
# names(cluster_labels) <- 1:K
# 
# 
# pc1 <- prcomp(irrelevantFeatures_gen$data)$x
# pcaSeriesPlot(x = pc1, labels = irrelevantFeatures_gen$cluster_IDs, n_comp = min(P_s, 6)) +
#   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
#   labs(
#     title = "PCA series for irrelevant features scenario",
#     subtitle = "In this example one might expect clusters 2 and 3 to merge, with cluster 5 also\nin danger of being subsumed."
#   )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

#### Varying proportions

This simulation is designed to test how sensitive the methods are to relatively 
small clusters when large clusters are present. For example, in precision 
medicine some of the clusters of patients present might
be very small, possibly even singletons; uncovering such structure in an 
analysis could be very important for clinical outcomes. This scenario is 
intended to offer insight into how the different inferential appraoches perform
in such a setting.

```{r, varyingProportionsView, echo = F, cache = T}
scn <- "Varying proportions"
row_ind <- which(sims_used$Scenario == scn)[1]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
varyingProportions_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


varying_prop_ph <- annotatedHeatmap(scale(varyingProportions_gen$data), varyingProportions_gen$cluster_IDs,
  main = paste0(scn, " (seed 1)"),
  show_rownames = F,
  show_colnames = F
)


# (wrap_elements(panel = simple2dPh$gtable, clip = FALSE) +
#   wrap_elements(panel = noStructurePh$gtable, clip = FALSE)) /
#   wrap_elements(panel = simple2dPh$gtable, clip = FALSE) 
  


# # Set labels for facet wrapping
# cluster_labels <- c(paste0("Cluster ", 1:K))
# names(cluster_labels) <- 1:K
# 
# 
# pc1 <- prcomp(varyingProportions_gen$data)$x
# pcaSeriesPlot(x = pc1, labels = varyingProportions_gen$cluster_IDs, n_comp = min(P_s, 6)) +
#   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
#   labs(
#     title = paste0("PCA series for ", scn, " scenario") #,
#     # subtitle = "Notice how the clusters separate across the components."
#   )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

### Data-generation

For each of the scenarios described above, I generate 100 membership vectors. Using the specific parameters associated with each scenario (the details of which are listed in a table in appendix A), 100 datasetsare then generated, defined by the membership vectors. These membership vectors will be the true clustering used in model comparison. Each pairing of a dataset and membership vector, $(X^{(i)}, c^{(i)})$, is referred to as a _simulation_ within a scenario.

### Inference

I use the C++ implementation of Bayesian mixture models provided by @mason2016mdi. This performs inference upon a mixture model using a Collapsed Gibbs sampler and assumes indepedent features. This is the software for which samples are generated for both consensus clustering and Bayesian inference.

For MLE, I use the R package ``Mclust`` [@R-mclust]. This is a popular implementation that uses an initialisation based upon hierarchical clustering (and is therefore a non-random initialisation, unlike the Bayesian inference and consensus clustering).

For each dataset the following inference are performed:

1. Consensus clustering using a range of different choices of $R$ and $S$. The sets used are:

    * $R = \{1, 2, \ldots, 10, 20, \ldots, 100, 200, \ldots, 1000, 2000, \ldots, 10000\}$; and
    * $S = \{1, 2, \ldots, 100\}$.
    
  I will showcase a subset of these combinations, with _Consensus ($r$, $s$)_ denoting the inference performed using the $r^{th}$ sample from $s$ different chains for all $s \in S, r \in R$.
2. Bayesian inference using 10 chains running for 1 million iterations, saving every thousandth sample.
3. Mclust over a range of possible $K$.

Each method will be timed using the ``time`` bash function.

### Model evaluation


In this study we will be considering both within-simulation and across-simulation performance for the $N_{sim}$ simulations run under each scenario.

The aim of each model is uncovering the true clustering of the data, $C'$. The performance of each model may be judged by compaing the predicted clustering, $C^*$, to $C'$ using the adjusted rand index [**${ARI(C^*, C')}$**, @hubert1985comparing]. For ``Mclust``, only the point estimate of the clustering is generated, and thus is trivial to compare. For Bayesian inference and consensus clustering we have a number of samples. These are used to construct a PSM/Consensus matrix from which a point estimate of the clustering is predicted using the ``maxpear`` function in the ``mcclust`` R package [@R-mcclust; theory described by @fritsch2009improved]. 

How well the inference quantifies uncertainty about the predicted clustering is also of interest. A model may have a low score under the ARI for its predicted clustering, but may have explored the partition space that included the true clustering. Ideally a model that explores sensible partitions is rewarded, even if the final partition is not ideal. The Frobenius norm of the difference of the coclustering matrix of the true clustering with the 

1. posterior similarity matrix for the Bayesian inference;
2. the coclustering matrix for Mclust; and
3. the consensus matrix for the consensus clustering

is used to attempt to quantify performance here. Note that as Mclust only proposes a single partition its perfomance under this metric is a linear function of the ARI whereas this is not necessarily the case for Bayesian inference and consensus clustering.

The Frobenius norm is a flawed proxy for uncertainty quantification. As a standalone statistic it is not sufficient as if no model is performing well the best performer under this metric will probably be that which is most sparse. It is important, particularly in the cases where the ARI is very low, to inspect the heatmaps of the PSMs and consensus matrices to understand what is happening. The Frobenius norm is used as it does contain _some_ information about how well uncertainty is quantified and describes this within a single number. This means that it translates well to comparison across many simulations within a single scenario. Furthermore, if there is a trend within the distribution of the Frobenius norms for each model than inspection of the PSMs and consensus matrices for a single simulation might be sufficient to understand the behaviour within the entire scenario in combination with the Frobenius norm.

For the Bayesian models I am also interested in within-chain and across-chain convergence. I calculate the Geweke statistic [@geweke1992evaluating] and the Gelman-Rubin shrinkage parameter [$\hat{R}$, @gelman1992inference] as measures of these respectively.
If the Geweke statistic of a chain is not appreaing to follow a normal-distribution, as tested by a Shapiro-Wilks test, that chain is removed from the analysis. Chains are tested for the failure to converge using the extension of $\hat{R}$ proposed by @vats2018revisiting, using a threshold of 1.01. This is a summary statistic for entire set of chains; the original $\hat{R}$ value described by @gelman1992inference is plotted as a function of the chain iteration as a visual aid in assessing convergence. Note that a low value in these statistics is not sufficient to definitively claim convergence, but a high value **does** indicate a lack of convergence.

The samples from the chains that pass the Shapiro-Wilks test are then pooled for the comparison beyond methods. I do not drop the entire analysis based upon overall convergence as doing so might lead some scenarios to having a very small number of data points for the Bayesian inference in the model comparison plots. Pooling samples is done to avoid model selection problems and to enable comparison across simulations. The pooling will mean that the statistics for the Bayesian inference in model comparison will be more robust to modes (as we might have several different modes present in the pooled samples), thus inflating performance. This will be an upper limit on the performance of the Bayesian method.

## Results

<!-- I assume that if the consensus matrices are both consistent with the PSMs and unchanging beyond a given value of $R$ that the chain has reached the staitonary distribution by said value of $R$ (albeit possibly a single mode of the target distribution).  -->

For a collapsed Gibbs sampler, the number of features present influences how diverse a range of partitions the sampler can explore. It can be seen in figure [exploration] that for a small number features the sampler suggests a different partition with every sample, but as  the number of features scales the diversity becomes more and more limited. This is not inherently a flaw; as the number of features increases so too does the amount of information pertaining to the structure in the dataset in the simulations used here. However, this does mean that it is more probable that the sampler becomes trapped in any modes it encounters as the attraction of any single mode becomes more extreme as too does the distance between modes. 

![As the number of features present beguns to significantly exceed the number of items present the collapsed Gibbs sampler struggles to sample a wide range of partitions. If the clarity of the underlying structure is low, this can be pathologically poor (only a single partition, the initialisation, is sampled). These results are for datasets of varying numbers of items and features (as labelled) and $K=5$. There is a single dataset generated for each value of $P \in \{1, \ldots, 500\}$ for each different value of $\Delta \mu$. ](./Images/Simulations/Exploration.png)


For inference using a single chain this does represent a problem. In the PSMs in figure [psms for large P], the chains all converge to a mode, but there is diversity in the modes reached. The chains do not converge or all reach the global mode, and none appear to explore a range of partitions regardless of the number of iterations. Therefore no individual chain appears to describe the uncertainty present well as they repeatedly sample the same partition whereas the use of  consensus clustering enables inclusion of many of the modes (possibly all) present in the data in the inference. In the results shown in this report it appears that the global mode is represented within the first ten chains, but one can imagine that there will be cases where the global mode is not reached in the initial chains. Thus, consensus clustering offers greater confidence that the distribution described by the ensemble has captured the global mode and that this will be the predominant mode in a set of 100 chains.

<!-- This means that for higher number of features the probability of becoming trapped within a single mode grows; thus when applying consensus clustering in this setting a larger number of chains should be included in the consensus clustering. -->


![The PSMs for the 8 chains that have passed the Shapiro-Wilks test of normality for the Geweke Z-score indicating within-chain stationarity in simulation 2 from a scenario with 50 items, 500 features and $(\Delta \mu, \sigma^2)=(1, 1)$. Three distinct modes can be identified here; consensus clustering can successfully include all of these within the inference where inference using any one chain cannot. Indeed it appears that consensus clustering finds an additional mode not described here (shown in appendix A). ](./Images/Simulations/small_n_large_p_base/PSMs/BayesianSimulation2PSMs.png)


However, in the cases where the clarity of the structure is less (i.e. as the distance between the means of the generating distributions becomes less), the sampler can display pathological behviour for large numbers of features. There is no new partition sampled after the initialisation (seen in the lines for $\Delta \mu = (0.2, 0.4, 0.6)$ in figure [ref exploraiton]). This is a fundamental problem for the collapsed Gibbs sampler that cannot be overcome by using consensus clustering. If one were to use consensus clustering in a context where the sampler cannot sample any partitions bar initialisation, then the CM will have approximately the same value in every off-diagonal entry once a sufficiently large number of chains is used for the CM to have stabilised. This indicates no clustering structure - no two items are considered more similar than any other two. Thus dimensionality can be a fundamental problem for the sampler that cannot be overcome by the use of many different initialisations.

Based upon the results for the scenarios where the clarity of the subpopulation structure becomes weaker, whether as a source of irrelevant features or increasing overlap between generating distributions (whether this be due to small $\Delta \mu$ or large $\sigma^2$), the samples are more representative of the true structure the deeper into the chain they are generated, assuming that the chain explores a range of partitions.

![The predictive performance of the different methods for the 100 simulations of the scenario with 20 relevant and 100 irrelevant features. Consensus ($r, s$) refers to $s$ samples drawn from the $r^{th}$ iteration of $s$ different chains. ``Mclust`` cannot uncover the true structure. For the sampling-based methods it appears that deeper into the chain the parttions become closer to the truth, as seen by the continuing improvement for higher values of $r$ and the excellent performance of the pooled long chains in the Bayesian inference. ](./Images/Simulations/irrelevant_features_100/irrelevant_features_100_all_model_performance.png)

As more irrelevant features are added the behaviour of the sampler changes. In this case the samples do not appear to be being drawn from the target distribution until much further into the chain. In the comparison of the consensus matrices for the scenario with 20 relevant features and 100 irrelevant feature shown in figure [cms], there is evidence of stabilising only by using $R=10,000$. This suggests that deeper chain depth is ideal for more noisy data. Despite this, consensus clustering can find the true structure using very short chains, although the uncertainty present is quite high. This is due to the different chains clustering different groups of items in different orders; the individual chains have not merged all of the generated components correctly at early iterations. This leads to the partitions presented containing too many clusters in the early samples. However, the chains do not appear to cluster items that are from different subpopulations in any sample. This means that the union of partitions from the short chains used in forming the consensus matrix does capture the true structure as the averaging over the partitions does have the correct items grouped together. Thus, the predicted clustering matches the true clustering even if the level of uncertainty present is high. This shows that the consensus clustering does work even in the case that each individual learner is very weak. If each contains some signal the ensemble can perform very well. This is reassuring as in the case that the number of irrelevant features is high, one would ideally sample from deeper in the chain; that one may avoid this and still perform meaningful inference means that consensus clustering may be used in settings where a single iteration is very slow and thus many short chains are necessary. 

![A grid of consensus matrices from the first simulation within the scenario with 20 relevant and 100 irrelevant features annotated by the true clustering, ordered by the CM from Consensus (10000, 100). Within the grid, each row has a constant $R$ and each column is a constant $S$ for $R \in (1, 10, 100, 1000, 10000)$ and $S \in (1, 10, 30, 50, 100)$. One can see that no structure is uncovered in the first iteration (corresponding to the result in figure ari-boxplots). The uncertainty is very high for $R=10$ and $S \neq 1$, but the structure is beginning to be uncovered (again this corresponds with the result in figure ari-boxplots). By the 100th iteration the structure is uncovered with a large degree of confidence. The 4 rightmost columns appear practically identical - the benefits of using more than 10 chains is less than the benefit of using more than 10 iterations within each chain. ](./Images/Simulations/irrelevant_features_100/CMs/ConsensusSimulation1ConsensusMatrixGrid.png)

This means that as the subpopulations become more similar or the level of irrelevant featues is high, deeper samples are preferred for a well-informed analysis. However, it is rewarding to see that if a point clustering is the object of interest, then there is evidence (see figure [ari-boxplots]) that extremely short chains can suffice, predicting a point estimate that is close to the generating structure, if enough learners are present in the final ensemble. 


<!-- ![For the scenario with 20 relevant features and $\sigma^2=9$, there is a similar process. Increasing $R$ appears to be more important than increasing $S$ as the (4, 2) entry in the grid corresponds closely to the (5, 5) entry .  ](./Images/Simulations/large_standard_deviation_3/CMs/ConsensusSimulation1ConsensusMatrixGrid.png) -->


<!-- The likelihood surface is very spiky due to the number of features (recall that the likelihood is an $(N \times P)$-polynomial for a mixture of Gaussian distributions with a diagonal covariance structure). -->


<!-- In this setting the benefits of the ensemble become clear as a number of modes can be represented in the analysis rather than the individual mode in each individual chain. However, if the distinction between the subpopulations is sufficiently small the sampler never explores any new partitions at all.  The results suggest that as the clarity of the structure weakens (i.e. no single mode is uniquely attractive) and the number of features scales, then the collapsed Gibbs sampler struggles more and more to explore **any** range of partitions, remaining trapped in the initialisation. This is a fundamental problem for the collapsed Gibbs sampler; using consensus clustering here returns a very sparse CM that implies no clustering structure as no two items will be expected to cluster together any more frequently than any other combination of items. -->



<!-- One can see that the for a constant number of samples, that the consensus clustering results stabilise for all $R \geq 10$ in the two-dimensional case.  -->

<!-- In considering both the PSMs and CMs, one can see that the large number of low magnitude values here suggests that many different possible partitions are being sampled. The fact that this occurs within each individual chain, as represented by the PSMs, and that there is no obvious difference between the different chains suggests that the sampler is exploring the full support of the distribution  within a single chain.  -->

<!-- The consensus clustering of short chains produces a similar distribution to the set of samples from a single long chain. No individual partition in any chain finds the true structure, but the distribution explored appears reasonable. In this setting it is the number of chains/samples, $S$, that is most important for improving performance. -->

<!-- As more relevant features are added the geometry of the likelihood surface becomes more complex. The sampler reaches a mode of the stationary distribution very quickly but appears to explore a decreasing number of possible partitions in any single chain; the attraction of each mode becomes greater, the distance between each mode increasing and the chain becomes more prone to finding a local mode. For example, in the PSMs from the small $N$, large $P$ case one can see that three different modes are present despite. However, consensus clustering can describe all of these modes. Again, a shallow chain depth, $R$, is sufficient and it is the number of chains that is important.  -->




The advantages of being able to use independent short chains in terms of run time is significant. Figure [time] shows that the run time of a sampler has a linear dependance upon the number of iterations run. In the scenario that there is only a single CPU present a consensus clustering using the 10th iteration from 100 independant chains is expected to be approximately the same as running a single chain for 10,000 iterations (a not unreasonable number of samples in Bayesian inference). In the more realistic scenario that one has access to multiple CPUs and is running several long chains to test convergence, than the gains are immediate and quite significant as each chain may be run on in parallel to all others, but the chain itself must be run serially. This means that consensus clustering can benefit more significantly from parallelisation than Bayesian inference.


<!-- The combined results of the scenarios with high numbers of irrelevant features but some true structure and the scenario where no clustering signal is present is highly reassuring. -->

<!-- These show that one can use consensus clustering on real, noisy data with the expectation that any structure uncovered is of interest and not  in the data and not uncovering structure within the noise. -->




<!-- These results also show that one can include features in the analysis that do not necessarily contain structure, allowing one to be less rigorous or even avoid feature selection.  -->

<!-- However, it appears that in an 'omics scenario where there may be thousands or tens of thousands of possible features that feature selection is possibly an unavoidable part of an analysis pipeline. -->



![The time taken for each Markov chain to finish appears to scale linearly with the number of samples generated. This suggests that using shorter chains will reduce the computational time linearly, thus if the individual chains may be run in parallel significant time savings may be available. Assume one has 4 CPUs available to run chains upon and a single iteration takes $t$ seconds. Then based upon the relationship shown here, 10 chains of 10,000 iterations would take approximately $3 \times 10^4 t$ seconds and 100 chains of 10 iterations would take roughly $2.5 \times 10^2 t$ seconds, a difference of several orders of magnitude. ](./Images/Simulations/base_case/base_caseTimeComparisonGibbs.png)




## Discussion

Based upon the results shown above and those in the supplementary material, consensus clustering is a powerful method for performing inference using Bayesian mixture models. 

Even for extremely short chains, an ensemble of clsuterers can produce powerful results. In many scenarios consensus clustering of mixture models correctly identifies the true number of clusters present and the membership of these clusters. 

In settings where a single chain does not display pathological issues (i.e. it explores the full support of the target distribution), consensus clustering performs competitvely with Bayesian inference.

However, it is the settings where any single chain struggles to represent more than a single mode of the target distribution that consensus clustering offers the greatest benefits. In this setting consensus clustering enables inclusion of many modes in the inference, capturing more of the model uncertainty than any single chain. Furthermore, these settings tend to be high dimensional and thus the sampler tends to be slower to perform a single iteration and the possible computational gains offered by the ensemble in a parallel environment become more significant.

Additionally, I have shown that the short chains are robust to irrelevant features, outperforming ``Mclust`` when the dataset is dominated by irrelevant features. 

However, consensus clustering for Bayesian mixture models is intended to solve problems encountered in real, complex data. How then, does one apply consensus clustering when there is no known solution? How does one choose $S$ and $R$, the parameters of the ensemble? This is not a problem that currently has an analytic solution. The ideal solution, analagously to Bayesian inference, is to let chains be as long as possible and to use as many different chains as possible, i.e. let $S \to \infty, R \to \infty$. In practice there are some rules based upon observations in the simulation study that can help in deciding one's parameters *a priori*

* If the data is noisy, believed to contain many irrelevant features or poor distinction between sub-populations, one should try increase the value of $R$. 
* As the total number of features scales so too should $S$, the number of learners used in the ensemble. 

<!-- However, to check whether one's initial choices of $R$ and $S$ are sufficiently large, I recommend constructing a grid of consensus matrices similar to that displayed in figure [cms]. Using this to visuallycompare the consensus matrices for different values of $S$ and $R$ is useful. Consider the following method for deciding if the inputs $(R', S')$ are sufficiently large for one's inference: -->

1. Generate $S'$ Markov chains of $R'$ partitions.
2. Use these to construct 3 consensus matrices. For some $a, b \in [0.5,1]$ (for example $a=b=\frac{3}{4}$), compure CMs for each of:

    * Consensus$(a R', S')$;
    * Consensus$(R', b S')$;
    * Consensus$(R', S')$;
  
3. If the consensus matrices are sufficiently stable, stop. If the matrices are not considered sufficiently stable for the change in $R$ between $aR'$ and $R'$,  repeat the analysis for some $R'' > R'$. Similarly, one may judge if the results have sufficiently stabilised for increases in $S$.

<!-- Possibly a stopping rule could be created base upon some threshold $\epsilon \in \mathbb{R_{+}}$ (for example see equation \@ref(eq:stoppingRule)), though in practice a visual comparison of the CMs should be sufficient. If the matrices are not considered sufficiently stable for the change in $R$ between $aR'$ and $R'$,  repeat the analysis for some $R'' > R'$. Similarly one may judge if the results have sufficiently stabilised for increases in $S$. -->

Of course, there may be settings in which one is using consensus clustering as each iteration of the sampler is slow and thus there is a computational limit upon $R$. In this case if one believes that this limit is less than the value required for a stable CM (i.e. probably on the scale of $10^1$) one must validate the final clustering upon the data and prior knowledge of relaitonships within the data. Of course, this step is intrinsic to any cluster analaysis but perhaps acquires a greater depth of rigour in this case.

<!-- \begin{align} -->
<!-- &||CM(R', S') - CM(R', b S')||_F &< \epsilon \\ -->
<!-- &||CM(R', S') - CM(a R', S')||_F &< \epsilon -->
<!-- (\#eq:stoppingRule)  -->
<!-- \end{align} -->

<!-- ### Performance -->

<!-- I think that if uncertainty is ever of interst than Mclust has to be dropped. If this is not the case then Mclust is a powerful tool in the absence of irrelevant features. -->

<!-- If $P$ is small that Bayesian inference is ideal; however once $P$ breaks into the hundreds and the attraction of modes becomes more extreme I think one should consider consensus clustering as a natural way to represent these in one's analysis, or else consider other types of sampler. However, if $P$ is extreme these samplers might be sufficiently poor at exploring the partition space as to be infeasible regardless of the method of inference. -->

<!-- #### Small $P$ -->

<!-- In a low dimensional, low signal setting Mclust performs excellently. In a real applicaiton where no labels are present and uncertainty quantification is important, than Bayesian inference performs very well, exploring the target distribution thoroughly. Based upon the behaviour of the consensus matrices and the improvement in ARI as a function of chain depth, $R$, it appears that one could use consensus clustering in a low dimensional setting as $R=10$ appears sufficient to perform similar inference to the Bayesian inference. Therefore one could use a parallel environment to decrease the time required to perform the analsysis. However, as true Bayesian inference can be performed quickly on this scale and is exploring a range of partitions, I would recommend using Bayesian inference, with all its guarantees in this setting. -->

<!-- #### False structure -->

<!-- It is reassuring to see that the methods all correctly identify no structure. This helps to reassure us that we can use these methods and will not identify noise as structure. The low degree of signal present in the case with $\sigma=5$ and in the small $N$ large $P$ scenario with close cluster means also saw no structure uncovered, suggesting that one may use these methods and be somewhat reassured that any signal uncovered is true. In the high dimensional setting one would have to recognise that the clustering in the PSMs corresponded to the initialisation; for practitioners who might be confused by this and the appearance of 10 modes in their 10 chains, perhaps consensus clustering, with it's very clear lack of structure, is preferable. -->

<!-- #### Loud signal -->

<!-- When signal is sufficiently loud that the clustering structure is easily visible within a heatmap, choice of method is less impotant. -->

<!-- #### Weak and hidden signal -->

<!-- As the signal becomes weaker sampler based methods perform better than Mclust, exploring a greater vairety of partitions and clearly stating that the uncertainty is high. If the signal is buried beneath a large amount of noise (rather than being inherently weak as in the large standard deviation cases), then the sampling-based methods perfom very well; these appear very robust to a large number of irrelevant features. However as the number of noise features increases sampling from deeper within the chain becomes more important for consensus clustering. I suspect that this is partially a function of the increase in the number of features independant of the ratio of relevant to irrelevant features. -->

<!-- #### Large $P$ -->

<!-- In this setting I think consensus clustering shines. As the collapsed Gibbs sampler appears to find a sensible partition quite quickly (as seen by the stability of results for consensus clustering after $R$ exceeds 1) one can run the chains for a short length of time. Furthermore, one may use the lack of correlation of samples across chains to explore multiple modes. Bayesian inference struggles in this setting, becoming trapped in a mode and never escaping its attraction. It does identify useful structure, but claims of convergence become dubious and some of the main attractions of Bayesian inference, i.e. principled quantification of the uncertainty and the asymptotic guarantees, appear to not truly hold in practice. -->

<!-- ### Using consensus clustering in practice -->

<!-- Consensus clustering has shown significant advantages. As $P$ grows and modes become more extreme, consensus clustering is better able to represent multiple modes than Bayesian inference (as seen in the irrelevant features 100 and the small $N$ large $P$ base case). However, if the sampler never explores a sensible space (as is the case in the scenario with small $N$ and large $P$ where the distance between cluster means, $\Delta \mu$, is small), than consensus clustering does not solve the problems. (Aside: I do think that the small $N$ large $P$ case does not contain strong signal of structure, and in this case finding no structure is an acceptable solution as the consensus clustering does.) -->

<!-- I think the performance of consensus clustering for even very small values of $R$ is surprising. In many scenarios setting $R=10$ was sufficient to uncover much of the structure or at least indicate it (as is in the case where $P_n=100$ where the consensus matrices corresponding to $R=10$ indicate the true structure with high uncertainty). I think that is exciting, as in cases where each iteration is slow (such as the large $P$ case) than consensus clustering performs well even if chains are severely constrained. -->

<!-- Based upon the results seen I would say that in practice Consensus ($R$, $S$) should be driven by the heuristics: -->

<!-- \begin{align} -->
<!--  R &\propto P_n \\ -->
<!--  R &\propto P \\ -->
<!--  S &\propto \frac{\mu}{\sigma} \\ -->
<!--  S &\propto \textrm{The number of modes present.} -->
<!-- \end{align} -->

<!-- Of these, only $P$ can be confidently known. As this is the case I think making use of grids of the consensus matrices could help. They can provide evidence for when increasing $S$ and $R$ no longer offers significant gains. I think the Frobenius norm could be used to construct a stopping rule for these inputs of the inference, e.g. if the Frobenius norm of the difference of the matrix for Consensus $(R, S_1)$ and the matrix for Consensus $(R, S_2)$ (for some $S_1, S_2, R \in \mathbb{N}, S_2 > S_1$) is small than one should stop increasing $S$ and similarly for different values of $R$. One would have to pursued this far enough that one can believe that the stability of the consensus matrix is true. Based upon the results so far I think, letting $CM(r, s)$ denote a consensus matrix for consensus clustering using the $r^{th}$ iteration of $s$ chains, that if: -->

<!-- \begin{align} -->
<!-- &||CM(r, s) - CM(r, s + 10)||_F &< \epsilon \\ -->
<!-- &||CM(r, s) - CM(r + 100, s)||_F &< \epsilon -->
<!-- \end{align} -->

<!-- for some small $\epsilon$ (which itself will be a function of $N$ as the Frobenius norm does not have an upper bound), then one should stop increasing the parameters of the inference. In practice  I think a visual inspection of the grid would be sufficient. Note that this appears to be asymptotic behaviour and increasing $S$ and $R$ to the largest values possible within one's computational constraints would not do any harm. -->

## Work remaining

Further investigation is required for consensus clustering of Bayesian mixture models. There is no large $N$ small $P$ scenarios in this analysis. These scenarios have yet to be completed as the problem of performing inference upon the PSM/Consensus matrix of size $10,000 \times 10,000$ is proving non-trivial. Beyond these scenariosI intend to investigate two additional cases 

1. an additional small $N$ large $P$ case where $\Delta \mu$ is a value between that used in the two scenarios presented here (e.g. 0.6); and
2. a varying proportions scenario where the overlap between clusters is less defined.

Of these, (1) would be inteded to test how much signal is needed in a high-dimensional setting before the collapsed Gibbs sampler explores the space at all. (2) would test how well the methods perform in a more realistic setting where the subpopulations are significantly different in size. In the current scenario of varying proportions, the signal is too clear and identifying the correct number of clusters is non-trivial. All methods perform well; the tenth sample drawn from a single Markov chain aligns perfectly with the generating structure.

<!-- ## Clustering performance -->



<!-- Consider an item $x_i$ that truthfully has allocation label $c_i$. Now say that our similarity matrix has $x_i$ allocated correctly (i.e. with the other items that have allocation $c_i$) with a score of 0.4, but misallocated to some $c_j \neq c_i$ with a score of 0.6. In the predicted clustering calculated from our similarity matrix we will allocate $x_i$ to the wrong cluster and this will lessen the ARI between the truth and the predicted clustering. However, the model has been uncertain about $x_i$'s allocation. The Frobenius inner product will capture this uncertainty and (in this case) reward the model with a higher score. Thus the Frobenius product more accurately describes the model performance. -->

