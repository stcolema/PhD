# Simulations {#simulations}

## Introduction

I want to create a simulation study to do two things:

1. absolute evaluation of consensus clustering as a inferential technique for mixture models; and 
2. comparative evaluation of consensus and Bayesian inference of mixture models and Mclust, a popular MLE-based mixture model R package.

I am interested in evaluating:

* Predictive performance - how well each method predicts the true clustering (metric: *Adjusted Rand Index* [@hubert1985comparing] between the predicted clustering from each method and the true clustering);
* Uncertainty quantification - how well each method quantifies uncertainty about the true clustering (metric: *Frobenius norm* of the difference of the coclustering / consensus / posterior similarity matrices and the true coclustering matrix); and
* Speed - how long each method takes to run (metric: *seconds*).

## Data generating mechanism

<!-- For this case I use the notation: -->

<!-- * $N$: the number of items generated; -->
<!-- * $P$: the number of relevant features used; -->
<!-- * $P_n$: the number of irrelevant features used; -->
<!-- * $K$: the true number of clusters present; -->
<!-- * $X = (x_1, \ldots, x_N)$: the items generated; -->
<!-- * $\pi=(\pi_1, \ldots, \pi_K)$: the expected proportions of the population belonging to each cluster; -->
<!-- * $c=(c_1, \ldots, c_N)$: the allocation variable for each item; -->
<!-- * $\theta=(\theta_1, \ldots, \theta_K)$: the parameters associated with each component; and -->
<!-- * $\phi=(\phi_1,\ldots, \phi_P)$: the indicator variable of feature relevance. -->

Our data generating model is a finite mixture model with independent features where irrelevant features have global parameters rather than component specific parameters:

\[
\begin{aligned}
p(x, z, \theta, \pi) &= \prod_{i=1}^N p(x_i | z_i, \theta_{z_i}) \prod_{i=1}^N p (z_i | \pi) p(\pi) p(\theta) \\
  &= \prod_{i=1}^N \prod_{p=1}^P p(x_{ip} | z_i, \theta_{z_ip})^{(1 - \phi_p)} p(x_{ip} | \theta_p) ^ {\phi_p} \prod_{i=1}^N p (z_i | \pi) p(\pi) p(\theta)
\end{aligned}
\]

During my simulations I assume that the model in question is a mixture of *Gaussians* and thus $\theta_{kp}=(\mu_{kp}, \sigma^2_{kp})$.

As each method of inference uses a common model, this data-generating mechanism is not expected to favour any one method over another.

I test seven different scenarios that change various parameters in this model. I will define the component parameters by $\Delta_{\mu}$, the distance between the possible means in each feature, and $\sigma^2$, a common standard deviation across all components and features. 

The scenarios tested are:

1. The 2D Gaussian case (this is a sense-check);
2. The lack-of-structure case in 2 dimensions;
3. The base case for which scenarios 4-6 are variations;
4. Increasing $\sigma^2$;
5. Increasing the number of irrelevant features;
6. Varying the expected proportion of the total population assigned to each sub-population;
7. The large $N$, small $P$ paradigm; and
8. The small $N$, large $P$ case.

<!-- A more detailed description of each scenario and various sub-scenarios is given in the below table. -->

<!-- ```{r scenario_table, echo=F} -->
<!-- set.seed(1) -->
<!-- scn_table <- data.frame( -->
<!--   Scenario = c("Simple 2D", "No structure", "Base Case", rep("Large N, small P", 3), rep("Large standard deviation", 3), rep("Irrelevant features", 5), rep("Small N, large P", 2), "Varying proportions"), -->
<!--   N = c(100, 100, 2e2, 1e4, 1e4, 1e4, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 50, 50, 200), -->
<!--   P_s = c(2, 0, 20, 4, 4, 4, 20, 20, 20, 20, 20, 20, 20, 20, 500, 500, 20), -->
<!--   P_n = c(0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 10, 20, 100, 200, 0, 0, 0), -->
<!--   K = c(5, 1, 5, 5, 50, 50, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5), -->
<!--   Delta_mu = c(3, 0, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.2, 1), -->
<!--   sigma2 = c(1, 1, 1, 1, 1, 1, 3, 5, 10, 1, 1, 1, 1, 1, 1, 1, 1), -->
<!--   Pi = c(rep("vec(1/K)", 16), "(0.5, 0.25, 0.125, 0.0675, 0.0675)") -->
<!-- ) -->

<!-- colnames(scn_table) <- c( -->
<!--   "Scenario", -->
<!--   "$N$", -->
<!--   "$P_s$", -->
<!--   "$P_n$", -->
<!--   "$K$", -->
<!--   "$\\Delta_{\\mu}$", -->
<!--   "$\\sigma^2$", -->
<!--   "$\\pi$" -->
<!-- ) -->

<!-- sims_used <- scn_table[c(1:3, 7, 8, 11:13, 17, 4:6, 15:16), ] %>% -->
<!--   set_rownames(1:nrow(.)) -->

<!-- knitr::kable(sims_used, row.names = T, escape = F) # , "latex", longtable = T, booktabs = T, caption = "Longtable") -->
<!-- ``` -->

<!-- \begin{center} -->
<!-- \captionof{table}{Multirow Table.} -->
<!-- \begin{tabular}{l|l|r} -->
<!-- \textbf{Value 1} & \textbf{Value 2} & \textbf{Value 3} \\ -->
<!-- $\alpha$ & $\beta$ & $\gamma$ \\ \hline -->
<!-- \multirow{2}{*}{12} & 1110.1 & a \\ -->
<!--  & 10.1 & b \\ \hline -->
<!-- 3 & 23.113231 & c \\ -->
<!-- 4 & 25.113231 & d -->
<!-- \end{tabular} -->
<!-- \end{center} -->

### Scenario purposes

Each scenario is seen as testing certain cases or concepts.

#### Simple 2D
This case is seen as an introduction to method performance. In this case the data can be plotted as a scatter plot (whereas higher dimensions mean any two dimensional representation is missing some information about the data). This means that one may view the data and have an intuition if it is possible for a mixture model to cluster the data; furthermore the small dimensionality means that the distance between modes on the likelihood surface is less so we do not expect the Gibbs sampler to become trapped. This means that we can compare consensus clustering to Bayesian inference with a strong belief that the Markov chain has converged (given appropriate tests). This means that we can process how well consensus clustering performs with a deeper understanding of the context than might be possible in higher dimensional cases.

<!-- ```{r, simple2dView, echo = F, cache = T} -->
<!-- scn <- "Simple 2D" -->
<!-- row_ind <- which(sims_used$Scenario == scn) -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->

<!-- # Generate dataset -->
<!-- simple2D_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(simple2D_gen$data), simple2D_gen$cluster_IDs, -->
<!--   main = "Simple 2D (seed 1)", -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->

<!-- simple2D_gen$data %>% -->
<!--   scale() %>% -->
<!--   data.frame() %>% -->
<!--   dplyr::mutate(Cluster = as.factor(simple2D_gen$cluster_IDs)) %>% -->
<!--   ggplot(aes(x = Gene_1, y = Gene_2, colour = Cluster)) + -->
<!--   geom_point() + -->
<!--   scale_colour_viridis_d() + -->
<!--   labs( -->
<!--     title = "Example: Simple 2D case (seed 1)", -->
<!--     x = "Gene 1", -->
<!--     y = "Gene 2", -->
<!--     subtitle = "Note that multiple clusters may overlap." -->
<!--   ) -->
<!-- ``` -->

#### No structure

In this case all items are drawn from the same normal distribution. It is meant to be a test to see how the inference methods perform when there is no structure to find. It may be considered as similar to attempting to quantify the false positive rate in a predictive method.

<!-- ```{r, noStructureView, echo = F, cache = T} -->
<!-- scn <- "No structure" -->
<!-- row_ind <- which(sims_used$Scenario == scn) -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->

<!-- # Generate dataset -->
<!-- noStructure_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(noStructure_gen$data), noStructure_gen$cluster_IDs, -->
<!--   main = "No structure (seed 1)", -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->

<!-- noStructure_gen$data %>% -->
<!--   scale() %>% -->
<!--   data.frame() %>% -->
<!--   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>% -->
<!--   ggplot(aes(x = Noise_1, y = Noise_2, colour = Cluster)) + -->
<!--   geom_point() + -->
<!--   scale_colour_viridis_d() + -->
<!--   labs( -->
<!--     title = "Example: No structure case (seed 1)", -->
<!--     x = "Gene 1", -->
<!--     y = "Gene 2" -->
<!--   ) -->
<!-- ``` -->

#### Base case

This is the _base case_ which most of the other scenarios are some variation of. This is intended to be a benchmark against which performance may be judged. Each method is expected to perform well here across all metrics, predicting the true clustering correctly and with very little uncertainty.

<!-- ```{r, baseCase, echo = F, cache = T} -->
<!-- scn <- "Base Case" -->
<!-- row_ind <- which(sims_used$Scenario == scn)[1] -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->


<!-- # Generate dataset -->
<!-- baseCase_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(baseCase_gen$data), baseCase_gen$cluster_IDs, -->
<!--   main = paste0(scn, " (seed 1)"), -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->


<!-- # Set labels for facet wrapping -->
<!-- cluster_labels <- c(paste0("Cluster ", 1:K)) -->
<!-- names(cluster_labels) <- 1:K -->


<!-- pc1 <- prcomp(baseCase_gen$data)$x -->
<!-- pcaSeriesPlot(x = pc1, labels = baseCase_gen$cluster_IDs, n_comp = min(P_s, 6)) + -->
<!--   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) + -->
<!--   labs( -->
<!--     title = paste0("PCA series for ", scn, " scenario") #, -->
<!--     # subtitle = "Notice how the clusters separate across the components." -->
<!--   ) -->

<!-- # noStructure_gen$data %>% -->
<!-- #   scale() %>% -->
<!-- #   data.frame() %>% -->
<!-- #   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>% -->
<!-- #   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) + -->
<!-- #   geom_point() + -->
<!-- #   scale_colour_viridis_d() + -->
<!-- #   labs( -->
<!-- #     title = "Example: No structure case (seed 1)", -->
<!-- #     x = "Gene 1", -->
<!-- #     y = "Gene 2" -->
<!-- #   ) -->
<!-- ``` -->

#### Large standard deviation

This is to test how distinct the clusters must be for the inference to perform well. In many datasets there is no reason to believe that the clusters should be strongly separable; this setting is intended to better understand how sensitive the different approaches are to overlap of subpopulations.

<!-- _A priori_, I would think that Bayesian inference would perform optimally here. The reasonable number of features (20) means that the collapsed Gibbs sampler should explore the space well. The large variance within each cluster means that much of the signal is hidden and the data might appear more as a continuum with no clustering structure; but I hope that the Bayesian inference will correctly idenitfy some structure and capture a large amount of uncertainty. I think consensus clustering will perform quite well and do not have a strong intuition for how ``Mclust`` will perform. I think that the clever initialisation based upon ``hclust`` will help in unpicking much structure, but I suspect that the final clustering will be too certain. -->

<!-- ```{r, largeStandardDeviation, echo = F, cache = T} -->
<!-- scn <- "Large standard deviation" -->
<!-- row_ind <- which(sims_used$Scenario == scn)[1] -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->


<!-- # Generate dataset -->
<!-- largeStadDev_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(largeStadDev_gen$data), largeStadDev_gen$cluster_IDs, -->
<!--   main = "Large N, small P (seed 1)", -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->


<!-- # Set labels for facet wrapping -->
<!-- cluster_labels <- c(paste0("Cluster ", 1:K)) -->
<!-- names(cluster_labels) <- 1:K -->


<!-- pc1 <- prcomp(largeStadDev_gen$data)$x -->
<!-- suppressWarnings( -->
<!--   pcaSeriesPlot(x = pc1, labels = largeStadDev_gen$cluster_IDs, n_comp = min(P_s, 6)) + -->
<!--   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) + -->
<!--   labs( -->
<!--     title = "PCA series for large N, small P scenario", -->
<!--     subtitle = "Notice how the clusters separate across the components." -->
<!--   ) -->
<!-- ) -->
<!-- # noStructure_gen$data %>% -->
<!-- #   scale() %>% -->
<!-- #   data.frame() %>% -->
<!-- #   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>% -->
<!-- #   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) + -->
<!-- #   geom_point() + -->
<!-- #   scale_colour_viridis_d() + -->
<!-- #   labs( -->
<!-- #     title = "Example: No structure case (seed 1)", -->
<!-- #     x = "Gene 1", -->
<!-- #     y = "Gene 2" -->
<!-- #   ) -->
<!-- ``` -->

<!-- To highlight how strongly the clusters overlap within each feature I include a  -->
<!-- an example of the within-feature distributions for each case. -->

<!-- ```{r, example_large_std_dev, echo=F} -->


<!-- p1 <- lapply(1:5, function(x){rnorm(5000, x, 5)}) %>%  -->
<!--   data.frame() %>% -->
<!--   set_colnames(1:5) %>% -->
<!--   pivot_longer(everything(), names_to = "Cluster", values_to = "Value") %>%  -->
<!--   ggplot(aes(x = Value, colour = Cluster)) + -->
<!--   geom_density() + -->
<!--   scale_color_viridis_d() + -->
<!--   labs( -->
<!--     title = "Standard deviation 5" -->
<!--   ) + -->
<!--   xlim(-15, 25) -->


<!-- p2 <- lapply(1:5, function(x){rnorm(5000, x, 3)}) %>%  -->
<!--   data.frame() %>% -->
<!--   set_colnames(1:5) %>% -->
<!--   pivot_longer(everything(), names_to = "Cluster", values_to = "Value") %>%  -->
<!--   ggplot(aes(x = Value, colour = Cluster)) + -->
<!--   geom_density() + -->
<!--   scale_color_viridis_d() + -->
<!--   labs( -->
<!--     title = "Standard deviation 3" -->
<!--   ) + -->
<!--   xlim(-15, 25) -->

<!-- (p2 / p1) + plot_annotation(title = "Example of clusters within a feature for large standard deviation data") -->


<!-- ``` -->

<!-- One can see that the clusters overlap severaly within each feature and that unpicking any structure might be highly difficult, partiuclarly in the second case. -->

#### Large $N$, small $P$

This is to test the _large $N$, small $P$_ scenario.

<!-- This is a difficult case to -->
<!-- analyse as visualising the result is difficult for a PSM (due to the $N \times N$  -->
<!-- dimension of this), and obtaining a predicted clustering is also very slow. -->

<!-- ```{r, largeNsmallP, echo = F, cache = T} -->
<!-- scn <- "Large N, small P" -->
<!-- row_ind <- which(sims_used$Scenario == scn)[1] -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->


<!-- # Generate dataset -->
<!-- largeNsmallP_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(largeNsmallP_gen$data), largeNsmallP_gen$cluster_IDs, -->
<!--   main = "Large N, small P (seed 1)", -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->


<!-- # Set labels for facet wrapping -->
<!-- cluster_labels <- c(paste0("Cluster ", 1:K)) -->
<!-- names(cluster_labels) <- 1:K -->


<!-- pc1 <- prcomp(largeNsmallP_gen$data)$x -->
<!-- pcaSeriesPlot(x = pc1, labels = largeNsmallP_gen$cluster_IDs, n_comp = min(P_s, 6)) + -->
<!--   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) + -->
<!--   labs( -->
<!--     title = "PCA series for large N, small P scenario", -->
<!--     subtitle = "Notice how the clusters separate across the components." -->
<!--   ) -->

<!-- # noStructure_gen$data %>% -->
<!-- #   scale() %>% -->
<!-- #   data.frame() %>% -->
<!-- #   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>% -->
<!-- #   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) + -->
<!-- #   geom_point() + -->
<!-- #   scale_colour_viridis_d() + -->
<!-- #   labs( -->
<!-- #     title = "Example: No structure case (seed 1)", -->
<!-- #     x = "Gene 1", -->
<!-- #     y = "Gene 2" -->
<!-- #   ) -->
<!-- ``` -->


#### Small $N$, large $P$

This is to test the _small $N$, large $P$_ scenario so common in genetics. An 
example of this data would be in clustering patients based upon their gene 
expression data. Such datasets arise consistently in precision medicine, 
many of the the Cancer Genome Atlas (**TCGA**) datasets correspond to this 
setting.

<!-- I  -->
<!-- think all methods will succeed in unpicking structure in the first case, but as -->
<!-- the distance between means decreases it will be interesting to see how each  -->
<!-- method performs. If the distance becomes too small and the resulting mixture of  -->
<!-- Gaussians effectively merge such that unpicking anyone cluster from its  -->
<!-- neighbours is not feasible, than this will be similar to a high-dimensional  -->
<!-- extension to the _no structure_ case. -->

<!-- ```{r, smallNlargeP, echo = F, cache = T} -->
<!-- scn <- "Small N, large P" -->
<!-- row_ind <- which(sims_used$Scenario == scn)[2] -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->


<!-- # Generate dataset -->
<!-- smallNlargeP_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(smallNlargeP_gen$data), smallNlargeP_gen$cluster_IDs, -->
<!--   main = "Small N, large P (seed 1)", -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->


<!-- # Set labels for facet wrapping -->
<!-- cluster_labels <- c(paste0("Cluster ", 1:K)) -->
<!-- names(cluster_labels) <- 1:K -->


<!-- pc1 <- prcomp(smallNlargeP_gen$data)$x -->
<!-- pcaSeriesPlot(x = pc1, labels = smallNlargeP_gen$cluster_IDs, n_comp = min(P_s, 6)) + -->
<!--   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) + -->
<!--   labs( -->
<!--     title = "PCA series for small N, large P scenario", -->
<!--     subtitle = "The clusters appear to be separable, despite the wide range of loadings within clusters." -->
<!--   ) -->

<!-- # noStructure_gen$data %>% -->
<!-- #   scale() %>% -->
<!-- #   data.frame() %>% -->
<!-- #   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>% -->
<!-- #   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) + -->
<!-- #   geom_point() + -->
<!-- #   scale_colour_viridis_d() + -->
<!-- #   labs( -->
<!-- #     title = "Example: No structure case (seed 1)", -->
<!-- #     x = "Gene 1", -->
<!-- #     y = "Gene 2" -->
<!-- #   ) -->
<!-- ``` -->

#### Irrelevant features

This is to test how robust to noise the methods are. In many settings, such as 
gene expression data, a large number of noisy features that contribute little 
or no information about underlying structure are expected to be present. 
Investigating how robust each method is to irrelevant features is important; if
methods are highly sensitive to noise, feature selection becomes a far more 
important part of the analysis. Feature selection is a highly difficult problem
and presents an opportunity to remove valuable information if done incorrectly. 


<!-- I expect that all methods  -->
<!-- will perform wll when the number of feautres containing signal is greater than  -->
<!-- those containing noise, but as the ratio flips and the number of irrelevant  -->
<!-- features dominates, I expect ``Mclust`` to struggle. I will be interested to -->
<!-- see how Bayesian inference and consensus clustering perform, hoping that the Markov chain  -->
<!-- does produce sensible samples; it might be deeper into the chain before the  -->
<!-- correct space of partitions is being explored. This is of interest in the  -->
<!-- context of gene expression data where many irrelevant features are present;  -->
<!-- ideally our method will be robust to this, as it would mean that a practitioner -->
<!-- could use a less intense feature selection as removing all noisy features  -->
<!-- would not be a requisite to avoid obscuring subpopulation signal. -->

<!-- ```{r, irrelevantFeatures, echo = F, cache = T} -->
<!-- scn <- "Irrelevant features" -->
<!-- row_ind <- which(sims_used$Scenario == scn)[3] -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->


<!-- # Generate dataset -->
<!-- irrelevantFeatures_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(irrelevantFeatures_gen$data), irrelevantFeatures_gen$cluster_IDs, -->
<!--   main = paste0("Irrelevant features (seed 1, ", P_n, " irrelevant features)"), -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->


<!-- # Set labels for facet wrapping -->
<!-- cluster_labels <- c(paste0("Cluster ", 1:K)) -->
<!-- names(cluster_labels) <- 1:K -->


<!-- pc1 <- prcomp(irrelevantFeatures_gen$data)$x -->
<!-- pcaSeriesPlot(x = pc1, labels = irrelevantFeatures_gen$cluster_IDs, n_comp = min(P_s, 6)) + -->
<!--   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) + -->
<!--   labs( -->
<!--     title = "PCA series for irrelevant features scenario", -->
<!--     subtitle = "In this example one might expect clusters 2 and 3 to merge, with cluster 5 also\nin danger of being subsumed." -->
<!--   ) -->

<!-- # noStructure_gen$data %>% -->
<!-- #   scale() %>% -->
<!-- #   data.frame() %>% -->
<!-- #   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>% -->
<!-- #   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) + -->
<!-- #   geom_point() + -->
<!-- #   scale_colour_viridis_d() + -->
<!-- #   labs( -->
<!-- #     title = "Example: No structure case (seed 1)", -->
<!-- #     x = "Gene 1", -->
<!-- #     y = "Gene 2" -->
<!-- #   ) -->
<!-- ``` -->

#### Varying proportions

This simulation is designed to test how sensitive the methods are to relatively 
small clusters when large clusters are present. In precision medicine, one might
beleive that some of the clusters present might be singletons; uncovering such 
structure in an analysis could be very important for clinical outcomes.

<!-- ```{r, varyingProportionsView, echo = F, cache = T} -->
<!-- scn <- "Varying proportions" -->
<!-- row_ind <- which(sims_used$Scenario == scn)[1] -->
<!-- N <- sims_used[["$N$"]][row_ind] -->
<!-- P_s <- sims_used[["$P_s$"]][row_ind] -->
<!-- P_n <- sims_used[["$P_n$"]][row_ind] -->
<!-- K <- sims_used[["$K$"]][row_ind] -->
<!-- dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind] -->
<!-- s2 <- sims_used[["$\\sigma^2$"]][row_ind] -->
<!-- pi <- sims_used[["$\\pi$"]][row_ind] -->

<!-- if (pi == "vec(1/K)") { -->
<!--   pi <- rep(1 / K, K) -->
<!-- } else { -->
<!--   pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>% -->
<!--     stringr::str_remove_all("[() ]") %>% -->
<!--     strsplit(",") %>% -->
<!--     unlist() %>% -->
<!--     as.numeric() -->
<!-- } -->


<!-- # Generate dataset -->
<!-- varyingProportions_gen <- generateSimulationDataset(K, N, P_s, -->
<!--   delta_mu = dm, -->
<!--   p_n = P_n, -->
<!--   cluster_sd = s2, -->
<!--   pi = pi -->
<!-- ) -->


<!-- annotatedHeatmap(scale(varyingProportions_gen$data), varyingProportions_gen$cluster_IDs, -->
<!--   main = paste0(scn, " (seed 1)"), -->
<!--   show_rownames = F, -->
<!--   show_colnames = F -->
<!-- ) -->


<!-- # Set labels for facet wrapping -->
<!-- cluster_labels <- c(paste0("Cluster ", 1:K)) -->
<!-- names(cluster_labels) <- 1:K -->


<!-- pc1 <- prcomp(varyingProportions_gen$data)$x -->
<!-- pcaSeriesPlot(x = pc1, labels = varyingProportions_gen$cluster_IDs, n_comp = min(P_s, 6)) + -->
<!--   facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) + -->
<!--   labs( -->
<!--     title = paste0("PCA series for ", scn, " scenario") #, -->
<!--     # subtitle = "Notice how the clusters separate across the components." -->
<!--   ) -->

<!-- # noStructure_gen$data %>% -->
<!-- #   scale() %>% -->
<!-- #   data.frame() %>% -->
<!-- #   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>% -->
<!-- #   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) + -->
<!-- #   geom_point() + -->
<!-- #   scale_colour_viridis_d() + -->
<!-- #   labs( -->
<!-- #     title = "Example: No structure case (seed 1)", -->
<!-- #     x = "Gene 1", -->
<!-- #     y = "Gene 2" -->
<!-- #   ) -->
<!-- ``` -->

## Method

### Data-generation

For each of the scenarios described above, I generate 100 different datasets and cluster IDs.
These cluster IDs are the true labelling from which the subpopulations defining the full dataset are defined.

### Inference

I use the C++ implementation of Bayesian mixture models written by @mason2016mdi. This performs inference upon a mixture model using a Collapsed Gibbs sampler. This is the software for which samples are generated for both consensus clustering and Bayesian inference.

For MLE, I use the R package ``Mclust`` [@R-mclust]. This is a popular implementation that uses an initialisation based upon hierarchical clustering (and is therefore a non-random initialisation, unlike the Bayesian inference and consensus clustering).

For each dataset the following inference are performed:

1. Consensus clustering using a range of different choices of $R$ and $S$. The sets used are:
  * $R = \{1, 2, \ldots, 10, 20, \ldots, 100, 200, \ldots, 1000, 2000, \ldots, 10000\}$; and
  * $S = \{1, 2, \ldots, 100\}$.
  I will showcase a subset of these combinations, with _Consensus ($r$, $s$)_ denoting the inference performed using the $r^{th}$ sample from $s$ different chains.
2. Bayesian inference using 10 chains running for 1 million iterations, saving every thousandth sample.
3. Mclust over a range of possible $K$.

Each method will be timed using the ``time`` bash function.

### Model evaluation


In this study we will be considering both within-simulation and across-simulation performance for the $N_{sim}$ simulations run under each scenario.

The aim of each model is uncovering the true clustering of the data, $C'$. The performance of each model may be judged by compaing the predicted clustering, $C^*$, to $C'$ using the adjusted rand index [**${ARI(C^*, C')}$**, @hubert1985comparing]. For Mclust, only the point estimate of the clustering is generated, and thus is trivial to compare. For Bayesian inference and consensus clustering we have a number of samples. These are used to construct a PSM/Consensus matrix from which a point estimate of the clustering is predicted using the ``maxpear`` function in the ``mcclust`` R package [@R-mcclust; theory described by @fritsch2009improved]. 
How well the inference quantifies uncertainty about the predicted clustering is also of interest. A model may have a low score under the ARI for its predicted clustering, but may have explored the partition space that included the true clustering. Ideally I would like to reward a model that explores sensible partitions even if the final partition is not ideal. To do this I compare the Frobenius norm of the difference of the coclustering matrix of the true clustering with the 

1. posterior similarity matrix for the Bayesian inference;
2. the coclustering matrix for Mclust; and
3. the consensus matrix for the consensus clustering. 

As Mclust only proposes a single partition its perfomance under this metric is a linear function of the ARI.

The Frobenius norm is a flawed proxy for uncertainty quantification. As a standalone statistic it is not sufficient as if no model is performing well the best performer under this metric will probably be that which is most sparse. It is important, particularly in the cases where the ARI is very low, to inspect the heatmaps of the PSMs and consensus matrices to understand what is happening. When the ARI is high and the Frobenius norm is low, it indicates that there is low uncertainty about the true clustering as one would hope. It is the other cases that demand more scrutiny. I use the Frobenius norm though as it does contain _some_ information about how well uncertainty is quantified and is a single number and thus a vector of number can be used, one from each simulation within the scenario. Furthermore, if there is a trend within the distribution of the Frobenius norms for each model than inspection of the PSMs and consensus matrices for a single simulation might be sufficient to understand the behaviour within the entire scenario in combination with the Frobenius norm.

For the Bayesian models I am also interested in within-chain model and across-chain convergence. I calculate the Geweke statistic [@geweke1992evaluating] and the Gelman-Rubin shrinkage parameter [$\hat{R}$, @gelman1992inference] as measures of these respectively.
If the Geweke statistic of a chain is not appreaing to be a normal-distribution, as tested by a Shapiro-Wilks test, I drop that chain from the analysis. I test if chains are not converged using the extension of $\hat{R}$ proposed by @vats2018revisiting, using a threshold of 1.01. I plot the original $\hat{R}$ value described by @gelman1992inference, but the summary statistic for the entire simulation is the Vats-Knudson statistic. Note that a low value in this is not sufficient to definitively say that a set of chains have converged, but a high value does indicate a lack of convergence.

The samples from the chains that pass the Shapiro-Wilks test are then pooled for the comparison beyond methods. I do not drop the entire analysis based upon overall convergence as doing so might lead some scenarios to having a very small number of data points for the Bayesian inference in the model comparison plots. Pooling samples is done to avoid model selection problems and to enable comparison across simulations. The pooling will mean that the Bayesian inference might be more robust to modes (as we might have several different modes present in the pooled samples), thus inflating performance under uncertainty quantification. However, the $\hat{R}$ statistic should indicate the percentage of simulations for which multiple modes are encountered as these chains should not be exploring the same distribution and thus will not be considered converged.

## Results

In a low dimensional setting the collapsed Gibbs sampler appears to quickly reaches the target distribution. This is based upon a comparison of consensus clustering results for different values of $R$. I assume that if the consensus matrices are both consistent with the PSMs and unchanging beyond a given value of $R$ that the chain has reached the staitonary distribution by said value of $R$ (albeit possibly a single mode of the target distribution). 

One can see that the for a constant number of samples, that the consensus clustering results stabilise for all $R \geq 10$ in the two-dimensional case. 

In considering both the PSMs and CMs, one can see that the large number of low magnitude values here suggests that many different possible partitions are being sampled. The fact that this occurs within each individual chain, as represented by the PSMs, and that there is no obvious difference between the different chains suggests that the sampler is exploring the full support of the distribution  within a single chain. 

The consensus clustering of short chains produces a similar distribution to the set of samples from a single long chain. No individual partition in any chain finds the true structure, but the distribution explored appears reasonable. In this setting it is the number of chains/samples, $S$, that is most important for improving performance.

As more relevant features are added the geometry of the likelihood surface becomes more complex. The sampler reaches a mode of the stationary distribution very quickly but appears to explore a decreasing number of possible partitions in any single chain; the attraction of each mode becomes greater, the distance between each mode increasing and the chain becomes more prone to finding a local mode. For example, in the PSMs from the small $N$, large $P$ case one can see that three different modes are present despite. However, consensus clustering can describe all of these modes. Again, a shallow chain depth, $R$, is sufficient and it is the number of chains that is important. 

As more irrelevant features are added the behaviour of the sampler changes. In this case the samples do not appear to be being drawn from the stationary distribution until much further into the chain. If one compares the consensus matrices for the scenario with 20 relevant features and 100 irrelevant feature, there is evidence of stabilising only by using $R=10,000$ (i.e. the bottom row of figure). This suggests that deeper chain depth is ideal for more noisy data. Despite this, consensus clustering can find the true structure using very short chains, although the uncertainty present is quite high. This is due to the different chains clustering different groups of items in different orders; the individual chains have not merged all of the possible clusters correctly and the partitions presented contain too many clusters in the early samples from the Markov chain. However, the chains do not appear to cluster items that are from different subpopulations in any sample. This means that the union of partitions from the short chains used in forming the consensus matrix does capture the true structure. Thus the predicted clustering matches the true clustering even if the level of uncertainty present is high. This shows that the consensus clustering does work even in the case that each individual learner is very weak; as long as each contains some signal the ensemble can perform very well. This is reassuring as in the case that the number of irrelevant features is high, one would ideally sample from deeper in the chain; that one may avoid this and still perform meaningful inference means that consensus clustering may be used in settings where a single iteration is very slow and thus many short chains are necessary. 




![As the number of features present increases the collapsed Gibbs sampler struggles to sample a wide range of partitions. If the clarity of the underlying structure is low, this can be pathologically poor (only a single partition, the initialisation, is sampled). These results are for a datasets of similar structrue to the small $N$, large $P$ case; i.e. $N=50$, $K=5$. There is a single dataset generated for each value of $P \in \[1, 500\]$ for each different value of $\Delta \mu$. ](./Images/Simulations/Exploration.png)


The combined results of the scenarios with high numbers of irrelevant features but some true structure and the scenario where no clustering signal is present is highly reassuring. These show that one can use consensus clustering on real, noisy data with the expectation that any structure uncovered is present in the data and not uncovering false structure within the noise.


As the number of dimensions increases the attraction of different modes becomes more extreme. It can be seen that different chains converge to a mode, but not all reach the global mode or explore any range of partitions regardless of the number of iterations. In this setting the benefits of the ensemble become clear as a number of modes can be represented in the analysis rather than the individual mode in each individual chain. However, if the distinction between the subpopulations is sufficiently small the sampler never explores any new partitions at all. The likelihood surface is very spiky due to the number of features (recall that the likelihood is an $(N \times P)$-polynomial for independent features). The results suggest that as the clarity of the structure weakens (i.e. no single mode is uniquely attractive) and the number of features scales, then the collapsed Gibbs sampler struggles more and more to explore any range of partitions. 

These results also show that one can include features in the analysis that do not necessarily contain structure, allowing one to be less rigorous or even avoid feature selection. However, it appears that in an 'omics scenario where there may be thousands or tens of thousands of possible feautres that feature selection is possibly an unavoidable part of an analysis pipeline.



![The time taken for each Markov chain to finish appears to scale linearly with the number of samples generated. This suggests that using shorter chains will reduce the computational time linearly; thus if the individual chains may be run in parallel significant time savings may be available. Assume one has 4 CPUs available to run chains upon and a single iteration takes $t$ seconds. Then based upon the relationship shown here, 10 chains of 10,000 iterations would take approximately $3 \times 10^4 t$ seconds and 100 chains of 10 iterations would take roughly $2.5 \times 10^2 t$ seconds, a difference of several orders of magnitude. ](./Images/Simulations/base_case/base_caseTimeComparisonGibbs.png)








## Conclusions

### Performance

I think that if uncertainty is ever of interst than Mclust has to be dropped. If this is not the case then Mclust is a powerful tool in the absence of irrelevant features.

If $P$ is small that Bayesian inference is ideal; however once $P$ breaks into the hundreds and the attraction of modes becomes more extreme I think one should consider consensus clustering as a natural way to represent these in one's analysis, or else consider other types of sampler. However, if $P$ is extreme these samplers might be sufficiently poor at exploring the partition space as to be infeasible regardless of the method of inference.

#### Small $P$

In a low dimensional, low signal setting Mclust performs excellently. In a real applicaiton where no labels are present and uncertainty quantification is important, than Bayesian inference performs very well, exploring the target distribution thoroughly. Based upon the behaviour of the consensus matrices and the improvement in ARI as a function of chain depth, $R$, it appears that one could use consensus clustering in a low dimensional setting as $R=10$ appears sufficient to perform similar inference to the Bayesian inference. Therefore one could use a parallel environment to decrease the time required to perform the analsysis. However, as true Bayesian inference can be performed quickly on this scale and is exploring a range of partitions, I would recommend using Bayesian inference, with all its guarantees in this setting.

#### False structure

It is reassuring to see that the methods all correctly identify no structure. This helps to reassure us that we can use these methods and will not identify noise as structure. The low degree of signal present in the case with $\sigma=5$ and in the small $N$ large $P$ scenario with close cluster means also saw no structure uncovered, suggesting that one may use these methods and be somewhat reassured that any signal uncovered is true. In the high dimensional setting one would have to recognise that the clustering in the PSMs corresponded to the initialisation; for practitioners who might be confused by this and the appearance of 10 modes in their 10 chains, perhaps consensus clustering, with it's very clear lack of structure, is preferable.

#### Loud signal

When signal is sufficiently loud that the clustering structure is easily visible within a heatmap, choice of method is less impotant.

#### Weak and hidden signal

As the signal becomes weaker sampler based methods perform better than Mclust, exploring a greater vairety of partitions and clearly stating that the uncertainty is high. If the signal is buried beneath a large amount of noise (rather than being inherently weak as in the large standard deviation cases), then the sampling-based methods perfom very well; these appear very robust to a large number of irrelevant features. However as the number of noise features increases sampling from deeper within the chain becomes more important for consensus clustering. I suspect that this is partially a function of the increase in the number of features independant of the ratio of relevant to irrelevant features.

#### Large $P$

In this setting I think consensus clustering shines. As the collapsed Gibbs sampler appears to find a sensible partition quite quickly (as seen by the stability of results for consensus clustering after $R$ exceeds 1) one can run the chains for a short length of time. Furthermore, one may use the lack of correlation of samples across chains to explore multiple modes. Bayesian inference struggles in this setting, becoming trapped in a mode and never escaping its attraction. It does identify useful structure, but claims of convergence become dubious and some of the main attractions of Bayesian inference, i.e. principled quantification of the uncertainty and the asymptotic guarantees, appear to not truly hold in practice.

### Using consensus clustering in practice

Consensus clustering has shown significant advantages. As $P$ grows and modes become more extreme, consensus clustering is better able to represent multiple modes than Bayesian inference (as seen in the irrelevant features 100 and the small $N$ large $P$ base case). However, if the sampler never explores a sensible space (as is the case in the scenario with small $N$ and large $P$ where the distance between cluster means, $\Delta \mu$, is small), than consensus clustering does not solve the problems. (Aside: I do think that the small $N$ large $P$ case does not contain strong signal of structure, and in this case finding no structure is an acceptable solution as the consensus clustering does.)

I think the performance of consensus clustering for even very small values of $R$ is surprising. In many scenarios setting $R=10$ was sufficient to uncover much of the structure or at least indicate it (as is in the case where $P_n=100$ where the consensus matrices corresponding to $R=10$ indicate the true structure with high uncertainty). I think that is exciting, as in cases where each iteration is slow (such as the large $P$ case) than consensus clustering performs well even if chains are severely constrained.

Based upon the results seen I would say that in practice Consensus ($R$, $S$) should be driven by the heuristics:

\begin{align}
 R &\propto P_n \\
 R &\propto P \\
 S &\propto \frac{\mu}{\sigma} \\
 S &\propto \textrm{The number of modes present.}
\end{align}

Of these, only $P$ can be confidently known. As this is the case I think making use of grids of the consensus matrices could help. They can provide evidence for when increasing $S$ and $R$ no longer offers significant gains. I think the Frobenius norm could be used to construct a stopping rule for these inputs of the inference, e.g. if the Frobenius norm of the difference of the matrix for Consensus $(R, S_1)$ and the matrix for Consensus $(R, S_2)$ (for some $S_1, S_2, R \in \mathbb{N}, S_2 > S_1$) is small than one should stop increasing $S$ and similarly for different values of $R$. One would have to pursued this far enough that one can believe that the stability of the consensus matrix is true. Based upon the results so far I think, letting $CM(r, s)$ denote a consensus matrix for consensus clustering using the $r^{th}$ iteration of $s$ chains, that if:

\begin{align}
&||CM(r, s) - CM(r, s + 10)||_F &< \epsilon \\
&||CM(r, s) - CM(r + 100, s)||_F &< \epsilon
\end{align}

for some small $\epsilon$ (which itself will be a function of $N$ as the Frobenius norm does not have an upper bound), then one should stop increasing the parameters of the inference. In practice  I think a visual inspection of the grid would be sufficient. Note that this appears to be asymptotic behaviour and increasing $S$ and $R$ to the largest values possible within one's computational constraints would not do any harm.

## Work remaining

You will notice the absence of the large $N$ small $P$ scenarios from this analysis. These scenarios have yet to be completed as the problem of performing inference upon the PSM/Consensus matrix of size $10,000 \times 10,000$ is proving non-trivial. Beyond these scenarios two additional cases worthy of investigation occur to me; 

1. an additional small $N$ large $P$ case where $\delta \mu$ is a value between that used in the two scenarios presented here; and
2. a varying proportions scenario where the overlap between clusters is less defined.

Of these, (1) would be inteded to test how much signal is needed in a high-dimensional setting before the collapsed Gibbs sampler explores the space at all. (2) would test how well the methods perform in a more realistic setting where the subpopulations are significantly different in size. In the current scenario of varying proportions, the signal is too clear and identifying the correct number of clusters is non-trivial (all methods perform well).

<!-- ## Clustering performance -->



<!-- Consider an item $x_i$ that truthfully has allocation label $c_i$. Now say that our similarity matrix has $x_i$ allocated correctly (i.e. with the other items that have allocation $c_i$) with a score of 0.4, but misallocated to some $c_j \neq c_i$ with a score of 0.6. In the predicted clustering calculated from our similarity matrix we will allocate $x_i$ to the wrong cluster and this will lessen the ARI between the truth and the predicted clustering. However, the model has been uncertain about $x_i$'s allocation. The Frobenius inner product will capture this uncertainty and (in this case) reward the model with a higher score. Thus the Frobenius product more accurately describes the model performance. -->

