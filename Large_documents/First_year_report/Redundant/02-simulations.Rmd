# Simulations {#simulations}

## Introduction

I want to create a simulation study to do two things:

1. absolute evaluation of consensus inference as a inferential technique for mixture models; and 
2. comparative evaluation of consensus, Bayesian and frequentist inference of mixture models.

I am interested in evaluating:

* Performance - how well each method predicts the true clustering (metric: *Adjusted Rand Index* [@hubert1985comparing] between the predicted clustering from each method and the true clustering);
* Robustness - how well each method explores the model space (metric: *Frobenius product* between the consensus / posterior similarity matrices and the true coclustering matrix); and
* Speed - how long each method takes to run (metric: *seconds*).

## Data generating mechanism

For this case I use the notation:

* $N$: the number of items generated;
* $P$: the number of relevant features used;
* $P_n$: the number of irrelevant features used;
* $K$: the true number of clusters present;
* $X = (x_1, \ldots, x_N)$: the items generated;
* $\pi=(\pi_1, \ldots, \pi_K)$: the expected proportions of the population belonging to each cluster;
* $c=(c_1, \ldots, c_N)$: the allocation variable for each item;
* $\theta=(\theta_1, \ldots, \theta_K)$: the parameters associated with each component; and
* $\phi=(\phi_1,\ldots, \phi_P)$: the indicator variable of feature relevance.

Our data generating model is a finite mixture model with independent features where irrelevant features have global parameters rather than component specific parameters:

\[
\begin{aligned}
p(x, z, \theta, \pi) &= \prod_{i=1}^N p(x_i | z_i, \theta_{z_i}) \prod_{i=1}^N p (z_i | \pi) p(\pi) p(\theta) \\
  &= \prod_{i=1}^N \prod_{p=1}^P p(x_{ip} | z_i, \theta_{z_ip})^{(1 - \phi_p)} p(x_{ip} | \theta_p) ^ {\phi_p} \prod_{i=1}^N p (z_i | \pi) p(\pi) p(\theta)
\end{aligned}
\]

During my simulations I assume that the model in question is a mixture of *Gaussians* and thus $\theta_{kp}=(\mu_{kp}, \sigma^2_{kp})$.

As each method of inference uses a common model, this data-generating mechanism is not expected to favour any one method over another.

I test seven different scenarios that change various parameters in this model. I will define the component parameters by $\Delta_{\mu}$, the distance between the possible means in each feature, and $\sigma^2$, a common standard deviation across all components and features. The scenarios tested are:

1. The 2D Gaussian case (this is a sense-check);
2. The lack-of-structure case in 2 dimensions;
3. The base case for which scenarios 4-6 are variations;
4. Increasing $\sigma^2$;
5. Increasing the number of irrelevant features;
6. Varying the expected proportion of the total population assigned to each sub-population;
7. The large $N$, small $P$ paradigm; and
8. The small $N$, large $P$ case.

A more detailed description of each scenario and various sub-scenarios is given in the below table.

```{r scenario_table, echo=F}
scn_table <- data.frame(
  Scenario = c("Simple 2D", "No structure", "Base Case", rep("Large N, small P", 3), rep("Large standard deviation", 3), rep("Irrelevant features", 5), rep("Small N, large P", 2), "Varying proportions"),
  N = c(100, 100, 2e2, 1e4, 1e4, 1e4, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 2e2, 50, 50, 200),
  P_s = c(2, 0, 20, 4, 4, 4, 20, 20, 20, 20, 20, 20, 20, 20, 500, 500, 20),
  P_n = c(0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 10, 20, 100, 200, 0, 0, 0),
  K = c(5, 1, 5, 5, 50, 50, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5),
  Delta_mu = c(3, 0, 1, 1, 1, 0.5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.2, 1),
  sigma2 = c(1, 1, 1, 1, 1, 1, 3, 5, 10, 1, 1, 1, 1, 1, 1, 1, 1),
  Pi = c(rep("vec(1/K)", 16), "(0.5, 0.25, 0.125, 0.0675, 0.0675)")
)

colnames(scn_table) <- c(
  "Scenario",
  "$N$",
  "$P_s$",
  "$P_n$",
  "$K$",
  "$\\Delta_{\\mu}$",
  "$\\sigma^2$",
  "$\\pi$"
)

sims_used <- scn_table[c(1:3, 7, 8, 11:13, 17, 4:6, 15:16), ] %>%
  set_rownames(1:nrow(.))

knitr::kable(sims_used, row.names = T, escape = F) # , "latex", longtable = T, booktabs = T, caption = "Longtable")
```

<!-- \begin{center} -->
<!-- \captionof{table}{Multirow Table.} -->
<!-- \begin{tabular}{l|l|r} -->
<!-- \textbf{Value 1} & \textbf{Value 2} & \textbf{Value 3} \\ -->
<!-- $\alpha$ & $\beta$ & $\gamma$ \\ \hline -->
<!-- \multirow{2}{*}{12} & 1110.1 & a \\ -->
<!--  & 10.1 & b \\ \hline -->
<!-- 3 & 23.113231 & c \\ -->
<!-- 4 & 25.113231 & d -->
<!-- \end{tabular} -->
<!-- \end{center} -->

### Scenario purposes

Each scenario is seen as testing certain cases or concepts.

#### Simple 2D
This case is seen as an introduction to method performance. In this case the data can be plotted as a scatter plot (whereas higher dimensions mean any two dimensional representation is missing some information about the data). This means that one may view the data and have an intuition if it is possible for a mixture model to cluster the data; furthermore the small dimsnaionality of the problem means that the distance between modes on the likelihood surface is less so we do not expect the Gibbs sampler to become trapped. This means that we can compare consensus inference to Bayesian inference with a strong belief that our model has converged (given appropriate tests). This means that we can process how well consensus inference performs with a deeper understanding of the context than might be possible in higher dimensional cases.

```{r, simple2dView, echo = F, cache = T}
scn <- "Simple 2D"
row_ind <- which(sims_used$Scenario == scn)
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}

# Generate dataset
simple2D_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(simple2D_gen$data), simple2D_gen$cluster_IDs,
  main = "Simple 2D (seed 1)",
  show_rownames = F,
  show_colnames = F
)

simple2D_gen$data %>%
  scale() %>%
  data.frame() %>%
  dplyr::mutate(Cluster = as.factor(simple2D_gen$cluster_IDs)) %>%
  ggplot(aes(x = Gene_1, y = Gene_2, colour = Cluster)) +
  geom_point() +
  scale_colour_viridis_d() +
  labs(
    title = "Example: Simple 2D case (seed 1)",
    x = "Gene 1",
    y = "Gene 2",
    subtitle = "Note that multiple clusters may overlap."
  )
```

#### No structure

In this case all items are drawn from the same normal distribution. It is meant to be a test to see how the inference methods perform when there is no structure to find. It may be considered an similar to attempting to quantify the false positive rate in a predictive method.

```{r, noStructureView, echo = F, cache = T}
scn <- "No structure"
row_ind <- which(sims_used$Scenario == scn)
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}

# Generate dataset
noStructure_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(noStructure_gen$data), noStructure_gen$cluster_IDs,
  main = "No structure (seed 1)",
  show_rownames = F,
  show_colnames = F
)

noStructure_gen$data %>%
  scale() %>%
  data.frame() %>%
  dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
  ggplot(aes(x = Noise_1, y = Noise_2, colour = Cluster)) +
  geom_point() +
  scale_colour_viridis_d() +
  labs(
    title = "Example: No structure case (seed 1)",
    x = "Gene 1",
    y = "Gene 2"
  )
```

#### Base case

This is the _base case_ which most of the other scenarios are some variation of. This is intended to be a benchmark against which performance may be judged. Each method is expected to perform well here across all metrics, predicting the true clustering correctly and with very little uncertainty.

```{r, baseCase, echo = F, cache = T}
scn <- "Base Case"
row_ind <- which(sims_used$Scenario == scn)[1]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
baseCase_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(baseCase_gen$data), baseCase_gen$cluster_IDs,
  main = paste0(scn, " (seed 1)"),
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
cluster_labels <- c(paste0("Cluster ", 1:K))
names(cluster_labels) <- 1:K
 

pc1 <- prcomp(baseCase_gen$data)$x
pcaSeriesPlot(x = pc1, labels = baseCase_gen$cluster_IDs, n_comp = min(P_s, 6)) +
  facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
  labs(
    title = paste0("PCA series for ", scn, " scenario") #,
    # subtitle = "Notice how the clusters separate across the components."
  )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

#### Large standard deviation

This is to test how distinct the clusters must be for the inference to perform well. _A priori_, I would think that Bayesian inference would perform optimally here. The reasonable number of features (20) means that the collapsed Gibbs sampler should explore the space well. The large variance within each cluster means that most of the signal is hidden and teh data might appear more as a continuum with no clustering structure, but I hope that the Bayesian inference will correctly idenitfy some structure but alos capture a large amount of uncertainty. I think consensus inference will perform quite well and do not have a strong intuition for how ``Mclust`` will perform. I think that the clever initialisation based upon ``hclust`` will help in unpicking much structure, but I suspect that the final clustering will be too certain.

```{r, largeStandardDeviation, echo = F, cache = T}
scn <- "Large standard deviation"
row_ind <- which(sims_used$Scenario == scn)[1]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
largeStadDev_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(largeStadDev_gen$data), largeStadDev_gen$cluster_IDs,
  main = "Large N, small P (seed 1)",
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
cluster_labels <- c(paste0("Cluster ", 1:K))
names(cluster_labels) <- 1:K
 

pc1 <- prcomp(largeStadDev_gen$data)$x
pcaSeriesPlot(x = pc1, labels = largeStadDev_gen$cluster_IDs, n_comp = min(P_s, 6)) +
  facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
  labs(
    title = "PCA series for large N, small P scenario",
    subtitle = "Notice how the clusters separate across the components."
  )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```




#### Large $N$, small $P$

This is to test the _large $N$, small $P$_ scenario. This is a difficult case to
analyse as visualising the result is difficult for a PSM (due to the $N \times N$ 
dimension of this), and obtaining a predicted clustering is also very slow.

```{r, largeNsmallP, echo = F, cache = T}
scn <- "Large N, small P"
row_ind <- which(sims_used$Scenario == scn)[1]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
largeNsmallP_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(largeNsmallP_gen$data), largeNsmallP_gen$cluster_IDs,
  main = "Large N, small P (seed 1)",
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
cluster_labels <- c(paste0("Cluster ", 1:K))
names(cluster_labels) <- 1:K
 

pc1 <- prcomp(largeNsmallP_gen$data)$x
pcaSeriesPlot(x = pc1, labels = largeNsmallP_gen$cluster_IDs, n_comp = min(P_s, 6)) +
  facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
  labs(
    title = "PCA series for large N, small P scenario",
    subtitle = "Notice how the clusters separate across the components."
  )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```


#### Small $N$, large $P$

This is to test the _small $N$, large $P$_ scenario so common in genetics. I 
think all methods will succeed in unpicking structure in the first case, but as
the distance between means decreases it will be interesting to see how each 
method performs. If the distance becomes too small and the resulting mixture of 
Gaussians effectively merge such that unpicking anyone cluster from its 
neighbours is not feasible, than this will be similar to the _no structure_ case.

```{r, smallNlargeP, echo = F, cache = T}
scn <- "Small N, large P"
row_ind <- which(sims_used$Scenario == scn)[2]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
smallNlargeP_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(smallNlargeP_gen$data), smallNlargeP_gen$cluster_IDs,
  main = "Small N, large P (seed 1)",
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
cluster_labels <- c(paste0("Cluster ", 1:K))
names(cluster_labels) <- 1:K
 

pc1 <- prcomp(smallNlargeP_gen$data)$x
pcaSeriesPlot(x = pc1, labels = smallNlargeP_gen$cluster_IDs, n_comp = min(P_s, 6)) +
  facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
  labs(
    title = "PCA series for small N, large P scenario",
    subtitle = "The clusters appear to be separable, despite the wide range of loadings within clusters."
  )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

#### Irrelevant features

This is to test how robust to noise the methods are. I expect that all methods 
will perform wll when the number of feautres containing signal is greater than 
those containing noise, but as the ratio flips and the number of irrelevant 
features dominates, I expect ``Mclust`` to struggle and I will be interested to
see how Bayesian and Consensus inference perform. This is of interest in the 
context of gene expression data where many irrelevant features are present; 
ideally our method will be robust to this, as it would mean that a practitioner
could use a less intense feature selection as removing all noisy features 
would not be a requisite to avoid obscuring subpopulation signal.

```{r, irrelevantFeatures, echo = F, cache = T}
scn <- "Irrelevant features"
row_ind <- which(sims_used$Scenario == scn)[3]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
irrelevantFeatures_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(irrelevantFeatures_gen$data), irrelevantFeatures_gen$cluster_IDs,
  main = paste0("Irrelevant features (seed 1, ", P_n, " irrelevant features)"),
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
cluster_labels <- c(paste0("Cluster ", 1:K))
names(cluster_labels) <- 1:K
 

pc1 <- prcomp(irrelevantFeatures_gen$data)$x
pcaSeriesPlot(x = pc1, labels = irrelevantFeatures_gen$cluster_IDs, n_comp = min(P_s, 6)) +
  facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
  labs(
    title = "PCA series for irrelevant features scenario",
    subtitle = "In this example one might expect clusters 2 and 3 to merge, with cluster 5 also\nin danger of being subsumed."
  )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```

#### Varying proportions

This simulation is designed to test how sensitive the methods are to relatively 
small clusters when large clusters are present.

```{r, varyingProportionsView, echo = F, cache = T}
scn <- "Varying proportions"
row_ind <- which(sims_used$Scenario == scn)[1]
N <- sims_used[["$N$"]][row_ind]
P_s <- sims_used[["$P_s$"]][row_ind]
P_n <- sims_used[["$P_n$"]][row_ind]
K <- sims_used[["$K$"]][row_ind]
dm <- sims_used[["$\\Delta_{\\mu}$"]][row_ind]
s2 <- sims_used[["$\\sigma^2$"]][row_ind]
pi <- sims_used[["$\\pi$"]][row_ind]

if (pi == "vec(1/K)") {
  pi <- rep(1 / K, K)
} else {
  pi <- "(0.5, 0.25, 0.125, 0.0675, 0.0675)" %>%
    stringr::str_remove_all("[() ]") %>%
    strsplit(",") %>%
    unlist() %>%
    as.numeric()
}


# Generate dataset
varyingProportions_gen <- generateSimulationDataset(K, N, P_s,
  delta_mu = dm,
  p_n = P_n,
  cluster_sd = s2,
  pi = pi
)


annotatedHeatmap(scale(varyingProportions_gen$data), varyingProportions_gen$cluster_IDs,
  main = paste0(scn, " (seed 1)"),
  show_rownames = F,
  show_colnames = F
)


# Set labels for facet wrapping
cluster_labels <- c(paste0("Cluster ", 1:K))
names(cluster_labels) <- 1:K
 

pc1 <- prcomp(varyingProportions_gen$data)$x
pcaSeriesPlot(x = pc1, labels = varyingProportions_gen$cluster_IDs, n_comp = min(P_s, 6)) +
  facet_wrap(~Cluster, labeller = labeller(Cluster = cluster_labels)) +
  labs(
    title = paste0("PCA series for ", scn, " scenario") #,
    # subtitle = "Notice how the clusters separate across the components."
  )

# noStructure_gen$data %>%
#   scale() %>%
#   data.frame() %>%
#   dplyr::mutate(Cluster = as.factor(noStructure_gen$cluster_IDs)) %>%
#   ggplot(aes(x = Gene_1, y = Gene_1, colour = Cluster)) +
#   geom_point() +
#   scale_colour_viridis_d() +
#   labs(
#     title = "Example: No structure case (seed 1)",
#     x = "Gene 1",
#     y = "Gene 2"
#   )
```


## Model evaluation

In this study we will be considering both within-simulation and across-simulation performance for the $N_{sim}$ simulations run under each scenario.

The aim of each model is uncovering the true clustering of the data, $C'$. The performance of each model may be judged by compaing the predicted clustering, $C^*$, to $C'$ using the adjusted rand index (**${ARI(C^*, C')}$**).

Robustness may be judged by the uncertainty on this estimate. I compare the Frobenius norm of of the difference between the coclustering matrix of the true clustering with the posterior similarity matrix (from the Bayesian inference) and the consensus matrix (from the Consensus inference). For the Bayesian models I also will record the Gelman-Rubin shrinkage parameter ($\hat{R}$) and the Geweke statistic as measures of model convergence.

Each model will be timed in milliseconds.

### Simple 2D

![Comparison of predicted clustering to true labelling across 100 simulations of the Simple 2D scenario. Consensus inference models are represented by _Consensus (chain iteration used, number of chains used)_. Mclust performs very well, unpicking the true structure. The Bayeisan inference also performs well, and we see that Consensus inference improves in performance as more samples are added, but the improvement yielded by increasing chain length stabilises after 10 iterations into the chain. This suggests that either 1) chains have reached their stationary distribution within 10 iterations and therefore sampling from the 10th or 10,000th iteration is the approximately same as the samples are all drawn from the same distribution or 2) chains stop exploring the space and become trapped in a mode after the 10th iteration.](./Images/Simulations/simple_2d/simple_2d_all_model_performance.png)


<!-- ## Clustering performance -->




<!-- Consider an item $x_i$ that truthfully has allocation label $c_i$. Now say that our similarity matrix has $x_i$ allocated correctly (i.e. with the other items that have allocation $c_i$) with a score of 0.4, but misallocated to some $c_j \neq c_i$ with a score of 0.6. In the predicted clustering calculated from our similarity matrix we will allocate $x_i$ to the wrong cluster and this will lessen the ARI between the truth and the predicted clustering. However, the model has been uncertain about $x_i$'s allocation. The Frobenius inner product will capture this uncertainty and (in this case) reward the model with a higher score. Thus the Frobenius product more accurately describes the model performance. -->


