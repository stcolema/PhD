---
title: "Consensus clustering for Bayesian model clustering"
author: "Stephen Coleman"
date: "`r Sys.Date()`"
output:
  bookdown::gitbook:
    lib_dir: "book_assets"
  bookdown::pdf_book:
    keep_tex: yes
documentclass: book
bibliography: [FYRbib.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "First year report for MRC Biostatistcs Unit PhD upgrade."
header-includes:
- \usepackage{caption}
- \usepackage{multirow}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{wrapfig}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
- \usepackage{mathtools}
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(tidyr)
library(plot3D)
library(plotly)
library(magrittr)
library(ggplot2)
library(tibble)
library(dplyr)
library(patchwork)
library(mergeTrees)
library(pheatmap)
library(coda)
library(mclust)
library(mcclust)
library(mergeTrees)
library(clusternomics)
library(bayesCC)
library(coca)
library(klic)
library(stableGR)
# library(r.jive)

# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), "bookdown", "knitr", "rmarkdown"
), "packages.bib")

library(mdiHelpR)
set.seed(1)
# devtools::install_github("sarawade/mcclust.ext")
# library(mcclust.ext)

# Function for converting surfaces for 3D plot to long data for ggplot2
surface2Df <- function(x, n_seeds, n_iter) {
  data.frame(x) %>%
    set_colnames(1:n_seeds) %>%
    add_column(N_iter = as.factor(1:n_iter)) %>%
    pivot_longer(1:n_seeds,
      names_to = "N_chains",
      values_to = "ARI",
      names_ptypes = list(N_chains = factor()),
      values_ptypes = list(ARI = numeric())
    )
}

setMyTheme()
col_pal <- dataColPal()
sim_col_pal <- simColPal()

fracTrue <- log(1:1e4) / log(1e4)
nReq <- length(fracTrue)

ggplot(data.frame(fracTrue = fracTrue, Iter = 1:1e4), aes(x = Iter, y = fracTrue)) +
  geom_line() +
  labs(
    title = "Fraction of items allocated correctly",
    caption = "Within generated clusterings, the fraction of items allocated correctly as a function of the ``chain length''. ",
    x = "Number of iterations",
    y = "Fraction of items correctly allocated"
  )

# Generate an example clustering
ciSim <- function(true_labels, n_iter, K,
                  truth_at = 1e4) {

  # The number of samples present
  n <- length(true_labels)

  # The number of samples clustered truly
  n_true <- floor(1 + n * fracTrue[min(floor(1 + nReq * n_iter / truth_at), nReq)])

  # The index of these samples
  true_ind <- sample(1:n, n_true, replace = F)

  # Random partition
  out <- sample(1:K, size = n, replace = T)

  # Set the truly clustered correctly
  out[true_ind] <- true_labels[true_ind]

  out
}
```

# Introduction {#introduction}

Mixture models are powerful cluster analysis tools,
offering insights into complex, heterogeneous datasets such as those found in
biology [@pan2002model; @medvedovic2004bayesian; @dahl2006model;
@gelman2013bayesian]. Bayesian mixture models may be extended to perform
integrative clustering, sharing information across multiple paired datasets
[see for example @kirk2012bayesian; @lock2013bayesian; @gabasova2017clusternomics],
making them suitable for application in a mutliple 'omics setting.

However, performing inference for these models is a difficult problem,
particularly in the high-dimensional, noisy setting so prevalent in 'omics
analyses. This is further exacerbated in the multiple dataset case.
Normally inference is based upon one of either the Maximum Likelihood Estimates
(**MLE**) of the variables or else Bayesian inference. These methods and their
implementations have various advantages and disadvantages, some of which are
described here. Some of the disadvantages are pathological in high dimensional
settings and limit the biomedical application of these models. I propose
aggregating many short MCMC chains to perform inference upon these models,
in a novel extension of Consensus clustering [@monti2003consensus]. I show
through extensive simulation studies that this approach marries many of the
strengths of the Bayesian models with improvements in speed and robustness.

<!-- Consensus clustering for Bayesian mixture models is an ensemble clustering  -->
<!-- method. Using this approach, I aim  -->
<!-- to reduce the computational cost of inference and provide better exploration of  -->
<!-- multiple modes. Consensus inference attempts to keep many of the strengths of  -->
<!-- using MCMC samplers (such as some uncertainty quantification, the ability to use -->
<!-- Bayesian models and hence the ability to infer the number of clusters present  -->
<!-- and application to an integrative setting) and marry these to the advantages of  -->
<!-- ensemble learning. To do this the asymptotic guarantees of Bayesian inference -->
<!-- are surrendered. I compare the performance of a common implementation for each -->
<!-- of the three inferential methods within a large simulation study.  -->


<!-- In this report, I briefly describe some of these and propose consensus inference  -->
<!-- as an alternative to overcome some of the problems that I consider particularly  -->
<!-- limiting while maintaining certain features that I consider attractive in the  -->
<!-- other methods. -->

<!-- Consensus inference is intended as a general, sampling-based ensemble method  -->
<!-- that can be used to perform inference upon Bayesian models. As a method it aims -->
<!-- to reduce the computational cost of inference and provide better exploration of  -->
<!-- multiple modes. Consensus inference attempts to keep many of the strengths of  -->
<!-- using MCMC samplers (such as some uncertainty quantification, the ability to use -->
<!-- Bayesian models and hence the ability to infer the number of clusters present  -->
<!-- and application to an integrative setting) and marry these to the advantages of  -->
<!-- ensemble learning. To do this the asymptotic guarantees of Bayesian inference -->
<!-- are surrendered. I compare the performance of a common implementation for each -->
<!-- of the three inferential methods within a large simulation study.  -->

Much of the work displayed here relies upon R [@R-base]; the exploration of
results [@R-dplyr; @R-magrittr; @R-tibble; @R-tidyr], data generation and
visualisation [@R-ggplot2; @R-patchwork; @R-pheatmap] and MCMC
diagnostics [@R-coda; @R-mcclust; @R-stableGR] as well as the software
used to generate this report [@R-bookdown; @R-knitr; @R-rmarkdown].

<!-- I propose Consensus inference as an alternative to Bayesian or Frequentist inference of model-based clustering methods. It requires that methods are sampling-based (such as using Markov-Chain Monte Carlo methods). Consensus inference is proposed as MCMC methods can be slow and often fail to converge in finite time in large clustering problems; in integrative clustering, which makes use of multiple datasets, this problem is particularly prevalent. There exists implementations of MCMC methods that attempt to overcome this problem of convergence in clustering (for instance split-merge moves), but these methods remain very slow to run. I propose Consensus inference as a general solution to these problems. In clustering problems, partially due to the discrete nature of clustering labels, Gibbs sampling normally is quick to find a local mode (often within 10's of iterations). However, the sampler then remains trapped there for a very large number of iterations. This means that there is no variation in samples after burn-in. As a result of this, taking only a single sample from the chain after it becomes trapped is equivalent to taking all of the samples in terms of the inference performed. Consensus inference uses this behaviour to both acquire impressive reductions in runtime, but also to attempt better exploration of possible clusterings. I propose running many short chains using many different random initialisations to try and explore the model space. Taking the first sample after burn-in from each of many chains and translating these into a _consensus matrix_ offers a more robust exploration of realistic clusterings possible in the data than using a single long chain. Furthermore, as the chains can be very short (often on the scale of 10's or 100's of iterations) and are independent of one another, one can take advantage of a parallel environment to drastically reduce runtime. I do not offer any guarantees that consensus inference is sampling the posterior space; rather than as an approximation of Bayesian inference this should be thought of as ensemble method combining many weak models into one more powerful, more robust model.  -->

<!-- I offer empirical evidence from simulations and real data from the Cancer Genome Atlas to show that Consensus inference does perform comparably to Bayesian and Frequentist inference in many scenarios, even outperforming these approaches in some. -->

### Notation {-}

I refer to objects being clustered as _items_. Each item has observations across some variables that I refer to as _features_. I use the language of @law2003feature and refer to *relevant* and *irrelevant* features referring to features that provide component specific information and thus contribute to the clustering and those that do not.

More generally if we are considering a multiple dataset context:

<!-- \newcommand{\myG}[1]{\textnormal{\fontencoding{OT1}\fontfamily{eiadtt}\selectfont {#1} \normalfont}} -->

\begin{eqnarray}
N & & \textrm{The number of items in each dataset.} \\
L & & \textrm{The number of datasets present.} \\
P & & \textrm{The number of features in the dataset.} \\
K & & \textrm{The number of components present in the dataset.} \\
R & & \textrm{The number of iterations for which a sampler is run in Bayesian inference or consensus clustering.} \\
S & & \textrm{The number of samples generated for Bayesian inference or consensus clustering.} \\
X &= (x_{1}, \ldots, x_{N}) & \textrm{The observed data.} \\
c &= (c_{1}, \ldots, c_{N}) & \textrm{The unobserved membership vector.} \\
\pi &= (\pi_{1,}\ldots, \pi_{K}) & \textrm{The mixture weights.} \\
\phi &=(\phi_1,\ldots, \phi_P) & \textrm{The indicator variable of feature relevance.}
\end{eqnarray}

<!-- \begin{eqnarray} -->
<!-- N & & \textrm{The number of items in each dataset.} \\ -->
<!-- L & & \textrm{The number of datasets present.} \\ -->
<!-- P_l & & \textrm{The number of features in the $lth$ dataset.} \\ -->
<!-- K_l & & \textrm{The number of components present in the $lth$ dataset.} \\ -->
<!-- R & & \textrm{The number of iterations for which a smapler is in Bayesian or consensus inference.} \\ -->
<!-- S & & \textrm{The number of samples generated from an MCMC smapler for Bayesian or consensus inference.} \\ -->
<!-- \mathcal{K} & & \textrm{The number of components present in the global context (if applicable).} \\ -->
<!-- X &= (X_1, \ldots, X_L) & \textrm{The $L$ datasets.} \\ -->
<!-- X_l &= (x_{l1}, \ldots, x_{lN}) & \textrm{The observed data for the $lth$ dataset.} \\ -->
<!-- % c &= (c_1, \ldots, c_L) & \textrm{ The membership vectors for each dataset (our latent variable).} \\ -->
<!-- c_l &= (c_{l1}, \ldots, c_{lN}) & \textrm{The membership vector in the $lth$ dataset.} \\ -->
<!-- \varsigma &= (\varsigma_1, \ldots, \varsigma_N) & \textrm{The global allocation vector (if applicable).} \\ -->
<!-- \pi_l &= (\pi_{l1,}\ldots, \pi_{lK_l}) & \textrm{The mixture weights in the $lth$ context.} \\ -->
<!-- \varpi &= (\varpi_1, \ldots, \varpi_G) & \textrm{The mixture weights for the global components (if present).}  -->
<!-- \end{eqnarray} -->

<!-- If $K_1 = \ldots = K_L$ then we use $K$ as the number of components in each  -->
<!-- context. We treat $P_l$ in the same way. If $L=1$ then any subscript used to distinguish datasets is ignored.  -->

I denote abbreviations or terms that  will be used in place of another in the format "[Full name] ([name hereafter])". For some variable $\theta=(\theta_1, \ldots, \theta_Q)$, I use $\theta_{-i}$ to denote the vector $(\theta_1, \ldots, \theta_{i-1}, \theta_{i+1}, \ldots, \theta_Q)$.

### Definitions {-}

A number of definitions are required for some recurring concepts in this report.

```{definition, allocMatrix, name = "Allocation matrix"}
Let $\mathbf{Z}$ be a $N \times K$ binary matrix, with

\[
z_{ik} = \begin{cases}
 1 \textrm{ if items $x_i$ is assigned to cluster $k$}, \\
 0 \textrm{ else.}
\end{cases}
\]

$\mathbf{Z}$ describes the same information as membership vector $c$ in a different format and is referred to as an _allocation matrix_.
```

Following from \@ref(def:allocMatrix), I use the following notation:

\[
\begin{aligned}
\sum_{k=1}^K z_{ik} &= 1 \textrm{ for all $i \in \{1, \ldots, N\}$ }\\
N_k &= \sum_{i=1}^N z_{ik}  \\
\mu_k &= \frac{1}{N_k} \sum_{i=1}^N z_{ik} x_i \textrm{ for all $k \in \{1, \ldots, K\}$}
\end{aligned}
\]

```{definition, coclustMatrix, name = "Coclustering matrix"}
Let $\mathbf{B}$ be a $N \times N$ binary matrix, with

\[
b_{ij} = \begin{cases}
 1 \textrm{ if items $x_i$ and $x_j$ are assigned to the same cluster}, \\
 0 \textrm{ else.}
\end{cases}
\]

$\mathbf{B}$ is referred to as the _coclustering matrix_.
```

### Similarity matrices {-}

Two closely related concepts are that of the posterior similarity matrix
(**PSM**) and the consensus matrix (**CM**). These are defined as follows:

```{definition, consensusMatrix, name = "Consensus matrix"}
A _consensus matrix_ (**CM**) is an $(N \times N)$ matrix that stores,
for each pair of items, the proportion of partitions in which the pair are
clustered together. The consensus matrix is obtained by taking the average over
the coclustering matrices obtained for partition.

Let $B^{(i)}$ be the coclustering matrix associated with the $i^{th}$ partition.
Then, for $I$ partitions, let the consensus matrix, $M$, be defined:

\begin{align}
M = \frac{1}{I}\sum_{i=1}^I B^{(i)}.
\end{align}

In other words, the $(i, j)^{th}$ entry of the CM is the proportion of partitions
for which the $i^{th}$ and $j^{th}$ items are allocated to the same cluster.
Thus, the consensus matrix is a symmetric matrix with all entries between 0 and
1 and diagonal entries of 1.

```



```{definition, psm, name = "Posterior similarity matrix"}
A _posterior similarity matrix_ (**PSM**) is an $(N \times N)$ matrix that stores, for
each pair of items, the proportion of paritions sampled from the Markov chain
for which the pair are clustered together. Thus one may consider the PSM as a
special case of the consensus matrix where all of the partitions are generated
from a single Markov chain.

```

Note that using the PSM and CM overcome the issues associated with the purely
symbolic nature of labels in a cluster analysis.


## Cluster analysis {#clusterAnalysis}

Cluster analysis – also known as unsupervised learning – consists of defining
partitions of the items within a dataset such that the resulting sets are
homogeneous and distinct from one another. These groups are known as clusters.
Clustering is used in multivariate statistics to uncover _useful_ latent groups
suspected in the data or to discover _interesting_ groups of homogeneous
observations. In large datasets, such as modern 'omics datasets, estimation of
clusters is often necessary for improved understanding and interpretation
[@meinshausen2010stability] - reducing a complex, $P$-dimensional dataset to
a 1-dimensional, discrete variable can significantly aid interpretation.

```{definition, clustering, name="Clustering"}
If one has some collection of data $X=\left(x_1,\ldots,x_N\right)$, let a
_clustering_ or partition, $Y$, of the data be defined by:

\begin{align}
	Y &= \left\{Y_1,\ldots,Y_K\right\} \\
	Y_k &= \left\{x_i : c_i = k \right\}  \\
	Y_i \cap Y_j &= \emptyset \hspace{4 pt} \forall \hspace{4 pt} i,j \in \{1,\ldots,K\},  i \neq j \\
	n_k & = \mid Y_k \mid \hspace{4 pt} \geq 1 \hspace{4 pt} \forall \hspace{4 pt} k \in \{1,\ldots,K\} \\
	\sum_{k=1}^Kn_k &= n
\end{align}

In short there are $K$ non-empty disjoint sets of data, each of which is
referred to as a _cluster_, the set of which form a _clustering_. This
clustering may be described by a collection of allocation labels,
$c=(c_1, \ldots, c_N)$. For an item $x_i$, the label $c_i=k$ states that
point $x_i$ is assigned to cluster $Y_k$.

```

<!-- Cluster analysis has a myriad of applications. Some traditional applications -->
<!-- include data mining,  -->
<!-- which started from the search for groupings of customers and products in massive -->
<!-- retail datasets [@fraley2002model]; document clustering and the analysis of  -->
<!-- internet use data; and image analysis, where clustering is used for image -->
<!-- segmentation and quantization. -->

Within clinical medicine and molecular biology where complex, heterogeneous
datasets are prevalent, cluster analysis and its extensions have provided great
benefit in many settings, ranging from general tools to enable analysis
[e.g. @crook2018bayesian; @dahl2006model] to uncovering novel biology in specific
diseases [e.g. cancer in @sorlie2001gene; @lapointe2004gene;
@cancer2012comprehensive; @berger2018comprehensive; coronary artery disease from
@guo2017cluster; tinnitus in @van2017cluster; and asthma in @ortega2014cluster].
Cluster analysis is also integral to the idea of precision medicine, which is
based upon "the classification of people into subpopulations using their common
genetic patterns, lifestyles, drug responses, and environmental and cultural
factors" [@gameiro2018precision].

<!-- My area of interest is in clustering 'omics data to improve understanding of  -->
<!-- disease aetiology or the underlying biology of a system. Ideally this helps  -->
<!-- clinical practice through identifying driving mechanisms of a disease or in -->
<!-- defining subtypes within given diseases.  -->

<!-- Defining subtypes based upon the 'omics profile rather than upon phenotypic  -->
<!-- patterns can enable more effective treatment as greater diversity may be  -->
<!-- observed. Examples where cluster analysis has been used to propose  -->
<!-- new subtypes include such a diverse range of diseases as breast cancer [@cancer2012comprehensive; @berger2018comprehensive], Coronary artery disease [@guo2017cluster], tinnitus [@van2017cluster] and asthma [@ortega2014cluster]. -->

The extension of cluster analysis to analyse multiple paired datasets
simultaneously in _integrative clustering_ has enabled deeper understanding of
more complex diseases [@hasin2017multi; @integrative2014integrative], for
example Parkinson's disease [@markello2020integrated] and cancer
[@wang2016integrative; @rappoport2018multi].

<!-- Another interesting application is the defining of subpopulations of patients; -->
<!-- this application is integral to _precision medicine_, which relies upon  -->
<!-- "the classification of people into subpopulations using their common genetic -->
<!-- patterns, lifestyles, drug responses, and environmental and cultural factors"  -->
<!-- [@gameiro2018precision]. -->


Within cluster analysis there exists many methods.
Traditional approaches [as taught in many textbooks, see for e.g.
@fidell2001using; @manly2016multivariate; @stevens2012applied] include
hierarchical and non-hierarchical (normally $k$-means) clustering. These
methods are heuristic in nature; they are not based upon a formal model that
explicitly models the data-generating process and what guidance there is for
solving important practical questions that arise in every cluster analysis (
such as the number of clusters present, the measure of distance / similarity
between points, which solution is optimal, etc.) is subjective and informal.
This has consequences for the interpretation of the results, as evaluation of
the significance of results is difficult [@monti2003consensus].

## Model-based clustering

An approach partially inspired by the heuristic nature of these clustering
methods is _model-based cluster analysis_ or _mixture models_. This approach
embeds the cluster analysis within a statistical framework and offers a
principled approach to many issues associated with cluster analysis. Each subpopulation is described
within the model by a probability distribution (hence the model is a mixture of
distributions). In this setting, the problem of determining the number of
clusters can be recast as statistical model choice problem, and
models that differ in numbers of components and/or in component distributions
can be consistently compared. Extensions accounting for outliers add one or
more components representing a different distribution for outlying data
[@fraley2002model; see an example in @crook2018bayesian].

```{definition, mixtureModels, name="Finite mixture models"}

If one is given some data $X = (x_1, \ldots, x_N)$, we assume $K$ unobserved
subpopulations generate the data and that insights into these sub-populations
can be revealed by imposing a clustering $Y = \left\{Y_1,\ldots,Y_K\right\}$ on
the data. It is assumed that each of the $K$ clusters can be modelled by a
parametric distribution, $f_k(\cdot)$ with parameters $\theta_k$. Normally a
common distribution, $f(\cdot)$, is assumed across all components. We let
membership in the $k^{th}$ cluster for the $i^{th}$ individual be denoted by
$c_i = k$. The full model density is then the weighted sum of the probability
density functions where the weights, $\pi_k$, are the proportion of the total
population assigned to the $k^{th}$ cluster. Then for item $x_i$ in a _finite
mixture model_:

\begin{align}
	p(x_i|c_i = k) &= \pi_k f(x_i | \theta_k) \\
	p(x_i) &= \sum_{k=1}^K \pi_k f(x_i | \theta_k)
\end{align}

```

The flexibility in choice of $f(\cdot)$ means that these models can be applied
in many different scenarios.

Inference of the parameters of these models is normally performed using the
Maximum Likelihood Estimates (**MLE**). Due to the complex
likelihood equations usually associated with these models, the closed form
solution is not possible [@stahl2012model]. For MLE, the
Expectation-Maximisation (**EM**) algorithm is used to find a solution.

<!-- ### Maximum likelihood estimate -->

MLE-based inference tends to be very quick but suffers from several problems.
It is:

1. difficult to combine multiple sources of information [see @singh2005combining
for an  example of how to consider this];
2. sensitive to initialisation; EM is prone to finding local maxima - to account
for this these models are run from multiple different initialisations. In this
case observing the same log‐likelihood values from multiple starting points
increases confidence that the solution is a global maximum. Using a some
heuristic clustering method to initialise the method [such as in @mclust2016]
also offers stability; and
3. prone to singularities; there are points [@euclid300elements] where the
likelihood becomes infinite, resulting in degenerate distributions.
Singularities occur when the number of parameters to be estimated is large in
relation to the sample size [@stahl2012model], models with unrestrained
covariances and large numbers of components are prone to this problem.
@fraley2007bayesian suggest using Bayesian inference as a means of overcoming
this issue, recommending use of a _maximum a posteriori_ (*MAP*) estimate
instead of an ML estimate.

<!-- ### Bayesian inference -->

One may use probabilistic, or Bayesian, mixture models. These are very powerful
methods that, among other things, offer:

1. Incorporation of prior beliefs into the inference; this also means that one
may apply Bayesian methods even when the sample size is small.
2. Inference of $K$ through use of the Dirichlet Process [@ferguson1973bayesian].
3. Extension to a multiple dataset setting [see @kirk2012bayesian;
@gabasova2017clusternomics; and @lock2013bayesian for
examples].

The normal inference of these models is through MCMC methods. Using these means
that:

1. One may avoid some of the issues in convergence. As one often is only
interested in a subset of variables (for example, the allocation variables,
$c_i$), one may integrate over the remaining nuisance parameters. This means
that one may avoid directly calculating awkward variables (such as covariances
matrices) that are prone to causing issues such as singularities in the
likelihood function (an example of such a solution is the
_collapsed Gibbs sampler_).
2. The asymptotic guarantees of MCMC methods mean that, in comparison to EM,
if one runs the algorithm for infinite length of time, one is guaranteed to
describe the full posterior distribution and not only a local maximum.

Alas not all is roses. Asymptotic guarantees are all very well, but ensuring
that a sampler has:

1. reached its stationary distribution;
2. explored the entire support of the target distribution; and
3. converged to its expectation

in practice is difficult [@robert2018accelerating]. For example,
the _Gibbs sampler_ [@geman1984stochastic], a common choice of sampler, can
struggle to achieve these three goals due to high correlation between samples
[@bishop2006pattern]. Integrating over nuisance parameters as in a
_collapsed Gibbs sampler_ [@liu1994collapsed] can improve rate of convergence
upon the expectation, but can reduce the exploration rate of the chain.
There exists many more flavours of MCMC that attack these 3 problems in
different ways [@robert2018accelerating].  Methods which attempt to improve
the ability of the sampler to explore the
support of the target distribution such as simulated annealing
[@kirkpatrick1983optimization; @atchade2011towards], incorporating
global-local moves [@holmes2017adaptive] or split-merge steps [@green2001modelling; @jain2004split];
all offer some degree of success, but at the cost of additional computational
complexity. For high dimensional data these remain slow to converge and may have
more limited success. Other methods attempt to accelerate the speed at which the
sampler reaches the target distribution, e.g. utilising stochastic optimisation
[@welling2011bayesian] or to attempt and leverage parallel chains
[@ahmed2012scalable; however there is controversy over convergence in parallel
methods, @robert2018accelerating].

In short, different MCMC methods tend to trade off different weaknesses to be
applicable in different contexts, but there are no general, solve-all solutions
to the 3 problems listed by @robert2018accelerating.

An alternative means of performing Bayesian inference, VI is an optimisation
method more like EM. This proposes a family
of densities, $q(\theta)$, and then tries to find the member of this family that
is closest to the target density, where closeness is measured by the
Kullback–Leibler divergence  [@blei2017variational]. VI is intended as a fast
alternative to MCMC when traditional MCMC methods are not computationally
tractable. However, it does have several disadvantages:

1. VI underestimates the variance [@bishop2006pattern];
2. VI is prone to becoming stuck in local minima, thus several restarts are
required (as with EM);
3. VI does not offer theoretical guarantees (unlike MCMC) [@blei2017variational];
and
4. deriving the correct equations to apply VI in a specific situation is non-trivial.

A common theme across all these methods is that they can struggle to converge
to the global mode. If one is interested in capturing uncertainty and describing
multiple modes, than MCMC appears to be the best option, but it can struggle
to explore the full distribution. MCMC methods which overcome this issue can be
very slow.


<!-- ### Consensus inference -->

<!-- Consensus inference is an ensemble method such as _random forest_  -->
<!-- [@breiman2001random], and aggregates many (presumably) weak learners to improve  -->
<!-- ability to explore multiple modes and improve the model's ability to uncover  -->
<!-- structure. It follows a similar logic to  -->
<!-- _consensus clustering_ [@monti2003consensus], allowing many models with  -->
<!-- different random initialisations to contribute equally weighted  -->
<!-- votes on the clustering. In contrast to consensus clustering, consensus  -->
<!-- inference does not require model convergence. Furthermore, consensus clustering -->
<!-- is intended as a method to choose the number of clusters present based upon the -->
<!-- stability of clusters for different choice of $K$, whereas consensus inference  -->
<!-- infers $K$, and thus this uncertainty about $K$ is represented in the final  -->
<!-- consensus matrix. More precisely I define consensus inference as follows for -->
<!-- this report: -->

<!-- ```{definition, consensusInference, name="Consensus inference"} -->
<!-- A general method for performing inference upon clustering models using MCMC  -->
<!-- methods. Consensus inference runs $S$ independent chains of a collapsed Gibbs -->
<!-- sampler run for $R$ iterations saving the sample from the $R^{th}$ iteration.  -->
<!-- That is, from the $s^{th}$ chain a single value for each variable is recorded, -->
<!-- $\theta^{(s)}=(\theta_1^{(s)}, \ldots, \theta_Q^{(s)})$. -->

<!-- The samples are then compiled into a $S \times Q$ matrix (where the $(s, q)^{th}$  -->
<!-- entry is the sampled value for the $q^{th}$ parameter in the $s^{th}$ chain). -->

<!-- Inference is performed similarly to the case of a Bayesian inference using  -->
<!-- sampling methods, but the interpretation of results differs in that the  -->
<!-- uncertainty measured is not necessarily the posterior variance and we do not -->
<!-- assume that we are sampling from the posterior distribution. -->
<!-- ``` -->


<!-- As consensus inference uses MCMC methods, it may leverage some of the strengths  -->
<!-- of Bayesian inference (e.g. use of a prior, inference of $K$ through use of a  -->
<!-- _Dirichlet-Multinomial Allocation_ (**DMA**) mixture model [@green2001modelling; -->
<!-- see @savage2013identifying for an example], suitability for an  -->
<!-- integrative setting, and some quantification of uncertainty), but does not offer -->
<!-- the asymptotic guarantees of Bayesian inference.  -->

<!-- As each chain is independent of all others, this means that consensus inference  -->
<!-- can use a parallel environment to reduce computational time and improve  -->
<!-- scalability.  -->

<!-- Roughly summarising: -->

<!-- ```{r, my_table, echo = F} -->

<!-- col_names <- c("MLE", "Bayesian", "Consensus") -->
<!-- row_names <- c("Singularities in the likelihood",  -->
<!--   "Can infer $K$", -->
<!--   "Can use prior knowledge", -->
<!--   "Quantifies uncertainty", -->
<!--   "Robust to local maxima", -->
<!--   "Speed" -->
<!--   ) -->

<!-- scores <- c( -->
<!--   T, F, F, -->
<!--   F, T, T, -->
<!--   F, T, T, -->
<!--   F, T, T, -->
<!--   F, F, T, -->
<!--   "Fast", "Slow", "Fast" -->
<!-- ) %>%  -->
<!--   matrix(byrow = T, ncol = 3) %>% -->
<!--   data.frame() %>%  -->
<!--   set_rownames(row_names) %>%  -->
<!--   set_colnames(col_names) -->

<!-- knitr::kable(scores, row.names = T, escape = F) -->


<!-- ``` -->

## Ensemble methods

<!-- I use the same definition of an _ensemble_ as @re2012ensemble.  -->

```{definition, ensemble, name = "Ensemble"}
A set of learning machines that work together to find some structure within data,
whether this be a supervised problem such as _classification_ or an unsupervised
problem such as cluster analysis.

```


<!-- There exists many other terms referring to the same basic concept, e.g. -->
<!-- a fusion, combination, aggregation, committee of learners, but I will use the  -->
<!-- term ensemble throughout this report. -->

The concept of an ensemble of learners and its improvements is an old one, and
not limited to machincal learners [@condorcet1785essay]. Much of the theoretical
underpinings of ensemble learning has focused upon resampling-based methods
and classification [@friedman2007bagging; @breiman1996bias; @schapire1998boosting],
in keeping with the  majority of algorithms, of which _Random forest_
[@breiman2001random] is probably the most famous. There have been efforts to
provide a more general mathematical basis for ensembles; Eugene Kleinberg
deserves special mention for achievements based upon set theory and
combinatorics [@kleinberg1977infinitary; @kleinberg1990stochastic;
@kleinberg1996overtraining; @kleinberg2000mathematically]. More pertinently for
the practitioner, perhaps, these methods have also displayed great empricial
success, outperforming state-of-the-art methods in both simulations and
benchmark datasets [@breiman2001random; @monti2003consensus;
@sohn2007experimental; @afolabi2018ensemble]. Attractions of ensemble methods
include their ability to explore multiple modes [@ghaemi2011review]. Most
ensemble methods also enable use of a parallel enivronment to improve
computation speed [@ghaemi2009survey].

For a more thorough review of ensemble methods with a focus on classification,
please see @re2012ensemble.

@monti2003consensus proposed a general frameworks for ensembles of clusterers,
"Consensus clustering". It is an model-independent, sampling-based method, that
attempts to provide a rule-based approach to choosing $K$ in the model and to
improve cluster stability. The individual-level algorithm investigated in the
original publication and implemented in the ``ConsensusClusterPlus`` R package
[@wilkerson2010consensusclusterplus] is $k$-means clustering. This flavour of
Consensus clustering has been successfully used in cancer subtyping
[@verhaak2010integrated; @marisa2013gene].

I propose extending Consensus clustering to Bayesian models. As listed
previously, Bayesian mixture models have many attractive features such as:

1. allowing inference of $K$;
2. allowing integration of prior information; and
3. extending naturally to the multiple dataset scenario.

However, MCMC methods, the workhorse of Bayesian inference, have pathological
behaviour in the context of modern biomedical data which is often of sufficient
dimensionality that the sampler is both very slow to perform a single iteration,
and can become trapped within a single mode for any realistic length of run time
[@ronan2016avoiding; @chandra2020bayesian; @robert2018accelerating].
However, MCMC samplers are often adept at finding _a_ mode, therefore a small
number of iterations (relative to the number required for convergence) is often
sufficient to find a mode. Using many short chains each with a random
initialisation, one might believe that one will explore more modes that are
present in the data than would be investigated in any single chain. One may use
this approach to perform inference upon a Bayesian clustering model in contexts
where this is normally difficult or even impossible, although the inference
itself is not Bayesian. Thus one may marry the attractions of Bayesian _models_
with those of the ensemble.

<!-- can be slow to  -->
<!-- explore the target distribution. In practice, particularly for complex,  -->
<!-- high-dimensional data, ensuring that the sampler has -->

<!-- 1. reached its stationary distribution; -->
<!-- 2. explored the full support of the posterior space; and -->
<!-- 3. converged to the expectation -->

<!-- is very difficult [@robert2018accelerating]. -->

<!-- Bayesian clustering models are therefore attractive, but  -->


This does sacrifice certain qualities about MCMC methods that are attractive.

* there are no aymptotic guarantees here;
* the uncertainty described across the partitions is not the same as the
uncertainty described by Bayesian inference; and
* one cannot claim that the distribution of values generated for each variable
coincides with the posterior distribution.

In cluster analysis generally, but even more so in the context of biomedical
data and human health, any result has to be validated based upon knowledge of
the domain. This means that aymptotic guarantees are not sufficient to validate
a final partition; the defence and interpretation based upon the data is more
important. Thus I argue that if consensus clustering of Bayesian mixture models
produces sensible, interpretable results, than the loss of the asymptotic
guarantees is insufficient reason to prevent use of this approach, particularly
as the asymptotic guarantees do not appear to hold in practice.

There are three outstanding issues for any ensemble of clusterers
[@topchy2003combining] that need to be considered.

1. Consensus: How to combine different clusterings?
2. Diversity: How to generate different partitions?
3. Strength: How "weak" can each input partition be?

One may realise that many of the issues here are also present for sampling-based
inference of Bayesian mixture models. In this case, one could consider the MCMC
sampler as a special case of an clustering ensemble, being a set of many
partitions. In this case the problems of diversity and strength are mitigated
somewhat by mathematical properties of the Markov chain, but as the preceding
list of problems for MCMC makes clear, these questions are not answered as
satisfactorily as one might hope.

For the problem of finding a consensus between partitions, as the labels
generated for each partition are symbolic, combining clusterings
can be a difficult problem [@strehl2002cluster]. Aggregating partitions into a 
PSM or CM [as recommended by @monti2003consensus] offers a representation of the
partitions that overcomes this label-swapping problem. A point clustering may
then be estimated from these matrices using one of the methods listed by
@fritsch2009improved.

The source of diversity in my approach is the use random initialisation of the
sampler. In my simulations I show that a single MCMC chain can struggle to
produce a diverse range of partitions and that using many different
initialisations helps to overcome this problem.

I will investigate the strength of each individual in my simulation study,
showing emprical results for both the consensus and individual learners.
There already exist results in the literature that the ratio of the diversity
of partitions to the strength is more important that the strength alone,
with the caveat that each individual performs better than random
[@breiman2001random]. Prior expectation for a consensus of
Bayesian mixture models would be that within a single iteration
each Markov chain would be producing samples that are better than random as the
sampler proposes a more probable state.
