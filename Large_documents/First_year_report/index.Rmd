---
title: "Consensus clustering for Bayesian model-based clustering"
author: "Stephen Coleman"
date: "`r Sys.Date()`"
output:
  bookdown::gitbook:
    lib_dir: "book_assets"
  bookdown::pdf_book:
    keep_tex: yes
documentclass: book
bibliography: [FYRbib.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "First year report for MRC Biostatistcs Unit PhD upgrade."
header-includes:
- \usepackage{caption}
- \usepackage{multirow}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{wrapfig}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
- \usepackage{mathtools}
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(tidyr)
library(plot3D)
library(plotly)
library(magrittr)
library(ggplot2)
library(tibble)
library(dplyr)
library(patchwork)
library(mergeTrees)
library(pheatmap)
library(coda)
library(mclust)
library(mcclust)
library(mergeTrees)
library(clusternomics)
library(bayesCC)
library(coca)
library(klic)
library(stableGR)
# library(r.jive)

# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), "bookdown", "knitr", "rmarkdown"
), "packages.bib")

library(mdiHelpR)
set.seed(1)
# devtools::install_github("sarawade/mcclust.ext")
# library(mcclust.ext)

# Function for converting surfaces for 3D plot to long data for ggplot2
surface2Df <- function(x, n_seeds, n_iter) {
  data.frame(x) %>%
    set_colnames(1:n_seeds) %>%
    add_column(N_iter = as.factor(1:n_iter)) %>%
    pivot_longer(1:n_seeds,
      names_to = "N_chains",
      values_to = "ARI",
      names_ptypes = list(N_chains = factor()),
      values_ptypes = list(ARI = numeric())
    )
}

setMyTheme()
col_pal <- dataColPal()
sim_col_pal <- simColPal()

fracTrue <- log(1:1e4) / log(1e4)
nReq <- length(fracTrue)

ggplot(data.frame(fracTrue = fracTrue, Iter = 1:1e4), aes(x = Iter, y = fracTrue)) +
  geom_line() +
  labs(
    title = "Fraction of items allocated correctly",
    caption = "Within generated clusterings, the fraction of items allocated correctly as a function of the ``chain length''. ",
    x = "Number of iterations",
    y = "Fraction of items correctly allocated"
  )

# Generate an example clustering
ciSim <- function(true_labels, n_iter, K,
                  truth_at = 1e4) {

  # The number of samples present
  n <- length(true_labels)

  # The number of samples clustered truly
  n_true <- floor(1 + n * fracTrue[min(floor(1 + nReq * n_iter / truth_at), nReq)])

  # The index of these samples
  true_ind <- sample(1:n, n_true, replace = F)

  # Random partition
  out <- sample(1:K, size = n, replace = T)

  # Set the truly clustered correctly
  out[true_ind] <- true_labels[true_ind]

  out
}
```

# Introduction {#introduction}

<!-- Cluster analysis can answer and inspire biomedical questions. For example, clustering of -->

<!-- * patients based upon disease characteristics (such as the ‘omics profile) and reactions to treatment can be used to define disease subtypes for more specific treatments. Improving understanding of common and unique mechanisms of diseases at the molecular level can enable development of treatments and deepen understanding of disease aetiology. -->
<!-- * patients based upon their ‘omics profile and environmental and lifestyle factors, “precision medicine”, enables bespoke treatment that attempts to improve the patient outcome. -->
<!-- *	genes based upon their co-expression can improve our understanding of cellular biology and gene pathways; such holistic understanding of human biology can enable targeted drugs and offer insights into diseases. -->

<!-- Bayesian mixture models are a popular statistical method for performing cluster analysis. However, the computational methods for applying these have certain limitations. MCMC methods, the workhorse of Bayesian inference, struggle to scale to the high dimension of modern biomedical data. Two prevalent issues in high dimensions are that these methods -->

<!-- *	can struggle to explore the full target distribution, becoming trapped in a single, possibly local, mode. -->
<!-- * become very computationally costly, even a single iteration can be very slow to perform. -->

<!-- Due to the attractions of Bayesian models, we would like to find some computationally tractable method for using them in a biomedical context. I propose a novel extension of the Consensus clustering framework, a general clustering ensemble approach, to Bayesian mixture models to alleviate these issues. In this report I attempt to explain the attraction of mixture models and specifically Bayesian mixture models and their extensions; the limits of current methods for performing inference in mixture models and how I implement consensus clustering in this setting. I then describe results of a simulation study that compares established methods of inference to consensus clustering. I finish with some possible future applications and extensions of the method. -->


<!-- Mixture models are powerful cluster analysis tools, -->
<!-- offering insights into complex, heterogeneous datasets such as those found in -->
<!-- biology [@pan2002model; @medvedovic2004bayesian; @dahl2006model; -->
<!-- @gelman2013bayesian]. Bayesian mixture models may be extended to perform -->
<!-- integrative clustering, sharing information across multiple paired datasets -->
<!-- [see for example @kirk2012bayesian; @lock2013bayesian; @gabasova2017clusternomics], -->
<!-- making them suitable for application in a mutliple 'omics setting. -->

<!-- However, performing inference for these models is a difficult problem, -->
<!-- particularly in the high-dimensional, noisy setting so prevalent in 'omics -->
<!-- analyses. This is further exacerbated in the multiple dataset case. -->
<!-- Normally inference is based upon one of either the Maximum Likelihood Estimates -->
<!-- (**MLE**) of the variables or else Bayesian inference. These methods and their -->
<!-- implementations have various advantages and disadvantages, some of which are -->
<!-- described here. Some of the disadvantages are pathological in high dimensional -->
<!-- settings and limit the biomedical application of these models. I propose -->
<!-- aggregating many short MCMC chains to perform inference upon these models, -->
<!-- in a novel extension of Consensus clustering [@monti2003consensus]. I show -->
<!-- through extensive simulation studies that this approach marries many of the -->
<!-- strengths of the Bayesian models with improvements in speed and robustness. -->

<!-- Consensus clustering for Bayesian mixture models is an ensemble clustering  -->
<!-- method. Using this approach, I aim  -->
<!-- to reduce the computational cost of inference and provide better exploration of  -->
<!-- multiple modes. Consensus inference attempts to keep many of the strengths of  -->
<!-- using MCMC samplers (such as some uncertainty quantification, the ability to use -->
<!-- Bayesian models and hence the ability to infer the number of clusters present  -->
<!-- and application to an integrative setting) and marry these to the advantages of  -->
<!-- ensemble learning. To do this the asymptotic guarantees of Bayesian inference -->
<!-- are surrendered. I compare the performance of a common implementation for each -->
<!-- of the three inferential methods within a large simulation study.  -->


<!-- In this report, I briefly describe some of these and propose consensus inference  -->
<!-- as an alternative to overcome some of the problems that I consider particularly  -->
<!-- limiting while maintaining certain features that I consider attractive in the  -->
<!-- other methods. -->

<!-- Consensus inference is intended as a general, sampling-based ensemble method  -->
<!-- that can be used to perform inference upon Bayesian models. As a method it aims -->
<!-- to reduce the computational cost of inference and provide better exploration of  -->
<!-- multiple modes. Consensus inference attempts to keep many of the strengths of  -->
<!-- using MCMC samplers (such as some uncertainty quantification, the ability to use -->
<!-- Bayesian models and hence the ability to infer the number of clusters present  -->
<!-- and application to an integrative setting) and marry these to the advantages of  -->
<!-- ensemble learning. To do this the asymptotic guarantees of Bayesian inference -->
<!-- are surrendered. I compare the performance of a common implementation for each -->
<!-- of the three inferential methods within a large simulation study.  -->

Much of the work displayed here relies upon R [@R-base]; the exploration of
results [@R-dplyr; @R-magrittr; @R-tibble; @R-tidyr], data generation and
visualisation [@R-ggplot2; @R-patchwork; @R-pheatmap] and MCMC
diagnostics [@R-coda; @R-mcclust; @R-stableGR] as well as the software
used to generate this report [@R-bookdown; @R-knitr; @R-rmarkdown].

<!-- I propose Consensus inference as an alternative to Bayesian or Frequentist inference of model-based clustering methods. It requires that methods are sampling-based (such as using Markov-Chain Monte Carlo methods). Consensus inference is proposed as MCMC methods can be slow and often fail to converge in finite time in large clustering problems; in integrative clustering, which makes use of multiple datasets, this problem is particularly prevalent. There exists implementations of MCMC methods that attempt to overcome this problem of convergence in clustering (for instance split-merge moves), but these methods remain very slow to run. I propose Consensus inference as a general solution to these problems. In clustering problems, partially due to the discrete nature of clustering labels, Gibbs sampling normally is quick to find a local mode (often within 10's of iterations). However, the sampler then remains trapped there for a very large number of iterations. This means that there is no variation in samples after burn-in. As a result of this, taking only a single sample from the chain after it becomes trapped is equivalent to taking all of the samples in terms of the inference performed. Consensus inference uses this behaviour to both acquire impressive reductions in runtime, but also to attempt better exploration of possible clusterings. I propose running many short chains using many different random initialisations to try and explore the model space. Taking the first sample after burn-in from each of many chains and translating these into a _consensus matrix_ offers a more robust exploration of realistic clusterings possible in the data than using a single long chain. Furthermore, as the chains can be very short (often on the scale of 10's or 100's of iterations) and are independent of one another, one can take advantage of a parallel environment to drastically reduce runtime. I do not offer any guarantees that consensus inference is sampling the posterior space; rather than as an approximation of Bayesian inference this should be thought of as ensemble method combining many weak models into one more powerful, more robust model.  -->

<!-- I offer empirical evidence from simulations and real data from the Cancer Genome Atlas to show that Consensus inference does perform comparably to Bayesian and Frequentist inference in many scenarios, even outperforming these approaches in some. -->

### Notation {-}

I refer to objects being clustered as _items_. Each item has observations across some variables that I refer to as _features_. I use the language of @law2003feature of *relevant* and *irrelevant* features referring to features that provide component specific information and thus contribute to the clustering and those that do not.

More generally:

<!-- \newcommand{\myG}[1]{\textnormal{\fontencoding{OT1}\fontfamily{eiadtt}\selectfont {#1} \normalfont}} -->

\begin{eqnarray}
N & & \textrm{The number of items in a dataset.} \\
 % L & & \textrm{The number of datasets present.} \\ 
P & & \textrm{The number of features in a dataset.} \\
K & & \textrm{The number of components present in a dataset.} \\
R & & \textrm{The number of iterations for which a sampler is run.} \\
S & & \textrm{The number of samples used to perform inference.} \\
X &= (x_{1}, \ldots, x_{N}) & \textrm{The observed data.} \\
c &= (c_{1}, \ldots, c_{N}) & \textrm{The unobserved membership vector.} \\
\pi &= (\pi_{1,}\ldots, \pi_{K}) & \textrm{The mixture weights.} \\
\phi &=(\phi_1,\ldots, \phi_P) & \textrm{The indicator variable of feature relevance.} \\
\theta &= (\theta_1, \ldots, \theta_Q) & \textrm{The generic notation for unobserved variables.}
\end{eqnarray}

<!-- \begin{eqnarray} -->
<!-- N & & \textrm{The number of items in each dataset.} \\ -->
<!-- L & & \textrm{The number of datasets present.} \\ -->
<!-- P_l & & \textrm{The number of features in the $lth$ dataset.} \\ -->
<!-- K_l & & \textrm{The number of components present in the $lth$ dataset.} \\ -->
<!-- R & & \textrm{The number of iterations for which a smapler is in Bayesian or consensus inference.} \\ -->
<!-- S & & \textrm{The number of samples generated from an MCMC smapler for Bayesian or consensus inference.} \\ -->
<!-- \mathcal{K} & & \textrm{The number of components present in the global context (if applicable).} \\ -->
<!-- X &= (X_1, \ldots, X_L) & \textrm{The $L$ datasets.} \\ -->
<!-- X_l &= (x_{l1}, \ldots, x_{lN}) & \textrm{The observed data for the $lth$ dataset.} \\ -->
<!-- % c &= (c_1, \ldots, c_L) & \textrm{ The membership vectors for each dataset (our latent variable).} \\ -->
<!-- c_l &= (c_{l1}, \ldots, c_{lN}) & \textrm{The membership vector in the $lth$ dataset.} \\ -->
<!-- \varsigma &= (\varsigma_1, \ldots, \varsigma_N) & \textrm{The global allocation vector (if applicable).} \\ -->
<!-- \pi_l &= (\pi_{l1,}\ldots, \pi_{lK_l}) & \textrm{The mixture weights in the $lth$ context.} \\ -->
<!-- \varpi &= (\varpi_1, \ldots, \varpi_G) & \textrm{The mixture weights for the global components (if present).}  -->
<!-- \end{eqnarray} -->

<!-- If $K_1 = \ldots = K_L$ then we use $K$ as the number of components in each  -->
<!-- context. We treat $P_l$ in the same way. If $L=1$ then any subscript used to distinguish datasets is ignored.  -->

I denote abbreviations or terms that  will be used in place of another in the format "[Full name] ([name hereafter])". For some variable $\theta=(\theta_1, \ldots, \theta_Q)$, I use $\theta_{-i}$ to denote the vector $(\theta_1, \ldots, \theta_{i-1}, \theta_{i+1}, \ldots, \theta_Q)$.

<!-- ### Definitions {-} -->

<!-- A number of definitions are required for some recurring concepts in this report. -->

<!-- ```{definition, allocMatrix, name = "Allocation matrix"} -->
<!-- Let $\mathbf{Z}$ be a $N \times K$ binary matrix, with -->

<!-- \[ -->
<!-- z_{ik} = \begin{cases} -->
<!--  1 \textrm{ if items $x_i$ is assigned to cluster $k$}, \\ -->
<!--  0 \textrm{ else.} -->
<!-- \end{cases} -->
<!-- \] -->

<!-- $\mathbf{Z}$ describes the same information as membership vector $c$ in a different format and is referred to as an _allocation matrix_. -->
<!-- ``` -->

<!-- Following from \@ref(def:allocMatrix), I use the following notation: -->

<!-- \[ -->
<!-- \begin{aligned} -->
<!-- \sum_{k=1}^K z_{ik} &= 1 \textrm{ for all $i \in \{1, \ldots, N\}$ }\\ -->
<!-- N_k &= \sum_{i=1}^N z_{ik}  \\ -->
<!-- \mu_k &= \frac{1}{N_k} \sum_{i=1}^N z_{ik} x_i \textrm{ for all $k \in \{1, \ldots, K\}$} -->
<!-- \end{aligned} -->
<!-- \] -->

<!-- ```{definition, coclustMatrix, name = "Coclustering matrix"} -->
<!-- Let $\mathbf{B}$ be a $N \times N$ binary matrix, with -->

<!-- \[ -->
<!-- b_{ij} = \begin{cases} -->
<!--  1 \textrm{ if items $x_i$ and $x_j$ are assigned to the same cluster}, \\ -->
<!--  0 \textrm{ else.} -->
<!-- \end{cases} -->
<!-- \] -->

<!-- $\mathbf{B}$ is referred to as the _coclustering matrix_. -->
<!-- ``` -->

<!-- #### Similarity matrices {-} -->

<!-- Two closely related concepts are that of the posterior similarity matrix -->
<!-- (**PSM**) and the consensus matrix (**CM**). These are defined as follows: -->

<!-- ```{definition, consensusMatrix, name = "Consensus matrix"} -->
<!-- A _consensus matrix_ (**CM**) is an $(N \times N)$ matrix that stores, -->
<!-- for each pair of items, the proportion of partitions in which the pair are -->
<!-- clustered together. The consensus matrix is obtained by taking the average over -->
<!-- the coclustering matrices obtained for partition. -->

<!-- Let $B^{(i)}$ be the coclustering matrix associated with the $i^{th}$ partition. -->
<!-- Then, for $I$ partitions, let the consensus matrix, $M$, be defined: -->

<!-- \begin{align} -->
<!-- M = \frac{1}{I}\sum_{i=1}^I B^{(i)}. -->
<!-- \end{align} -->

<!-- In other words, the $(i, j)^{th}$ entry of the CM is the proportion of partitions -->
<!-- for which the $i^{th}$ and $j^{th}$ items are allocated to the same cluster. -->
<!-- Thus, the consensus matrix is a symmetric matrix with all entries between 0 and -->
<!-- 1 and diagonal entries of 1. -->

<!-- ``` -->



<!-- ```{definition, psm, name = "Posterior similarity matrix"} -->
<!-- A _posterior similarity matrix_ (**PSM**) is an $(N \times N)$ matrix that stores, for -->
<!-- each pair of items, the proportion of paritions sampled from the Markov chain -->
<!-- for which the pair are clustered together. Thus one may consider the PSM as a -->
<!-- special case of the consensus matrix where all of the partitions are generated -->
<!-- from a single Markov chain. -->

<!-- ``` -->

<!-- Note that using the PSM and CM overcome the issues associated with the purely -->
<!-- symbolic nature of labels in a cluster analysis. -->


## Cluster analysis {#clusterAnalysis}

Cluster analysis – also known as unsupervised learning – consists of defining
partitions of the items within a dataset such that the resulting sets are
homogeneous and distinct from one another. These groups are known as clusters.
Clustering is used in multivariate statistics to uncover useful latent groups
suspected in the data or to discover interesting groups of homogeneous
observations. In large datasets, such as modern 'omics datasets, estimation of
clusters is often necessary for improved understanding and interpretation
[@meinshausen2010stability] - reducing a complex, $P$-dimensional dataset to
a 1-dimensional, discrete variable can significantly aid interpretation. 
The groups defined by cluster analysis are more homogenous than the full dataset,
this can enable downstream analysis, for example if there is some outcome variable 
of interest local linear models can explain variation within the clusters whereas
the global variation is too complex for a linear model [see @gadd2020enriched
for an example of a joint model for this kind of analysis]. In partitioning the
data, structure within the variation has been captured by the cluster analysis,
enabling a more interpretable analysis of each individual cluster.


Cluster analysis defines $K$ non-empty disjoint sets of data, each of which is
referred to as a _cluster_, the set of which form a _clustering_. This
clustering may be described by a collection of allocation labels,
$c=(c_1, \ldots, c_N)$. For an item $x_i$, the label $c_i=k$ states that
point $x_i$ is assigned to cluster $Y_k$.

```{definition, clustering, name="Clustering"}
If one has some collection of data $X=\left(x_1,\ldots,x_N\right)$, let a
_clustering_ or partition, $Y$, of the data be defined by:

\begin{align}
	Y &= \left\{Y_1,\ldots,Y_K\right\} \\
	Y_k &= \left\{x_i : c_i = k \right\}  \\
	Y_i \cap Y_j &= \emptyset \hspace{4 pt} \forall \hspace{4 pt} i,j \in \{1,\ldots,K\},  i \neq j \\
	n_k & = \mid Y_k \mid \hspace{4 pt} \geq 1 \hspace{4 pt} \forall \hspace{4 pt} k \in \{1,\ldots,K\} \\
	\sum_{k=1}^Kn_k &= n
\end{align}
	
```

The partition, $Y$, may be represented within a _coclustering matrix_. 

```{definition, coclustMatrix, name = "Coclustering matrix"}
Let $\mathbf{B}$ be a $N \times N$ binary matrix, with entries

\[
b_{ij} = \begin{cases}
 1 \textrm{ if items $x_i$ and $x_j$ are assigned to the same cluster}, \\
 0 \textrm{ else.}
\end{cases}
\]

for all $i, j \in 1, \ldots, N$. $\mathbf{B}$ is referred to as the _coclustering matrix_.
```


Cluster analysis and its extensions have provided great benefit in many biomedical settings, ranging from general tools to enable analysis [e.g. @crook2018bayesian; @dahl2006model] to uncovering novel biology in specific
diseases [e.g. cancer in @sorlie2001gene; @lapointe2004gene;
@cancer2012comprehensive; @berger2018comprehensive; coronary artery disease from
@guo2017cluster; tinnitus in @van2017cluster; and asthma in @ortega2014cluster].
Cluster analysis is also integral to the idea of precision medicine, which is
based upon "the classification of people into subpopulations using their common
genetic patterns, lifestyles, drug responses, and environmental and cultural
factors" [@gameiro2018precision].

The extension of cluster analysis to analyse multiple paired datasets
simultaneously in _integrative clustering_ has enabled deeper understanding of
more complex diseases [@hasin2017multi; @integrative2014integrative], for
example Parkinson's disease [@markello2020integrated] and cancer
[@wang2016integrative; @rappoport2018multi].

In a biomedical setting, different statistical items might be clustered to answer
different questions. For example, clustering of

* patients based upon their ‘omics profiles, environmental and lifestyle 
factors, and diseases characterisitics can be used to define disease subtypes
for more specific treatments. Improving understanding of common and unique mechanisms of diseases 
at the molecular level can enable development of treatments and deepen 
understanding of disease aetiology.
*	genes based upon their co-expression can improve our understanding of cellular
biology and gene pathways; such holistic understanding of human biology can 
enable targeted drugs and offer insights into diseases.

By 'omics data I mean the data produced by the high-throughput biochemical assays
known as "omics technologies". The "omics" notion refers to the comprehensive 
nature of the assay; all or nearly all instances of the targeted molecular space
are captured in the assay [@conesa2019making]. Thus, they provide holistic views
of the biological system. For example, transcriptomics refers to the complete
set of RNA transcripts produced by the genome under specific conditions or 
within a specific cell.

<!-- Cluster analysis has a myriad of applications. Some traditional applications -->
<!-- include data mining,  -->
<!-- which started from the search for groupings of customers and products in massive -->
<!-- retail datasets [@fraley2002model]; document clustering and the analysis of  -->
<!-- internet use data; and image analysis, where clustering is used for image -->
<!-- segmentation and quantization. -->

<!-- Within clinical medicine and molecular biology where complex, heterogeneous -->
<!-- datasets are prevalent, cluster analysis and its extensions have provided great -->
<!-- benefit in many settings, ranging from general tools to enable analysis -->
<!-- [e.g. @crook2018bayesian; @dahl2006model] to uncovering novel biology in specific -->
<!-- diseases [e.g. cancer in @sorlie2001gene; @lapointe2004gene; -->
<!-- @cancer2012comprehensive; @berger2018comprehensive; coronary artery disease from -->
<!-- @guo2017cluster; tinnitus in @van2017cluster; and asthma in @ortega2014cluster]. -->
<!-- Cluster analysis is also integral to the idea of precision medicine, which is -->
<!-- based upon "the classification of people into subpopulations using their common -->
<!-- genetic patterns, lifestyles, drug responses, and environmental and cultural -->
<!-- factors" [@gameiro2018precision]. -->

<!-- My area of interest is in clustering 'omics data to improve understanding of  -->
<!-- disease aetiology or the underlying biology of a system. Ideally this helps  -->
<!-- clinical practice through identifying driving mechanisms of a disease or in -->
<!-- defining subtypes within given diseases.  -->

<!-- Defining subtypes based upon the 'omics profile rather than upon phenotypic  -->
<!-- patterns can enable more effective treatment as greater diversity may be  -->
<!-- observed. Examples where cluster analysis has been used to propose  -->
<!-- new subtypes include such a diverse range of diseases as breast cancer [@cancer2012comprehensive; @berger2018comprehensive], Coronary artery disease [@guo2017cluster], tinnitus [@van2017cluster] and asthma [@ortega2014cluster]. -->


<!-- Another interesting application is the defining of subpopulations of patients; -->
<!-- this application is integral to _precision medicine_, which relies upon  -->
<!-- "the classification of people into subpopulations using their common genetic -->
<!-- patterns, lifestyles, drug responses, and environmental and cultural factors"  -->
<!-- [@gameiro2018precision]. -->


Within cluster analysis there exists many methods.
Traditional approaches [as taught in many textbooks, see for e.g.
@fidell2001using; @manly2016multivariate; @stevens2012applied] include
hierarchical and non-hierarchical (normally $k$-means) clustering. These
methods are heuristic in nature; they are not based upon a formal model that
explicitly models the data-generating process and what guidance there is for
solving important practical questions that arise in every cluster analysis (
such as the number of clusters present, the measure of distance / similarity
between points, which solution is optimal, etc.) is subjective and informal.
This has consequences for the interpretation of the results, as evaluation of
the significance of results is difficult [@monti2003consensus].

## Ensemble methods

<!-- I use the same definition of an _ensemble_ as @re2012ensemble.  -->

```{definition, ensemble, name = "Ensemble"}
A set of learning machines that work together to find some structure within data,
whether this be a supervised problem such as _classification_ or an unsupervised
problem such as cluster analysis.

```

<!-- There exists many other terms referring to the same basic concept, e.g. -->
<!-- a fusion, combination, aggregation, committee of learners, but I will use the  -->
<!-- term ensemble throughout this report. -->

The concept of an ensemble of learners and its improvements is an old one, and
not limited to machincal learners [@condorcet1785essay]. Much of the theoretical
underpinings of ensemble learning has focused upon resampling-based methods
and classification [@friedman2007bagging; @breiman1996bias; @schapire1998boosting],
in keeping with the  majority of algorithms, of which _Random forest_
[@breiman2001random] is possibly the most famous. There have been efforts to
provide a more general mathematical basis for ensembles; Eugene Kleinberg
deserves special mention for achievements based upon set theory and
combinatorics [@kleinberg1977infinitary; @kleinberg1990stochastic;
@kleinberg1996overtraining; @kleinberg2000mathematically]. More pertinently for
the practitioner, perhaps, these methods have also displayed great empricial
success, outperforming state-of-the-art methods in both simulations and
benchmark datasets [@breiman2001random; @monti2003consensus;
@sohn2007experimental; @afolabi2018ensemble]. Attractions of ensemble methods
include their ability to explore multiple modes [@ghaemi2011review] and reducitons
in computational runtime due to the fact that most
ensemble methods also enable use of a parallel enivronment to improve
computation speed [@ghaemi2009survey].

For a more thorough review of ensemble methods with a focus on classification,
please see @re2012ensemble.

@monti2003consensus proposed a general frameworks for ensembles of clusterers,
"Consensus clustering". It is an model-independent, sampling-based method, that
attempts to provide a rule-based approach to choosing $K$ in the model and to
improve cluster stability. The individual-level algorithm investigated in the
original publication and implemented in the ``ConsensusClusterPlus`` R package
[@wilkerson2010consensusclusterplus] is $k$-means clustering. This flavour of
Consensus clustering has been successfully used in cancer subtyping
[@verhaak2010integrated; @marisa2013gene].

Consensus clustering creates a _consensus matrix_ and uses this to perform 
choose $K$ and create a point clustering [@monti2003consensus].

```{definition, consensusMatrix, name = "Consensus matrix"}
A _consensus matrix_ (**CM**) is an $(N \times N)$ matrix that stores,
for each pair of items, the proportion of partitions in which the pair are
clustered together. The consensus matrix is obtained by taking the average over
the coclustering matrices obtained for partition.

Let $B^{(i)}$ be the coclustering matrix associated with the $i^{th}$ partition.
Then, for $I$ partitions, let the consensus matrix, $M$, be defined:

\begin{align}
M = \frac{1}{I}\sum_{i=1}^I B^{(i)}.
\end{align}

In other words, the $(i, j)^{th}$ entry of the CM is the proportion of partitions
for which the $i^{th}$ and $j^{th}$ items are allocated to the same cluster.
Thus, the consensus matrix is a symmetric matrix with all entries between 0 and
1 and diagonal entries of 1.

```

The original algorithm is described below. 

| **PROCEDURE:** Consensus Clustering
|
| **Input:** 
| a dataset $X=(x_1, \ldots, x_N)$
| a clustering algorithm _Cluster_
| a resampling scheme _Resample_
| number of resampling iterations $S$
| set of cluster numbers to try, $\mathcal{K}=\{K_1, \ldots, K_{max}\}$
| 
| **Method:** 
| for $K$ in $\mathcal{K}$ do 
|   $M^{(K)} \leftarrow 0_{N \times N}$ \{the initialised (and empty) consensus matrix.\}
|   for $s$ in $1, 2, \ldots, S$ do
|     $X^{(s)} \leftarrow$ _Resample_$(X)$ \{generate a peturbed version of $X$.\}
|     $B^{(s)} \leftarrow$ _Cluster_$(X^{(s)}, K)$ \{cluster $X^{(s)}$ into $K$ clusters, represented by the coclustering matrix.\}
|     $M^{(K)} \leftarrow M^{(K)} + B^{(s)}$ \{add the current clustering to the record.\}
|   end
|   $M^{(K)} \leftarrow \frac{1}{S} M^{(K)}$ \{normalise the consensus matrix.\}
| end
| $\hat{K} \leftarrow$ best $K \in \mathcal{K}$ based upon all $M^{(K)}$
| $\hat{Y} \leftarrow$ partition $X$ based upon $M^{(\hat{K})}$
|
| **Output:** 
| consensus matrix $M^{(\hat{K})}$
| partition $\hat{Y}$

## Model-based clustering

Although ensemble methods can offer improvements and some more defined 
rules on choosing $K$, they do not overcome the heuristic nature of the 
underlying clustering algorithm or analysis decisions. 
_Model-based cluster analysis_ or _mixture models_ approach embeds the cluster
analysis within a statistical framework and offers a principled approach to many
issues associated with cluster analysis. Each subpopulation is described
within the model by a probability distribution (hence the model is a mixture of
distributions). In this setting, the problem of determining the number of
clusters can be recast as statistical model choice problem, and
models that differ in numbers of components and/or in component distributions
can be consistently compared. Extensions accounting for outliers add one or
more components representing a different distribution for outlying data
[@fraley2002model; see an example in @crook2018bayesian].

```{definition, mixtureModels, name="Finite mixture models"}

If one is given some data $X = (x_1, \ldots, x_N)$, we assume $K$ unobserved
subpopulations generate the data and that insights into these sub-populations
can be revealed by imposing a clustering $Y = \left\{Y_1,\ldots,Y_K\right\}$ on
the data. It is assumed that each of the $K$ clusters can be modelled by a
parametric distribution, $f_k(\cdot)$ with parameters $\theta_k$. Normally a
common distribution, $f(\cdot)$, is assumed across all components. We let
membership in the $k^{th}$ cluster for the $i^{th}$ individual be denoted by
$c_i = k$. The full model density is then the weighted sum of the probability
density functions where the weights, $\pi_k$, are the proportion of the total
population assigned to the $k^{th}$ cluster. Then for item $x_i$ in a _finite
mixture model_:

\begin{align}
	p(x_i|c_i = k) &= \pi_k f(x_i | \theta_k) \\
	p(x_i) &= \sum_{k=1}^K \pi_k f(x_i | \theta_k)
\end{align}

```

The flexibility in choice of $f(\cdot)$ means that these models can be applied
in many different scenarios.

Inference of the parameters of these models is normally performed using the
Maximum Likelihood Estimates (**MLE**) or Bayesian inference. Due to the complex
likelihood equations usually associated with these models, the closed form
solution is not possible [@stahl2012model]. For MLE, an
Expectation-Maximisation (**EM**) algorithm is used to find a solution whereas 
in Bayesian inference the most common method used is a Markov-Chain Monte Carlo 
(**MCMC**) method.

### Maximum likelihood estimate

MLE-based inference of mixture models tends to be very quick but it is:

<!-- 1. difficult to combine multiple sources of information [see @singh2005combining -->
<!-- for an  example of how to consider this]; -->
1. sensitive to initialisation; EM is prone to finding local maxima - to account
for this these models are run from multiple different initialisations. In this
case observing the same log‐likelihood values from multiple starting points
increases confidence that the solution is a global maximum. Using a some
heuristic clustering method to initialise the method [such as in @mclust2016]
also offers stability.
2. prone to singularities; there are points where the
likelihood becomes infinite, resulting in degenerate distributions.
Singularities occur when the number of parameters to be estimated is large in
relation to the sample size [@stahl2012model], models with unrestrained
covariances and large numbers of components are prone to this problem.
@fraley2007bayesian suggest using Bayesian inference as a means of overcoming
this issue, recommending use of a _maximum a posteriori_ (*MAP*) estimate
instead of an ML estimate.

### Bayesian inference

Another choice is to perform Bayesian inference. This allows:

1. Incorporation of prior beliefs into the inference; this also means that one
may apply Bayesian methods even when the sample size is small.
2. Inference of $K$ through use of the Dirichlet Process [@ferguson1973bayesian].
3. Avoidance of some of the issues in convergence. As one often is only
interested in a subset of variables (for example, the allocation variables,
$c_i$), one may integrate over the remaining nuisance parameters. This means
that one may avoid directly calculating awkward variables (such as covariances
matrices) that are prone to causing issues such as singularities in the
likelihood function [an example of such a solution is the
_collapsed Gibbs sampler_; @liu1994collapsed].
4. Extension to a multiple dataset setting [see @kirk2012bayesian;
@gabasova2017clusternomics; and @lock2013bayesian for
examples].

MCMC methods also offer asymptotic guarantees with regards to exploring the 
full posterior distribution rather than a single mode. However, in practice 
ensuring that a sampler has:

1. reached its stationary distribution;
2. explored the entire support of the target distribution; and
3. converged to its expectation

is difficult despite these aysmptotic guarantees [@robert2018accelerating]. 
<!-- For example, the _Gibbs sampler_ [@geman1984stochastic], a common choice of sampler, can struggle to achieve these three goals due to high correlation between samples [@bishop2006pattern]. Integrating over nuisance parameters as in a _collapsed Gibbs sampler_ [@liu1994collapsed] can improve rate of convergence upon the expectation, but can reduce the exploration rate of the chain. -->
There exists many more flavours of MCMC that attack these 3 problems in
different ways [@robert2018accelerating].  Methods which attempt to improve
the ability of the sampler to explore the
support of the target distribution such as simulated annealing
[@kirkpatrick1983optimization; @atchade2011towards], incorporating
global-local moves [@holmes2017adaptive] or split-merge steps [@jain2004split];
all offer some degree of success, but at the cost of additional computational
complexity. For high dimensional data these remain slow to converge and may have
limited success. Other methods attempt to accelerate the speed at which the
sampler reaches the target distribution, e.g. utilising stochastic optimisation
[@welling2011bayesian] or to attempt and leverage parallel chains
[@ahmed2012scalable; however there is controversy over convergence in parallel
methods, @robert2018accelerating].

In short, different MCMC methods tend to trade off different weaknesses to be
applicable in different contexts, but there are no general solution to the 3 
problems listed by @robert2018accelerating.

<!-- An alternative means of performing Bayesian inference, VI is an optimisation -->
<!-- method more like EM. This proposes a family -->
<!-- of densities, $q(\theta)$, and then tries to find the member of this family that -->
<!-- is closest to the target density, where closeness is measured by the -->
<!-- Kullback–Leibler divergence  [@blei2017variational]. VI is intended as a fast -->
<!-- alternative to MCMC when traditional MCMC methods are not computationally -->
<!-- tractable. However, it does have several disadvantages: -->

<!-- 1. VI underestimates the variance [@bishop2006pattern]; -->
<!-- 2. VI is prone to becoming stuck in local minima, thus several restarts are -->
<!-- required (as with EM); -->
<!-- 3. VI does not offer theoretical guarantees (unlike MCMC) [@blei2017variational]; -->
<!-- and -->
<!-- 4. deriving the correct equations to apply VI in a specific situation is non-trivial. -->


Analagous to the consensus matrix within Bayesian inference is the notion of the
Posterior similarity matrix.

```{definition, psm, name = "Posterior similarity matrix"}
A _posterior similarity matrix_ (**PSM**) is an $(N \times N)$ matrix that stores, for
each pair of items, the proportion of partitions sampled from the Markov chain
for which the pair are clustered together. Thus one may consider the PSM as a
special case of the consensus matrix where all of the partitions are generated
from a single Markov chain.

```

<!-- A common theme across all these methods is that they can struggle to converge -->
<!-- to the global mode. If one is interested in capturing uncertainty and describing -->
<!-- multiple modes, than MCMC appears to be the best option, but it can struggle -->
<!-- to explore the full distribution. MCMC methods which overcome this issue can be -->
<!-- very slow. -->


