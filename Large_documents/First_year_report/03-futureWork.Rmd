# Future work {#futureWork}

There are two main aspects to current future work; immediate work to turn my 
work-to-date upon consensus inference and mixutre models into a paper and more 
long-term plans for my PhD.

1. CI:
  * multiple-dataset simulations
  * real dataset
  
2. Where is my PhD going? There are two areas that are becomign more interesting 
to me as possible applications of integrative clustering:
  * immunology: is an area of biology where many pathologies have been shown to 
  have a genetic component. Investigating the heterogeneity amongst patients and
  genes through integrative clustering of 'omics data is an exciting proposal as
  there is known to be relevant and coherent signal in genetic data already. 
  Investigating diseases and settings where complex molecular biology is 
  unfolding strikes me as a natural applicaiton of clustering methods, the main 
  attraction of which is improved interpretability.
    1. Clustering cells: we know that our classification of cells into different
    groups is simplistic. Each cell is unique; many share broad properties and 
    particularly characterisitcs, but the pretence that all cells of a certain 
    type are exact clones is misleading. Clustering cells by their gene 
    expression might reveal interesting heterogeneity within populations 
    traditionally viewed as homogeneous. Particlarly within patients with 
    diseases associated with specific cell-types, it might be interesting to see
    if subsets of traditional cell groups are behaving significantly differently
    by assesing all genes present, rather than relying on a small number of key 
    signals;
    2. Clustering genes: understanding how genes are expressed differently 
    within different environments is key to understanding local pathologies like
    many auto-immune diseases (such as RA and T1D). Clustering genes across 
    tissues and cell types would be a natural application of Bayesian 
    integrative clustering as uncertainty is quantified and the similarity
    between tissues may be inferred from the data (thus tissue heterogeneity
    will be shown in the final results and we may learn which tissues may 
    contain useful information regarding other tissues that might be harder / 
    more damaging to access for a patient).
    3. Clustering patients: it is know believed that there is greater 
    heterogeneity within many traditional auto-immune diseases. Clustering 
    patients across multiple 'omics derived from the tissue associated with the 
    disease and other datasets such as patrolling CD4, CD14 cells (i.e. immune
    cells that are probably not activated due to being away from the inflammed 
    area but demonstrative of what the patients immune cells may have appeared 
    prior to disease onset) could be of interest. This would allow subdivison of
    patients based upon an omics profile rather than purely upon observed 
    phenotype and disease progression (although a validation of the clustering
    would include correlation between some of the predicted clusters and 
    traditional clusters).
  * cancer: this area is the traditional application of integrative methods; 
  there are a range of 'omics views available for many different cancer types.
  The large amount of data avaialble in this realm (such as the Human Tumour 
  Atlas Network) means that there exist large datasets that offer an opportunity 
  to showcase consensus inference; the parallel nature of CI means that one may 
  leverage more data than most Bayesian methods can use in an analysis (as the
  serial nature of traditional MCMC analysis means that one must reduce the 
  dataset to something feasible for a chain to converge upon in finite time). 
  Ideally the additional data and better ability to incorporate multiple modes
  means that CI might unveil new biology within some of these datasets.
  
Using a clustering method which contains uncertainty (partially representing the
diversity within cell types, partially model uncertainty) and can model across 
multiple datasets simulataneously (and may infer the similarity between datasets)
strikes me as greatly interesting. It would allow one to cluster genes across 
cell types, and 

and if both controls and people with the phenotype are present, to ideally observe
which genes are 


## Consensus inference

### Current project

Currently I have explored the performance of Consensus inference for mixture
models. The next step is to extend the simulation study to cover the multiple
dataset case. That consensus inference may be used as an alternative to 
Bayesian inference in integrative clustering is one of its attractions. 
Investigating performance in this setting, which is an area where MLE models do
not extend to and sampling-based Bayesian models can struggle to scale to, is 
key to highlighting the value of consensus inference. I have already shown that 
when a large number of features are present that consensus inference performs 
well.  and, improtantly, quickly in comparison to Bayesian inference. In 
multi-'omics analyses this problem is present in **each dataset**, exacerbating
the problems present in the single dataset case. This can make Bayesian 
inference unfeasible in this setting. As I have already shown in the single 
dataset case, a Gibbs sampler can struggle as dimensionality scales, becoming
trapped in modes as the likelihood function is a $P$-polynomial. Adding 
additional parameters for each dataset means that the likelihood surface becomes
even more extreme and modes are extremely dense relative to the surronding 
space. In this setting I hope that Consensus inference would perform well.

To finish my current project I then intend to apply Consensus inference to a 
real dataset. The Cancer Genome Atlas (**TCGA**) has many datasets that are used
in a number of integrative methods papers. Cancer data is seen as useful for 
validating a clustering method, as the subtypes (of many cancers) are defined
by their 'omics profile. This means that there exists known structure in the 
data that a method should uncover (at least to some degree). Furthermore, many 
of the specific datasets have already been studied, so a comparison between
methods is quite easy. 

To uncover subtypes within  cancer data requires that we infer a global 
clustering. This is a challenge that MDI is not designed for. Based upon the 
models I mentioned in \@ref(bayesIntClust), I would propose using 
either Bayesian Consensus Clustering, Clusternomics or else MDI with the 
PSMs then processed by KLIC to create a global clustering. However, the
Clusternomics R package as available on Github does not work in its current 
state. Thus I propose use of BCC and MDI + KLIC. 

I think a more interesting dataset (and one better suited to the MDI model) 
would be investigation the clustering behaviour of genes across disease states.
Using data such as that available at the Human Cell Atlas it might be 
interesting to cluster genes in people with the disease state and controls, 
treating the different disease states and controls as different datasets. Due to
the paired dataset correlation parameter, $\phi_{ij}$, datasets will be 
encouraged to cluster similarly (particularly as I would expect the majority of
clusters to align well). Thus any dataset specific clusters would hopefully 
contain strong signal and might describe novel biology that helps improve our
understanding of the disease aetiology.

### Posisble future extensions

Expansions to Consensus inference are possible. Currently Consensus inference
leverages many of the strenghts of Bayesian inference (although it lacks 
certain strengths). I would consider using _sampling techniques_ to similar 
improve performance. I think sampling items with replacement and sampling
features could both be beneficial. Ideally sampling the items would reveal that
some clusters are merging due to a small number of items. As dimensionality is
such a creator of problems for the inference, sampling features would reduce the
relative density of modes. If we sample features that contain no signal, we have
seen in the no structure case that a chain will not discover false signal 
(although if the number of features is not small enough it may stay in the 
initialised clustering rather than reduce to no structure). Thus sampling 
features could help make any signal present more clear without the fear that the 
cases where there are no structure will hide any signal when results are 
compiled. A condition here would be that one samples a sufficiently small number 
of features that modes are less attractive to each chain. I suspect that 
sampling the items and features would require more chains for stable results, 
but I think that several individual chains would be significantly improved, 
improving the inference overall sufficiently to make the cost worthwhile. 

Note: sampling features might also help if there are several clustering 
structures present (although this is a different problem).

I think that the issue of inference in high $P$ space is still difficult enough
that any improvement that sampling offers is attractive. I also think that
the computational cost of additional chains will be partially offset by each
individual chain becoming quicker due to reducing the dimensionality of the 
problem.

