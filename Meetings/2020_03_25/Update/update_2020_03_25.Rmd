---
title: "Update 25/03/2020"
author: "Stephen Coleman"
date: "23/03/2020"
output: pdf_document
bibliography: update_2020_03_25_bib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(shiny)
library(MASS)
library(pheatmap)
library(ggplot2)
library(viridis)
library(magrittr)
# library(mdiHelpR)
library(ggfortify)
library(bookdown)

set.seed(1)

# define our ggplot2 theme of choice
theme_set(theme_bw() +
  theme(strip.background = element_rect(fill = "#21677e")) +
  theme(strip.text = element_text(colour = "white")))

defineBreaks <- function(col_pal, lb = -1, ub = 1, mid_point = 0.5 * (lb + ub)) {
  palette_length <- length(col_pal)

  breaks <- c(
    seq(lb, mid_point, length.out = ceiling(palette_length / 2) + 1),
    seq(mid_point + 1 / palette_length, ub, length.out = floor(palette_length / 2))
  )
}

defineDataBreaks <- function(x, col_pal, mid_point = NULL) {
  if (is.null(col_pal)) {
    col_pal <- dataColPal(n)
  }
  lb <- min(x)
  ub <- max(x)

  if (is.null(mid_point)) {
    if (lb < 0 & ub > 0) {
      mid_point <- 0
    } else {
      mid_point <- 0.5 * (lb + ub)
    }
  }

  defineBreaks(col_pal, lb = lb, ub = ub, mid_point = mid_point)
}

#' @title Generate dataset
#' @description Generate a dataset based upon the cluster means
#' (assumes each feature is independent)
#' @param cluster_means A k-vector of cluster means defining the k clusters.
#' @param n The number of samples to generate in the entire dataset.
#' @param p The number of columns to generate in the dataset.
#' @param pi A k-vector of the expected proportion of points to be drawn from
#' each distribution.
#' @param row_names The row names of the generated dataset.
#' @param col_names The column names of the generated dataset.
generateDataset <- function(cluster_means, cluster_sds, n, p, pi,
                            row_names = paste0("Person_", 1:n),
                            col_names = paste0("Gene_", 1:p)) {

  # The number of distirbutions to sample from
  K <- length(cluster_means)

  # The membership vector for the n points
  cluster_IDs <- sample(K, n, replace = T, prob = pi)

  # The data matrix
  my_data <- matrix(nrow = n, ncol = p)

  # Iterate over each of the columns permuting the means associated with each
  # label.
  for (j in 1:p)
  {
    reordered_cluster_means <- sample(cluster_means)
    reordered_cluster_sds <- sample(cluster_sds)

    # Draw n points from the K univariate Gaussians defined by the permuted means.
    for (i in 1:n) {
      my_data[i, j] <- rnorm(1,
        mean = reordered_cluster_means[cluster_IDs[i]],
        sd = reordered_cluster_sds[cluster_IDs[i]]
      )
    }
  }

  # Order based upon allocation label
  row_order <- order(cluster_IDs)

  # Assign rownames and column names
  rownames(my_data) <- row_names
  colnames(my_data) <- col_names

  # Return the data and the allocation labels
  list(
    data = my_data[row_order, ],
    cluster_IDs = cluster_IDs[row_order]
  )
}

# This is an idea for generating data
generateFullDataset <- function(K, n, p,
                                delta_mu = 1,
                                cluster_sd = 1,
                                pi = rep(1/K, K),
                                p_noisy = 0,
                                alpha = 2) {
  my_data <- list(
    data = NA,
    cluster_IDs = NA
  )

  # Generate some cluster means
  cluster_means <- seq(from = 0, to = (K - 1) * delta_mu, by = delta_mu) %>%
    scale(center = T, scale = F)

  if (delta_mu == 0) {
    cluster_means <- rep(0, K)
  }

  # Generate some cluster standard deviations
  cluster_sds <- rep(cluster_sd, K)

  # if (pi_method == "even") {
  #   pi <- rep(1, K)
  # } else {
  #   pi <- rgamma(K, alpha)
  #   pi <- pi / sum(pi)
  # }

  # Find the number of requested informative features
  p_signal <- p # max(0, (p - p_noisy))

  data_sd <- 1

  # Generate data
  if (p_signal > 0) {
    my_data <- generateDataset(cluster_means, cluster_sds, n, p_signal, pi)
    data_sd <- sd(my_data$data)
  }

  # If irrelevant features are desired, generate such data
  if (p_noisy > 0) {
    noisy_data <- lapply(1:p_noisy, function(x) {
      rnorm(n, sd = data_sd)
    }) %>%
      unlist() %>%
      matrix(ncol = p_noisy) %>%
      set_colnames(paste0("Noise_", 1:p_noisy))

    if (p_signal > 0) {
      my_data$data <- cbind(my_data$data, noisy_data)
    } else {
      my_data$data <- noisy_data %>%
        set_rownames(paste0("Person_", 1:n))

      my_data$cluster_IDs <- rep(1, n)
    }
  }

  my_data
}

# Badly named heatmapping function
plotData <- function(x, cluster_IDs,
                     col_pal = colorRampPalette(c("#146EB4", "white", "#FF9900"))(100),
                     my_breaks = defineDataBreaks(x, col_pal, mid_point = 0),
                     main = "gen_dataset",
                     ...) {
  anno_row <- data.frame(Cluster = factor(paste("Cluster", cluster_IDs))) %>%
    set_rownames(rownames(x))

  K <- length(unique(cluster_IDs))

  ann_colours <- list(Cluster = viridis(K) %>%
    set_names(paste("Cluster", sort(unique(cluster_IDs)))))

  ph <- pheatmap(x,
    color = col_pal,
    breaks = my_breaks,
    annotation_row = anno_row,
    annotation_colors = ann_colours,
    main = main,
    ...
  )

  ph
}


pcaScatterPlot <- function(pca_mat, labels, n_comp = 10){
  
  plt_pca_data <- pca_mat %>% 
    as.data.frame(row.names = row.names(.)) %>% 
    tibble::add_column(Cluster = as.factor(labels)) %>% 
    tidyr::gather(key = "Component", value = "Loading", -Cluster, factor_key = T)
  
  p <- ggplot(plt_pca_data, aes(x = Component, y = Loading, colour = Cluster)) +
    geom_point() +
    scale_color_viridis_d() +
    # theme(axis.text.x = element_text(angle = 30)) +
    xlim(paste0("PC", 1:n_comp))
  
  p
}

```

## General model

For data $X=(x_1, \ldots, x_N)$, where each item $x_i = (x_{i1}, \ldots, x_{iP})$, we use a $K$-component mixture-model paramaterised by $\theta$ to describe the data:

\[
p(x_i | \theta, \pi) = \sum_{k=1}^K \pi_k f(x | \theta_k).
\]

Here $\pi=(\pi_1, \ldots, \pi_K)$ is the proportion of items assigned to each component and $\theta_k$ is the component specific parameters.

We assume that there is a common probability density function, $f(\cdot)$, associated with each component (e.g. Gaussian). Independence is assumed between the $P$ features, thus:

\[
p(x_i | \theta, \pi) = \sum_{k=1}^K \pi_k \prod_{p=1}^P f(x | \theta_{kp}),
\]

where $\theta_{kp}$ is the parameters for the $p^{th}$ feature within the $k^{th}$ component (e.g. if we are using a \emph{Gaussian mixture model}, then $\theta_{kp}=(\mu_{kp}, \sigma^2_{kp})$, the mean and standard deviation of the items in the $k^{th}$ component within the $p^{th}$ feature).

In the language of @law2003feature, we assume that a subset of the features are \emph{irrelevant}. By this we mean that for a given item $x_i$,

\[
f(x_i|\theta_{kp}) = f(x_i|\theta_{lp}) = g(x_i | \lambda_p) \hspace{1.5mm} \forall \hspace{1.5mm} k, l \in \{1, \ldots, K\}.
\]

Thus an irrelevant feature does not contribute any component specific information and is irrelevant to uncovering structure within the data. Let $\Phi=(\phi_1, \ldots, \phi_P)$ be a binary variable indicating the relevance of a feature (i.e. $\phi_p = 1$ if the $p^{th}$ feature is relevant and $0$ otherwise). Then our model can be written:

\begin{equation} 
p(x_i | \theta, \pi, \Phi) = \sum_{k=1}^K \pi_k \prod_{p=1}^P f(x_i | \theta_{kp})^{\phi_p} g(x_i | \lambda_p)^{(1 - \phi_p)}.
\end{equation} 

If we use an allocation variable $z=(z_1,\ldots,z_N)$ to indicate which component the items belong to, we may write:

\[
\begin{aligned} 
p(x_i | z_i = k, \theta, \pi, \Phi) &= \prod_{p=1}^P f(x_i | z_i = k, \theta_{kp})^{\phi_p} g(x_i | \lambda_p)^{(1 - \phi_p)} \\
p(x | z, \theta, \pi, \Phi) &= \prod_{i=1}^N \prod_{p=1}^P f(x_i | z_i = k, \theta_{kp})^{\phi_p} g(x_i | \lambda_p)^{(1 - \phi_p)}
\end{aligned} 
\]

<!-- Let $Z$ be the allocation matrix (i.e. a $N \times K$ binary matrix where each row contains only one non-zero entry; a value of 1 in the $(i, k)^{th}$ entry indicates that $z_i = k$).  -->

# Simulations

In our simulations we are interested in testing how *consensus inference* compares to Bayesian inference of mixture models in various circumstances. In each simulation we will assume a generative model that can be described by a finite mixture of Gaussian models.

\begin{equation} 
p(x_i | z_i = k, \theta, \pi, \Phi) = \prod_{p=1}^P f(x_i | z_i = k, \theta_{kp})^{\phi_p} f(x_i | \theta_p)^{(1 - \phi_p)}.
\end{equation} 

where $f(\cdot)$ describes the Gaussian pdf and thus $\theta=(\mu, \sigma^2)$. We assume that the irrelevant variables will be Gaussian in form and that the observed values will be drawn from the same Gaussian distribution for the entire population. Let $P_n=\sum_{p=1}^P\phi_p$ be the number of irrelevant features present, and $P_s = P - P_n$ be the number of relevant features present. Then in each simulation we will change various variables associated with this model:

* $N$: the number of items being clustered;
* $P_s$: the number of *relevant* features present;
* $P_n$: the number of *irrelevant* features present;
* $K$: the true number of subpopulations present;
* $\pi$: the proportion of points sampled from each component;
* $\Delta_{\mu}$: the difference between the means associated with each component in each feature; 
* $\sigma^2$: the standard deviation within each feature for each component; and
* $\alpha$: the concentration of the Dirichlet distribution $\pi$ might be generated from (only relevant when $\pi$ is sampled as explained below).

$\pi$ will be chosen in one of two ways:

* "Even": a $K$-vector with all entries equal to $\frac{1}{K}$; or
* "Varying": sampled from a Dirichlet distribution with concetration of $alpha$.

In the second case we will explain our choice of $\alpha$ each time.

I would expect that there is some function of the number of samples, the number of informative features, the number of clusters, the distance between component means and the value of $\sigma^2$ used that explains how easy it is to resolve the clutering structure. If $C'$ is the true clustering and $C^*$ is that predicted by the model, and $ARI(X, Y)$ is the adjusted rand index between partitions $X$ and $Y$, then I expect there to be some relationship of the nature:

\[
\begin{aligned}
ARI(C^*, C') &\propto \log(N) \\
ARI(C^*, C') &\propto \sqrt{P_S} \\
ARI(C^*, C') &\propto -K \log(K) \\
ARI(C^*, C') &\propto \Delta_{\mu} \\
ARI(C^*, C') &\propto \frac{1}{\sigma} \\
ARI(C^*, C') &\propto \frac{\Delta_{\mu}K\log\left({\frac{N}{K}}\right) \sqrt{P_S}}{\sigma} 
\end{aligned}
\]

I do not expect that the stated nature of these relationships is true, but the directionality of these relationships is expected to hold and something of the relative speed of improvement in resolving the true structure is intended to be indicated by the use of $\log(\cdot)$ and $\sqrt{\cdot}$. Note that the positive linear dependence on $K$ is due to how I am defining $\Delta_{\mu}$. These statements also convey that for many of the variables it is relative values that matter; for instance if $\Delta_{\mu}$ increases we expect improved resolution of clusters, but if $\sigma^2$ also grows proportionally, then we would not expect the improvement to be at all as significant.

We will test:

1. The 2D Gaussian case (this is a sense-check);
2. The lack-of-structure case in 2 dimensions;
3. The large $N$, small $P$ paradigm;
4. Increasing $\sigma^2$;
5. Increasing the number of irrelevant features;
6. The small $N$, large $P$ case; and
7. Varying the proportion of the total population assigned to each sub-population.

## Simulation 1: 2D Gaussian

This is a sense-test case. It is the easiest to judge how well sensible the final clustering is as we can visualise the data fully in a 2D setting.

* $N$ = 100;
* $P_s$ = 2;
* $P_n$ = 0;
* $K$ = 5;
* $\pi = (0.2, 0.2, 0.2, 0.2, 0.2)^T$;
* $\Delta_{\mu} = 2$; and
* $\sigma^2_{kp} = 1$.

```{r sim_1_2d_gaussian, echo = F}

# Generate data based upon inputs
gaussian_means <- matrix(
  c(-3, -3, -3, 3, 3, -3, 3, 3, 0, 0),
  nrow = 2
)

# 
k <- ncol(gaussian_means)
n <- 100
p <- nrow(gaussian_means)

Sigma <- diag(1, nrow = p)

for (i in 1:k) {
  cluster_data <- mvrnorm(n / k, gaussian_means[, i], Sigma)

  if (i == 1) {
    my_data <- cluster_data
  } else {
    my_data <- rbind(my_data, cluster_data)
  }
}

my_data <- data.frame(
  Gene_1 = my_data[, 1],
  Gene_2 = my_data[, 2]
) %>% set_rownames(paste0("Person_", 1:n))

plot_data <- my_data
plot_data$Cluster <- as.factor(rep(1:k, each = n / k))

ggplot(plot_data, aes(x = Gene_1, y = Gene_2, colour = Cluster)) +
  geom_point(alpha = 1) +
  labs(
    title = "Simple mixture of Gaussians",
    x = "Gene 1",
    y = "Gene 2"
  ) +
  scale_colour_viridis_d()


plotData(my_data, rep(1:k, each = n / k),
  main = "Simulation 1: 2D Gaussian",
  show_rownames = F,
  show_colnames = F,
  cluster_rows = F,
  cluster_cols = T
)


```

## Simulation 2: No structure

We wish to test the case when there is no structure present (i.e. all items are generated from the same Gaussian distirbution). In this scenario there are no subpopulations present so all items should be allocated to the same component.

* $N$ = 100;
* $P_s$ = 0;
* $P_n$ = 2;
* $K$ = 1;
* $\pi = 1$;
* $\Delta_{\mu} = 0$; and
* $\sigma^2_{kp} = 1$.


```{r simulation_2_1_gaussian, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 0
P_n <- 2
K <- 1
pi <- rep(1/K, K)
delta_mu <- 0
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 1) +
  labs(
    title = "PCA of generated data",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 2",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

## Simulation 3: Large $N$, Small $P$

Test performance within the large $N$, small $P$ paradigm such as is encountered in flow cytometry. Many points will be confidently allocated, but as the number of features is small some items are liable to be on the border of several clusters and therefore harder to allocate (although even $P_S=4$ might be sufficient for high performance).

### Simulation 3a: Large $N$, Small $P$ dataset

We use a large sample size and a small number of features to investigate the *large $N$, small $P$* case.

* $N$ = 10,000;
* $P_s$ = 4;
* $P_n$ = 0;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_3a_large_N_small_P, echo = F}
# Generate data based upon inputs
N <- 10000
P_s <- 4
P_n <- 0
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 3a: large N, small P",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 4) +
  labs(
    title = "PCA for simulation 3a: large N, small P"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 3a: Large N, small P",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

### Simulation 3b: Large $N$, large $K$, small $P$ dataset

Similarly to 3a, we use a large $N$ and small $P$ but also grow $K$:

* $N$ = 10,000;
* $P_s$ = 4;
* $P_n$ = 0;
* $K$ = 50;
* $\pi = vec(\frac{1}{50})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_3b_large_N_small_P_large_K, echo = F}
# Generate data based upon inputs
N <- 10000
P_s <- 4
P_n <- 0
K <- 50
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 3b: large N, small P, large K",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 4) +
  labs(
    title = "PCA for simulation 3b: large N, small P, large K"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 3b: Large N, small P, large K",
  show_rownames = F,
  show_colnames = F,
  silent = F,
  annotation_legend = F
)
```

### Simulation 3c: Large $N$, large $K$, small $P$, small $\Delta_{\mu}$ dataset

Similar to 3b but now the distance between means is decreased. This should be very hard to cluster well as many of the components overlap on the small number of features. It might be an interesting case to compare many short chains to a single long chain. I suspect that with the small number of features that the Bayesian inference should be less liable to becoming trapped (however, if a cluster is emptied it may struggle to include a previously emptied component). If consensus inference behaves comparatively well here I think that's a good indication of robustness as my a priori expectations are that a long chain would perform better as it explores the different probable clusterings (basically, small number of features but high uncertainty in the clustering structure should favour a long chain I think).

To be clear, *no method will find the large number of ``true'' subpopulations present*, but there is structure present as well as many valid possible partitions.

* $N$ = 10,000;
* $P_s$ = 4;
* $P_n$ = 0;
* $K$ = 50;
* $\pi = vec(\frac{1}{50})$;
* $\Delta_{\mu} = 0.5$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_3c_large_N_small_P_large_K_close_means, echo = F}
# Generate data based upon inputs
N <- 10000
P_s <- 4
P_n <- 0
K <- 50
pi <- rep(1/K, K)
delta_mu <- 0.2
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 3c: large N, small P, large K, close means",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 4) +
  labs(
    title = "PCA for simulation 3c: large N, small P, large K, close means"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 3c: Large N, small P, large K, close means",
  show_rownames = F,
  show_colnames = F,
  silent = F,
  annotation_legend = F
)

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = T,
  main = "Simulation 3c: Large N, small P, large K, close means (clustered)",
  show_rownames = F,
  show_colnames = F,
  silent = F,
  annotation_legend = F
)

```

## Simulation 4: Increasing $\sigma^2$

We test the ratio of $\mu$ to $\sigma^2$ required for structure to be successfully uncovered in a feature-rich dataset.

### Simulation 4a: Large, informative dataset, medium $\sigma^2$

* $N$ = 100;
* $P_s$ = 500;
* $P_n$ = 0;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 5$.

```{r simulation_4_large_informed_large_sd, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 500
P_n <- 0
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 3
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 4a: Above-average standard deviation",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 4a: Above-average standard deviation"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 4a: Above-average standard deviation",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

### Simulation 4b: Large, informative dataset, larger $\sigma^2$

* $N$ = 100;
* $P_s$ = 500;
* $P_n$ = 0;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 30$.

```{r simulation_4b_large_informed_larger_sd, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 500
P_n <- 0
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 5
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 4b: Large standard deviation",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 4b: Large standard deviation"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 4b: Large standard deviation",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

### Simulation 4c: Large, informative dataset, largest $\sigma^2$

* $N$ = 100;
* $P_s$ = 500;
* $P_n$ = 0;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 10$.

```{r simulation_4c_large_informed_largest_sd, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 500
P_n <- 0
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 10
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 4c: Very large standard deviation",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 4c: Very large standard deviation"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 4c: Very large standard deviation",
  show_rownames = F,
  show_colnames = F,
  silent = F
)


ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = T,
  main = "Simulation 4c: Very large standard deviation (ordered)",
  show_rownames = F,
  show_colnames = F,
  silent = F
)

```

## Simulation 5: Large, noisy dataset

This case is intended to test how well structure can be uncovered as $P_n$ increases. We test for:

\[
\begin{aligned}
P_n &= 0.1 \times P_s \\
P_n &= 0.5 \times P_s \\
P_n &= P_s \\
P_n &= 5 \times P_s \\
P_n &= 10 \times P_s
\end{aligned}
\]

### Simulation 5a: Large, slightly noisy dataset

* $N$ = 100;
* $P_s$ = 20;
* $P_n$ = 2;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_5a_noise, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 20
P_n <- 2
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 5a: slightly noisy dataset",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 5a: slightly noisy dataset"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 5a: slightly noisy dataset",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

### Simulation 5b: Large, mildly noisy dataset

* $N$ = 100;
* $P_s$ = 20;
* $P_n$ = 10;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_5b_noise, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 20
P_n <- 10
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 5b: mildly noisy dataset",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 5b: mildly noisy dataset"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 5b: Mildly noisy dataset",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

### Simulation 5c: Large, noisy dataset

* $N$ = 100;
* $P_s$ = 20;
* $P_n$ = 20;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_5c_noise, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 20
P_n <- 20
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 5c: noisy dataset",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 5c: noisy dataset"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 5c: Noisy dataset",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

### Simulation 5d: Large, very noisy dataset

* $N$ = 100;
* $P_s$ = 20;
* $P_n$ = 100;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_5d_noise, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 20
P_n <- 100
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 5d: very noisy data",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 5d: very noisy data"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 5d: Very noisy data",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

### Simulation 5e: Large, extremely noisy dataset

* $N$ = 100;
* $P_s$ = 20;
* $P_n$ = 200;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_5e_noise, echo = F}
# Generate data based upon inputs
N <- 100
P_s <- 20
P_n <- 200
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 5e: extremely noisy data",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 5e: extremely noisy data"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 5e: Extremely noisy data",
  show_rownames = F,
  show_colnames = F,
  silent = F
)

ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = T,
  main = "Simulation 5e: Extremely noisy data (ordered)",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

## Simulation 6: Small $N$, large $P$

To test the paradigm that gene expression data often fits into. If the features are relevant than as $P$ grows so too does the resolution of the clustering and thus structure can be discovered even as $\Delta_{\mu}$ decreases.

### Simulation 6a: Small $N$, large $P$ dataset

We test the small $N$, large $P$ case.

* $N$ = 50;
* $P_s$ = 500;
* $P_n$ = 0;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_6_small_N_large_p, echo = F}
# Generate data based upon inputs
N <- 50
P_s <- 500
P_n <- 0
K <- 5
pi <- rep(1/K, K)
delta_mu <- 1
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 6a: Small N, large P",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 6a: Small N, large P"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 6a: Small N, large P",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

### Simulation 6b: Small $N$, large $P$, close means dataset

We test how small the $\Delta_{\mu}$ can be for the clustering structure to still be possible to resolve. To a degree, the level to which $\Delta_{\mu}$ can shrink without structure disappearing is a function of both $P_s$. 

* $N$ = 50;
* $P_s$ = 500;
* $P_n$ = 0;
* $K$ = 5;
* $\pi = vec(\frac{1}{5})$;
* $\Delta_{\mu} = 0.2$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_6a_small_N_large_p_close_means, echo = F}
# Generate data based upon inputs
N <- 50
P_s <- 500
P_n <- 0
K <- 5
pi <- rep(1/K, K)
delta_mu <- 0.2
sigma <- 1
alpha <- 1

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 6b: Small N, large P, close means",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 6b: Small N, large P, close means"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 6b: Small N, large P, close means",
  show_rownames = F,
  show_colnames = F,
  silent = F
)

ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = T,
  main = "Simulation 6b: Small N, large P, close means (ordered)",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```


## Simulation 7: varying subpoputlation proportions

* $N$ = 200;
* $P_s$ = 20;
* $P_n$ = 0;
* $K$ = 5;
* $\pi = (0.5, 0.25, 0.125, 0.675, 0.675)^T$;
* $\Delta_{\mu} = 1$; and
* $\sigma^2_{kp} = 1$.

```{r simulation_7, echo = F}
# Generate data based upon inputs
N <- 200
P_s <- 20
P_n <- 2
K <- 5
pi <- c(0.5, 0.25, 0.125, 0.0675, 0.0675)
delta_mu <- 1
sigma <- 1
alpha <- 0.5

my_data <- generateFullDataset(
  K,
  N,
  P_s,
  delta_mu,
  sigma,
  pi,
  P_n,
  alpha
)

# PCA analysis
pc_1 <- prcomp(my_data$data)

# Visualise the first two components
autoplot(pc_1, data = my_data$data) +
  geom_point(aes(colour = as.factor(my_data$cluster_IDs)), alpha = 0.4) +
  labs(
    title = "PCA of simulation 7: varying subpopulation proportions",
    subtitle = "Coloured by cluster IDs",
    colour = "Cluster"
  ) +
  scale_colour_viridis_d()

p_pca <- pcaScatterPlot(pc_1$x, my_data$cluster_IDs, n_comp = 10) +
  labs(
    title = "PCA for simulation 7: varying subpopulation proportions"
  )

p_pca

# Heatmap the data, annotated by cluster ID
ph <- plotData(my_data$data, my_data$cluster_IDs,
  cluster_rows = F,
  main = "Simulation 7: varying subpopulation proportions",
  show_rownames = F,
  show_colnames = F,
  silent = F
)
```

## References


